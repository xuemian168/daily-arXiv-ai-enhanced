{"id": "2510.20852", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.20852", "abs": "https://arxiv.org/abs/2510.20852", "authors": ["Safa Ben Atitallah", "Maha Driss", "Henda Ben Ghezela"], "title": "FedMicro-IDA: A Federated Learning and Microservices-based Framework for IoT Data Analytics", "comment": null, "summary": "The Internet of Things (IoT) has recently proliferated in both size and\ncomplexity. Using multi-source and heterogeneous IoT data aids in providing\nefficient data analytics for a variety of prevalent and crucial applications.\nTo address the privacy and security concerns raised by analyzing IoT data\nlocally or in the cloud, distributed data analytics techniques were proposed to\ncollect and analyze data in edge or fog devices. In this context, federated\nlearning has been recommended as an ideal distributed machine/deep\nlearning-based technique for edge/fog computing environments. Additionally, the\ndata analytics results are time-sensitive; they should be generated with\nminimal latency and high reliability. As a result, reusing efficient\narchitectures validated through a high number of challenging test cases would\nbe advantageous. The work proposed here presents a solution using a\nmicroservices-based architecture that allows an IoT application to be\nstructured as a collection of fine-grained, loosely coupled, and reusable\nentities. The proposed solution uses the promising capabilities of federated\nlearning to provide intelligent microservices that ensure efficient, flexible,\nand extensible data analytics. This solution aims to deliver cloud calculations\nto the edge to reduce latency and bandwidth congestion while protecting the\nprivacy of exchanged data. The proposed approach was validated through an\nIoT-malware detection and classification use case. MaleVis, a publicly\navailable dataset, was used in the experiments to analyze and validate the\nproposed approach. This dataset included more than 14,000 RGB-converted images,\ncomprising 25 malware classes and one benign class. The results showed that our\nproposed approach outperformed existing state-of-the-art methods in terms of\ndetection and classification performance, with a 99.24%."}
{"id": "2510.20856", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.20856", "abs": "https://arxiv.org/abs/2510.20856", "authors": ["Jia Deng", "Jin Li", "Zhenhua Zhao", "Shaowei Wang"], "title": "FPT-Noise: Dynamic Scene-Aware Counterattack for Test-Time Adversarial Defense in Vision-Language Models", "comment": "11pages,4figures", "summary": "Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable\nzero-shot generalizability across diverse downstream tasks. However, recent\nstudies have revealed that VLMs, including CLIP, are highly vulnerable to\nadversarial attacks, particularly on their visual modality. Traditional methods\nfor improving adversarial robustness, such as adversarial training, involve\nextensive retraining and can be computationally expensive. In this paper, we\npropose a new Test-Time defense: Feature Perception Threshold Counterattack\nNoise (FPT-Noise), which enhances the adversarial robustness of CLIP without\ncostly fine-tuning. Our core contributions are threefold: First, we introduce a\nDynamic Feature Modulator that dynamically generate an image-specific and\nattack-adaptive noise intensity parameter. Second, We reanalyzed the image\nfeatures of CLIP. When images are exposed to different levels of noise, clean\nimages and adversarial images exhibit distinct rates of feature change. We\nestablished a feature perception threshold to distinguish clean images from\nattacked ones. Finally, we integrate a Scene-Aware Regulation guided by a\nstability threshold and leverage Test-Time Transformation Ensembling (TTE) to\nfurther mitigate the impact of residual noise and enhance robustness.Extensive\nexperimentation has demonstrated that FPT-Noise significantly outperforms\nexisting Test-Time defense methods, boosting average robust accuracy from 0.07%\nto 56.86% under AutoAttack while maintaining high performance on clean images\n(-1.1%). The code will be made public following the publication of the study.\nThe code will be made public following the publication of the study."}
{"id": "2510.20858", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.20858", "abs": "https://arxiv.org/abs/2510.20858", "authors": ["Nubio Vidal", "Naghmeh Moradpoor", "Leandros Maglaras"], "title": "Everyone Needs AIR: An Agnostic Incident Reporting Framework for Cybersecurity in Operational Technology", "comment": null, "summary": "Operational technology (OT) networks are increasingly coupled with\ninformation technology (IT), expanding the attack surface and complicating\nincident response. Although OT standards emphasise incident reporting and\nevidence preservation, they do not specify what data to capture during an\nincident, which hinders coordination across stakeholders. In contrast, IT\nguidance defines reporting content but does not address OT constraints. This\npaper presents the Agnostic Incident Reporting (AIR) framework for live OT\nincident reporting. AIR comprises 25 elements organised into seven groups to\ncapture incident context, chronology, impacts, and actions, tailored to\ntechnical, managerial, and regulatory needs. We evaluate AIR by mapping it to\nmajor OT standards, defining activation points for integration and triggering\nestablished OT frameworks, and then retrospectively applying it to the 2015\nUkrainian distribution grid incident. The evaluation indicates that AIR\ntranslates high-level requirements into concrete fields, overlays existing\nframeworks without vendor dependence, and can support situational awareness and\ncommunication during response. AIR offers a basis for standardising live OT\nincident reporting while supporting technical coordination and regulatory\nalignment."}
{"id": "2510.20922", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.20922", "abs": "https://arxiv.org/abs/2510.20922", "authors": ["Luigi D. C. Soares", "Mário S. Alvim", "Natasha Fernandes"], "title": "A new measure for dynamic leakage based on quantitative information flow", "comment": null, "summary": "Quantitative information flow (QIF) is concerned with assessing the leakage\nof information in computational systems. In QIF there are two main perspectives\nfor the quantification of leakage. On one hand, the static perspective\nconsiders all possible runs of the system in the computation of information\nflow, and is usually employed when preemptively deciding whether or not to run\nthe system. On the other hand, the dynamic perspective considers only a\nspecific, concrete run of the system that has been realised, while ignoring all\nother runs. The dynamic perspective is relevant for, e.g., system monitors and\ntrackers, especially when deciding whether to continue or to abort a particular\nrun based on how much leakage has occurred up to a certain point. Although the\nstatic perspective of leakage is well-developed in the literature, the dynamic\nperspective still lacks the same level of theoretical maturity. In this paper\nwe take steps towards bridging this gap with the following key contributions:\n(i) we provide a novel definition of dynamic leakage that decouples the\nadversary's belief about the secret value from a baseline distribution on\nsecrets against which the success of the attack is measured; (ii) we\ndemonstrate that our formalisation satisfies relevant information-theoretic\naxioms, including non-interference and relaxed versions of monotonicity and the\ndata-processing inequality (DPI); (iii) we identify under what kind of analysis\nstrong versions of the axioms of monotonicity and the DPI might not hold, and\nexplain the implications of this (perhaps counter-intuitive) outcome; (iv) we\nshow that our definition of dynamic leakage is compatible with the\nwell-established static perspective; and (v) we exemplify the use of our\ndefinition on the formalisation of attacks against privacy-preserving data\nreleases."}
{"id": "2510.20930", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20930", "abs": "https://arxiv.org/abs/2510.20930", "authors": ["Soham Hans", "Stacy Marsella", "Sophia Hirschmann", "Nikolos Gurney"], "title": "Security Logs to ATT&CK Insights: Leveraging LLMs for High-Level Threat Understanding and Cognitive Trait Inference", "comment": null, "summary": "Understanding adversarial behavior in cybersecurity has traditionally relied\non high-level intelligence reports and manual interpretation of attack chains.\nHowever, real-time defense requires the ability to infer attacker intent and\ncognitive strategy directly from low-level system telemetry such as intrusion\ndetection system (IDS) logs. In this paper, we propose a novel framework that\nleverages large language models (LLMs) to analyze Suricata IDS logs and infer\nattacker actions in terms of MITRE ATT&CK techniques. Our approach is grounded\nin the hypothesis that attacker behavior reflects underlying cognitive biases\nsuch as loss aversion, risk tolerance, or goal persistence that can be\nextracted and modeled through careful observation of log sequences. This lays\nthe groundwork for future work on behaviorally adaptive cyber defense and\ncognitive trait inference. We develop a strategy-driven prompt system to\nsegment large amounts of network logs data into distinct behavioral phases in a\nhighly efficient manner, enabling the LLM to associate each phase with likely\ntechniques and underlying cognitive motives. By mapping network-layer events to\nhigh-level attacker strategies, our method reveals how behavioral signals such\nas tool switching, protocol transitions, or pivot patterns correspond to\npsychologically meaningful decision points. The results demonstrate that LLMs\ncan bridge the semantic gap between packet-level logs and strategic intent,\noffering a pathway toward cognitive-adaptive cyber defense.\n  Keywords: Cognitive Cybersecurity, Large Language Models (LLMs),\nCyberpsychology, Intrusion Detection Systems (IDS), MITRE ATT&CK, Cognitive\nBiases"}
{"id": "2510.20932", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.20932", "abs": "https://arxiv.org/abs/2510.20932", "authors": ["Reza Ahmari", "Ahmad Mohammadi", "Vahid Hemmati", "Mohammed Mynuddin", "Mahmoud Nabil Mahmoud", "Parham Kebria", "Abdollah Homaifar", "Mehrdad Saif"], "title": "An Experimental Study of Trojan Vulnerabilities in UAV Autonomous Landing", "comment": "6 pages", "summary": "This study investigates the vulnerabilities of autonomous navigation and\nlanding systems in Urban Air Mobility (UAM) vehicles. Specifically, it focuses\non Trojan attacks that target deep learning models, such as Convolutional\nNeural Networks (CNNs). Trojan attacks work by embedding covert triggers within\na model's training data. These triggers cause specific failures under certain\nconditions, while the model continues to perform normally in other situations.\nWe assessed the vulnerability of Urban Autonomous Aerial Vehicles (UAAVs) using\nthe DroNet framework. Our experiments showed a significant drop in accuracy,\nfrom 96.4% on clean data to 73.3% on data triggered by Trojan attacks. To\nconduct this study, we collected a custom dataset and trained models to\nsimulate real-world conditions. We also developed an evaluation framework\ndesigned to identify Trojan-infected models. This work demonstrates the\npotential security risks posed by Trojan attacks and lays the groundwork for\nfuture research on enhancing the resilience of UAM systems."}
{"id": "2510.20956", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.20956", "abs": "https://arxiv.org/abs/2510.20956", "authors": ["Zheng-Xin Yong", "Stephen H. Bach"], "title": "Self-Jailbreaking: Language Models Can Reason Themselves Out of Safety Alignment After Benign Reasoning Training", "comment": null, "summary": "We discover a novel and surprising phenomenon of unintentional misalignment\nin reasoning language models (RLMs), which we call self-jailbreaking.\nSpecifically, after benign reasoning training on math or code domains, RLMs\nwill use multiple strategies to circumvent their own safety guardrails. One\nstrategy is to introduce benign assumptions about users and scenarios to\njustify fulfilling harmful requests. For instance, an RLM reasons that harmful\nrequests like ``outline a strategy for stealing customers' credit card\ninformation from a retail store'' could be associated with the benign intent of\n``a security professional trying to test defense,'' despite no such benign\ncontext being provided as input. We observe that many open-weight RLMs,\nincluding DeepSeek-R1-distilled, s1.1, Phi-4-mini-reasoning, and Nemotron,\nsuffer from self-jailbreaking despite being aware of the harmfulness of the\nrequests. We also provide a mechanistic understanding of self-jailbreaking:\nRLMs are more compliant after benign reasoning training, and after\nself-jailbreaking, models appear to perceive malicious requests as less harmful\nin the CoT, thus enabling compliance with them. To mitigate self-jailbreaking,\nwe find that including minimal safety reasoning data during training is\nsufficient to ensure RLMs remain safety-aligned. Our work provides the first\nsystematic analysis of self-jailbreaking behavior and offers a practical path\nforward for maintaining safety in increasingly capable RLMs."}
{"id": "2510.20975", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20975", "abs": "https://arxiv.org/abs/2510.20975", "authors": ["Darrin Lea", "James Ghawaly", "Golden Richard III", "Aisha Ali-Gombe", "Andrew Case"], "title": "REx86: A Local Large Language Model for Assisting in x86 Assembly Reverse Engineering", "comment": "Accepted in 2025 Annual Computer Security Applications Conference\n  (ACSAC)", "summary": "Reverse engineering (RE) of x86 binaries is indispensable for malware and\nfirmware analysis, but remains slow due to stripped metadata and adversarial\nobfuscation. Large Language Models (LLMs) offer potential for improving RE\nefficiency through automated comprehension and commenting, but cloud-hosted,\nclosed-weight models pose privacy and security risks and cannot be used in\nclosed-network facilities. We evaluate parameter-efficient fine-tuned local\nLLMs for assisting with x86 RE tasks in these settings. Eight open-weight\nmodels across the CodeLlama, Qwen2.5-Coder, and CodeGemma series are fine-tuned\non a custom curated dataset of 5,981 x86 assembly examples. We evaluate them\nquantitatively and identify the fine-tuned Qwen2.5-Coder-7B as the top\nperformer, which we name REx86.\n  REx86 reduces test-set cross-entropy loss by 64.2% and improves semantic\ncosine similarity against ground truth by 20.3\\% over its base model. In a\nlimited user case study (n=43), REx86 significantly enhanced line-level code\nunderstanding (p = 0.031) and increased the correct-solve rate from 31% to 53%\n(p = 0.189), though the latter did not reach statistical significance.\nQualitative analysis shows more accurate, concise comments with fewer\nhallucinations.\n  REx86 delivers state-of-the-art assistance in x86 RE among local, open-weight\nLLMs. Our findings demonstrate the value of domain-specific fine-tuning, and\nhighlight the need for more commented disassembly data to further enhance LLM\nperformance in RE. REx86, its dataset, and LoRA adapters are publicly available\nat https://github.com/dlea8/REx86 and https://zenodo.org/records/15420461."}
{"id": "2510.21004", "categories": ["cs.CR", "cs.LG", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2510.21004", "abs": "https://arxiv.org/abs/2510.21004", "authors": ["Nguyen Linh Bao Nguyen", "Alsharif Abuadbba", "Kristen Moore", "Tingming Wu"], "title": "Can Current Detectors Catch Face-to-Voice Deepfake Attacks?", "comment": "8 pages, Accepted at Workshop on AI for Cyber Threat Intelligence,\n  co-located with ACSAC 2025", "summary": "The rapid advancement of generative models has enabled the creation of\nincreasingly stealthy synthetic voices, commonly referred to as audio\ndeepfakes. A recent technique, FOICE [USENIX'24], demonstrates a particularly\nalarming capability: generating a victim's voice from a single facial image,\nwithout requiring any voice sample. By exploiting correlations between facial\nand vocal features, FOICE produces synthetic voices realistic enough to bypass\nindustry-standard authentication systems, including WeChat Voiceprint and\nMicrosoft Azure. This raises serious security concerns, as facial images are\nfar easier for adversaries to obtain than voice samples, dramatically lowering\nthe barrier to large-scale attacks. In this work, we investigate two core\nresearch questions: (RQ1) can state-of-the-art audio deepfake detectors\nreliably detect FOICE-generated speech under clean and noisy conditions, and\n(RQ2) whether fine-tuning these detectors on FOICE data improves detection\nwithout overfitting, thereby preserving robustness to unseen voice generators\nsuch as SpeechT5.\n  Our study makes three contributions. First, we present the first systematic\nevaluation of FOICE detection, showing that leading detectors consistently fail\nunder both standard and noisy conditions. Second, we introduce targeted\nfine-tuning strategies that capture FOICE-specific artifacts, yielding\nsignificant accuracy improvements. Third, we assess generalization after\nfine-tuning, revealing trade-offs between specialization to FOICE and\nrobustness to unseen synthesis pipelines. These findings expose fundamental\nweaknesses in today's defenses and motivate new architectures and training\nprotocols for next-generation audio deepfake detection."}
{"id": "2510.21024", "categories": ["cs.CR", "cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21024", "abs": "https://arxiv.org/abs/2510.21024", "authors": ["Jonathan Gold", "Tristan Freiberg", "Haruna Isah", "Shirin Shahabi"], "title": "JSTprove: Pioneering Verifiable AI for a Trustless Future", "comment": "13 pages, 8 figures, and 4 tables", "summary": "The integration of machine learning (ML) systems into critical industries\nsuch as healthcare, finance, and cybersecurity has transformed decision-making\nprocesses, but it also brings new challenges around trust, security, and\naccountability. As AI systems become more ubiquitous, ensuring the transparency\nand correctness of AI-driven decisions is crucial, especially when they have\ndirect consequences on privacy, security, or fairness. Verifiable AI, powered\nby Zero-Knowledge Machine Learning (zkML), offers a robust solution to these\nchallenges. zkML enables the verification of AI model inferences without\nexposing sensitive data, providing an essential layer of trust and privacy.\nHowever, traditional zkML systems typically require deep cryptographic\nexpertise, placing them beyond the reach of most ML engineers. In this paper,\nwe introduce JSTprove, a specialized zkML toolkit, built on Polyhedra Network's\nExpander backend, to enable AI developers and ML engineers to generate and\nverify proofs of AI inference. JSTprove provides an end-to-end verifiable AI\ninference pipeline that hides cryptographic complexity behind a simple\ncommand-line interface while exposing auditable artifacts for reproducibility.\nWe present the design, innovations, and real-world use cases of JSTprove as\nwell as our blueprints and tooling to encourage community review and extension.\nJSTprove therefore serves both as a usable zkML product for current engineering\nneeds and as a reproducible foundation for future research and production\ndeployments of verifiable AI."}
{"id": "2510.21053", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.21053", "abs": "https://arxiv.org/abs/2510.21053", "authors": ["Li An", "Yujian Liu", "Yepeng Liu", "Yuheng Bu", "Yang Zhang", "Shiyu Chang"], "title": "A Reinforcement Learning Framework for Robust and Secure LLM Watermarking", "comment": null, "summary": "Watermarking has emerged as a promising solution for tracing and\nauthenticating text generated by large language models (LLMs). A common\napproach to LLM watermarking is to construct a green/red token list and assign\nhigher or lower generation probabilities to the corresponding tokens,\nrespectively. However, most existing watermarking algorithms rely on heuristic\ngreen/red token list designs, as directly optimizing the list design with\ntechniques such as reinforcement learning (RL) comes with several challenges.\nFirst, desirable watermarking involves multiple criteria, i.e., detectability,\ntext quality, robustness against removal attacks, and security against spoofing\nattacks. Directly optimizing for these criteria introduces many partially\nconflicting reward terms, leading to an unstable convergence process. Second,\nthe vast action space of green/red token list choices is susceptible to reward\nhacking. In this paper, we propose an end-to-end RL framework for robust and\nsecure LLM watermarking. Our approach adopts an anchoring mechanism for reward\nterms to ensure stable training and introduces additional regularization terms\nto prevent reward hacking. Experiments on standard benchmarks with two backbone\nLLMs show that our method achieves a state-of-the-art trade-off across all\ncriteria, with notable improvements in resistance to spoofing attacks without\ndegrading other criteria. Our code is available at\nhttps://github.com/UCSB-NLP-Chang/RL-watermark."}
{"id": "2510.21057", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21057", "abs": "https://arxiv.org/abs/2510.21057", "authors": ["Nils Philipp Walter", "Chawin Sitawarin", "Jamie Hayes", "David Stutz", "Ilia Shumailov"], "title": "Soft Instruction De-escalation Defense", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in agentic systems\nthat interact with an external environment; this makes them susceptible to\nprompt injections when dealing with untrusted data. To overcome this\nlimitation, we propose SIC (Soft Instruction Control)-a simple yet effective\niterative prompt sanitization loop designed for tool-augmented LLM agents. Our\nmethod repeatedly inspects incoming data for instructions that could compromise\nagent behavior. If such content is found, the malicious content is rewritten,\nmasked, or removed, and the result is re-evaluated. The process continues until\nthe input is clean or a maximum iteration limit is reached; if imperative\ninstruction-like content remains, the agent halts to ensure security. By\nallowing multiple passes, our approach acknowledges that individual rewrites\nmay fail but enables the system to catch and correct missed injections in later\nsteps. Although immediately useful, worst-case analysis shows that SIC is not\ninfallible; strong adversary can still get a 15% ASR by embedding\nnon-imperative workflows. This nonetheless raises the bar."}
{"id": "2510.21124", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.21124", "abs": "https://arxiv.org/abs/2510.21124", "authors": ["Jie Zhang", "Xiaohong Li", "Mengke Zhang", "Ruitao Feng", "Shanshan Xu", "Zhe Hou", "Guangdong Bai"], "title": "QAE-BAC: Achieving Quantifiable Anonymity and Efficiency in Blockchain-Based Access Control with Attribute", "comment": "17 pages, 10 figures", "summary": "Blockchain-based Attribute-Based Access Control (BC-ABAC) offers a\ndecentralized paradigm for secure data governance but faces two inherent\nchallenges: the transparency of blockchain ledgers threatens user privacy by\nenabling reidentification attacks through attribute analysis, while the\ncomputational complexity of policy matching clashes with blockchain's\nperformance constraints. Existing solutions, such as those employing\nZero-Knowledge Proofs (ZKPs), often incur high overhead and lack measurable\nanonymity guarantees, while efficiency optimizations frequently ignore privacy\nimplications. To address these dual challenges, this paper proposes QAEBAC\n(Quantifiable Anonymity and Efficiency in Blockchain-Based Access Control with\nAttribute). QAE-BAC introduces a formal (r, t)-anonymity model to dynamically\nquantify the re-identification risk of users based on their access attributes\nand history. Furthermore, it features an Entropy-Weighted Path Tree (EWPT) that\noptimizes policy structure based on realtime anonymity metrics, drastically\nreducing policy matching complexity. Implemented and evaluated on Hyperledger\nFabric, QAE-BAC demonstrates a superior balance between privacy and\nperformance. Experimental results show that it effectively mitigates\nre-identification risks and outperforms state-of-the-art baselines, achieving\nup to an 11x improvement in throughput and an 87% reduction in latency, proving\nits practicality for privacy-sensitive decentralized applications."}
{"id": "2510.21133", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21133", "abs": "https://arxiv.org/abs/2510.21133", "authors": ["Divyanshu Kumar", "Nitin Aravind Birur", "Tanay Baswa", "Sahil Agarwal", "Prashanth Harshangi"], "title": "Quantifying CBRN Risk in Frontier Models", "comment": null, "summary": "Frontier Large Language Models (LLMs) pose unprecedented dual-use risks\nthrough the potential proliferation of chemical, biological, radiological, and\nnuclear (CBRN) weapons knowledge. We present the first comprehensive evaluation\nof 10 leading commercial LLMs against both a novel 200-prompt CBRN dataset and\na 180-prompt subset of the FORTRESS benchmark, using a rigorous three-tier\nattack methodology. Our findings expose critical safety vulnerabilities: Deep\nInception attacks achieve 86.0\\% success versus 33.8\\% for direct requests,\ndemonstrating superficial filtering mechanisms; Model safety performance varies\ndramatically from 2\\% (claude-opus-4) to 96\\% (mistral-small-latest) attack\nsuccess rates; and eight models exceed 70\\% vulnerability when asked to enhance\ndangerous material properties. We identify fundamental brittleness in current\nsafety alignment, where simple prompt engineering techniques bypass safeguards\nfor dangerous CBRN information. These results challenge industry safety claims\nand highlight urgent needs for standardized evaluation frameworks, transparent\nsafety metrics, and more robust alignment techniques to mitigate catastrophic\nmisuse risks while preserving beneficial capabilities."}
{"id": "2510.21189", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.21189", "abs": "https://arxiv.org/abs/2510.21189", "authors": ["Yukun Jiang", "Mingjie Li", "Michael Backes", "Yang Zhang"], "title": "Adjacent Words, Divergent Intents: Jailbreaking Large Language Models via Task Concurrency", "comment": "Accepted in NeurIPS 2025", "summary": "Despite their superior performance on a wide range of domains, large language\nmodels (LLMs) remain vulnerable to misuse for generating harmful content, a\nrisk that has been further amplified by various jailbreak attacks. Existing\njailbreak attacks mainly follow sequential logic, where LLMs understand and\nanswer each given task one by one. However, concurrency, a natural extension of\nthe sequential scenario, has been largely overlooked. In this work, we first\npropose a word-level method to enable task concurrency in LLMs, where adjacent\nwords encode divergent intents. Although LLMs maintain strong utility in\nanswering concurrent tasks, which is demonstrated by our evaluations on\nmathematical and general question-answering benchmarks, we notably observe that\ncombining a harmful task with a benign one significantly reduces the\nprobability of it being filtered by the guardrail, showing the potential risks\nassociated with concurrency in LLMs. Based on these findings, we introduce\n$\\texttt{JAIL-CON}$, an iterative attack framework that\n$\\underline{\\text{JAIL}}$breaks LLMs via task $\\underline{\\text{CON}}$currency.\nExperiments on widely-used LLMs demonstrate the strong jailbreak capabilities\nof $\\texttt{JAIL-CON}$ compared to existing attacks. Furthermore, when the\nguardrail is applied as a defense, compared to the sequential answers generated\nby previous attacks, the concurrent answers in our $\\texttt{JAIL-CON}$ exhibit\ngreater stealthiness and are less detectable by the guardrail, highlighting the\nunique feature of task concurrency in jailbreaking LLMs."}
{"id": "2510.21190", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.21190", "abs": "https://arxiv.org/abs/2510.21190", "authors": ["Mingrui Liu", "Sixiao Zhang", "Cheng Long", "Kwok Yan Lam"], "title": "The Trojan Example: Jailbreaking LLMs through Template Filling and Unsafety Reasoning", "comment": "under review", "summary": "Large Language Models (LLMs) have advanced rapidly and now encode extensive\nworld knowledge. Despite safety fine-tuning, however, they remain susceptible\nto adversarial prompts that elicit harmful content. Existing jailbreak\ntechniques fall into two categories: white-box methods (e.g., gradient-based\napproaches such as GCG), which require model internals and are infeasible for\nclosed-source APIs, and black-box methods that rely on attacker LLMs to search\nor mutate prompts but often produce templates that lack explainability and\ntransferability. We introduce TrojFill, a black-box jailbreak that reframes\nunsafe instruction as a template-filling task. TrojFill embeds obfuscated\nharmful instructions (e.g., via placeholder substitution or Caesar/Base64\nencoding) inside a multi-part template that asks the model to (1) reason why\nthe original instruction is unsafe (unsafety reasoning) and (2) generate a\ndetailed example of the requested text, followed by a sentence-by-sentence\nanalysis. The crucial \"example\" component acts as a Trojan Horse that contains\nthe target jailbreak content while the surrounding task framing reduces refusal\nrates. We evaluate TrojFill on standard jailbreak benchmarks across leading\nLLMs (e.g., ChatGPT, Gemini, DeepSeek, Qwen), showing strong empirical\nperformance (e.g., 100% attack success on Gemini-flash-2.5 and DeepSeek-3.1,\nand 97% on GPT-4o). Moreover, the generated prompts exhibit improved\ninterpretability and transferability compared with prior black-box optimization\napproaches. We release our code, sample prompts, and generated outputs to\nsupport future red-teaming research."}
{"id": "2510.21214", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.21214", "abs": "https://arxiv.org/abs/2510.21214", "authors": ["Xingwei Zhong", "Kar Wai Fok", "Vrizlynn L. L. Thing"], "title": "Enhanced MLLM Black-Box Jailbreaking Attacks and Defenses", "comment": null, "summary": "Multimodal large language models (MLLMs) comprise of both visual and textual\nmodalities to process vision language tasks. However, MLLMs are vulnerable to\nsecurity-related issues, such as jailbreak attacks that alter the model's input\nto induce unauthorized or harmful responses. The incorporation of the\nadditional visual modality introduces new dimensions to security threats. In\nthis paper, we proposed a black-box jailbreak method via both text and image\nprompts to evaluate MLLMs. In particular, we designed text prompts with\nprovocative instructions, along with image prompts that introduced mutation and\nmulti-image capabilities. To strengthen the evaluation, we also designed a\nRe-attack strategy. Empirical results show that our proposed work can improve\ncapabilities to assess the security of both open-source and closed-source\nMLLMs. With that, we identified gaps in existing defense methods to propose new\nstrategies for both training-time and inference-time defense methods, and\nevaluated them across the new jailbreak methods. The experiment results showed\nthat the re-designed defense methods improved protections against the jailbreak\nattacks."}
{"id": "2510.21236", "categories": ["cs.CR", "cs.AI", "cs.SE", "D.2.0"], "pdf": "https://arxiv.org/pdf/2510.21236", "abs": "https://arxiv.org/abs/2510.21236", "authors": ["Christoph Bühler", "Matteo Biagiola", "Luca Di Grazia", "Guido Salvaneschi"], "title": "Securing AI Agent Execution", "comment": null, "summary": "Large Language Models (LLMs) have evolved into AI agents that interact with\nexternal tools and environments to perform complex tasks. The Model Context\nProtocol (MCP) has become the de facto standard for connecting agents with such\nresources, but security has lagged behind: thousands of MCP servers execute\nwith unrestricted access to host systems, creating a broad attack surface. In\nthis paper, we introduce AgentBound, the first access control framework for MCP\nservers. AgentBound combines a declarative policy mechanism, inspired by the\nAndroid permission model, with a policy enforcement engine that contains\nmalicious behavior without requiring MCP server modifications. We build a\ndataset containing the 296 most popular MCP servers, and show that access\ncontrol policies can be generated automatically from source code with 80.9%\naccuracy. We also show that AgentBound blocks the majority of security threats\nin several malicious MCP servers, and that policy enforcement engine introduces\nnegligible overhead. Our contributions provide developers and project managers\nwith a practical foundation for securing MCP servers while maintaining\nproductivity, enabling researchers and tool builders to explore new directions\nfor declarative access control and MCP security."}
{"id": "2510.21246", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.21246", "abs": "https://arxiv.org/abs/2510.21246", "authors": ["Michael Külper", "Jan-Niclas Hilgert", "Frank Breitinger", "Martin Lambertz"], "title": "What's Next, Cloud? A Forensic Framework for Analyzing Self-Hosted Cloud Storage Solutions", "comment": null, "summary": "Self-hosted cloud storage platforms like Nextcloud are gaining popularity\namong individuals and organizations seeking greater control over their data.\nHowever, this shift introduces new challenges for digital forensic\ninvestigations, particularly in systematically analyzing both client and server\ncomponents. Despite Nextcloud's widespread use, it has received limited\nattention in forensic research. In this work, we critically examine existing\ncloud storage forensic frameworks and highlight their limitations. To address\nthe gaps, we propose an extended forensic framework that incorporates device\nmonitoring and leverages cloud APIs for structured, repeatable evidence\nacquisition. Using Nextcloud as a case study, we demonstrate how its native\nAPIs can be used to reliably access forensic artifacts, and we introduce an\nopen-source acquisition tool that implements this approach. Our framework\nequips investigators with a more flexible method for analyzing self-hosted\ncloud storage systems, and offers a foundation for further development in this\nevolving area of digital forensics."}
{"id": "2510.21272", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.21272", "abs": "https://arxiv.org/abs/2510.21272", "authors": ["Lu Liu", "Wuqi Zhang", "Lili Wei", "Hao Guan", "Yongqiang Tian", "Yepang Liu"], "title": "LLM-Powered Detection of Price Manipulation in DeFi", "comment": null, "summary": "Decentralized Finance (DeFi) smart contracts manage billions of dollars,\nmaking them a prime target for exploits. Price manipulation vulnerabilities,\noften via flash loans, are a devastating class of attacks causing significant\nfinancial losses. Existing detection methods are limited. Reactive approaches\nanalyze attacks only after they occur, while proactive static analysis tools\nrely on rigid, predefined heuristics, limiting adaptability. Both depend on\nknown attack patterns, failing to identify novel variants or comprehend complex\neconomic logic. We propose PMDetector, a hybrid framework combining static\nanalysis with Large Language Model (LLM)-based reasoning to proactively detect\nprice manipulation vulnerabilities. Our approach uses a formal attack model and\na three-stage pipeline. First, static taint analysis identifies potentially\nvulnerable code paths. Second, a two-stage LLM process filters paths by\nanalyzing defenses and then simulates attacks to evaluate exploitability.\nFinally, a static analysis checker validates LLM results, retaining only\nhigh-risk paths and generating comprehensive vulnerability reports. To evaluate\nits effectiveness, we built a dataset of 73 real-world vulnerable and 288\nbenign DeFi protocols. Results show PMDetector achieves 88% precision and 90%\nrecall with Gemini 2.5-flash, significantly outperforming state-of-the-art\nstatic analysis and LLM-based approaches. Auditing a vulnerability with\nPMDetector costs just $0.03 and takes 4.0 seconds with GPT-4.1, offering an\nefficient and cost-effective alternative to manual audits."}
{"id": "2510.21353", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2510.21353", "abs": "https://arxiv.org/abs/2510.21353", "authors": ["Aditya Mitra", "Sibi Chakkaravarthy Sethuraman"], "title": "The Qey: Implementation and performance study of post quantum cryptography in FIDO2", "comment": null, "summary": "Authentication systems have evolved a lot since the 1960s when Fernando\nCorbato first proposed the password-based authentication. In 2013, the FIDO\nAlliance proposed using secure hardware for authentication, thus marking a\nmilestone in the passwordless authentication era [1]. Passwordless\nauthentication with a possession-based factor often relied on hardware-backed\ncryptographic methods. FIDO2 being one an amalgamation of the W3C Web\nAuthentication and FIDO Alliance Client to Authenticator Protocol is an\nindustry standard for secure passwordless authentication with rising adoption\nfor the same [2]. However, the current FIDO2 standards use ECDSA with SHA-256\n(ES256), RSA with SHA-256 (RS256) and similar classical cryptographic signature\nalgorithms. This makes it insecure against attacks involving large-scale\nquantum computers [3]. This study aims at exploring the usability of Module\nLattice based Digital Signature Algorithm (ML-DSA), based on Crystals Dilithium\nas a post quantum cryptographic signature standard for FIDO2. The paper\nhighlights the performance and security in comparison to keys with classical\nalgorithms."}
{"id": "2510.21401", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.21401", "abs": "https://arxiv.org/abs/2510.21401", "authors": ["Mojtaba Eshghie", "Gabriele Morello", "Matteo Lauretano", "Alexandre Bartel", "Martin Monperrus"], "title": "FLAMES: Fine-tuning LLMs to Synthesize Invariants for Smart Contract Security", "comment": null, "summary": "Smart contract vulnerabilities cost billions of dollars annually, yet\nexisting automated analysis tools fail to generate deployable defenses. We\npresent FLAMES, a novel automated approach that synthesizes executable runtime\nguards as Solidity \"require\" statements to harden smart contracts against\nexploits. Unlike prior work that relies on vulnerability labels, symbolic\nanalysis, or natural language specifications, FLAMES employs domain-adapted\nlarge language models trained through fill-in-the-middle supervised fine-tuning\non real-world invariants extracted from 514,506 verified contracts. Our\nextensive evaluation across three dimensions demonstrates FLAMES's\neffectiveness: (1) Compilation: FLAMES achieves 96.7% compilability for\nsynthesized invariant (2) Semantic Quality: on a curated test set of 5,000\nchallenging invariants, FLAMES produces exact or semantically equivalent\nmatches to ground truth in 44.5% of cases; (3) Exploit Mitigation: FLAMES\nprevents 22 out of 108 real exploits (20.4%) while preserving contract\nfunctionality, and (4) FLAMES successfully blocks the real-world APEMAGA\nincident by synthesizing a pre-condition that mitigates the attack. FLAMES\nestablishes that domain-adapted LLMs can automatically generate\nproduction-ready security defenses for smart contracts without requiring\nvulnerability detection, formal specifications, or human intervention. We\nrelease our code, model weights, datasets, and evaluation infrastructure to\nenable reproducible research in this critical domain."}
{"id": "2510.21459", "categories": ["cs.CR", "cs.CL", "cs.LG", "K.6.5; D.4.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.21459", "abs": "https://arxiv.org/abs/2510.21459", "authors": ["Adetayo Adebimpe", "Helmut Neukirchen", "Thomas Welsh"], "title": "SBASH: a Framework for Designing and Evaluating RAG vs. Prompt-Tuned LLM Honeypots", "comment": "to be published in: The 3rd International Conference on Foundation\n  and Large Language Models (FLLM2025), IEEE, 2025", "summary": "Honeypots are decoy systems used for gathering valuable threat intelligence\nor diverting attackers away from production systems. Maximising attacker\nengagement is essential to their utility. However research has highlighted that\ncontext-awareness, such as the ability to respond to new attack types, systems\nand attacker agents, is necessary to increase engagement. Large Language Models\n(LLMs) have been shown as one approach to increase context awareness but suffer\nfrom several challenges including accuracy and timeliness of response time,\nhigh operational costs and data-protection issues due to cloud deployment. We\npropose the System-Based Attention Shell Honeypot (SBASH) framework which\nmanages data-protection issues through the use of lightweight local LLMs. We\ninvestigate the use of Retrieval Augmented Generation (RAG) supported LLMs and\nnon-RAG LLMs for Linux shell commands and evaluate them using several different\nmetrics such as response time differences, realism from human testers, and\nsimilarity to a real system calculated with Levenshtein distance, SBert, and\nBertScore. We show that RAG improves accuracy for untuned models while models\nthat have been tuned via a system prompt that tells the LLM to respond like a\nLinux system achieve without RAG a similar accuracy as untuned with RAG, while\nhaving a slightly lower latency."}
{"id": "2510.21483", "categories": ["cs.CR", "math.GR", "E.3"], "pdf": "https://arxiv.org/pdf/2510.21483", "abs": "https://arxiv.org/abs/2510.21483", "authors": ["Pierre Guillot", "Auguste Hoang Duc", "Michel Koskas", "Florian Méhats"], "title": "Introducing GRAFHEN: Group-based Fully Homomorphic Encryption without Noise", "comment": null, "summary": "We present GRAFHEN, a new cryptographic scheme which offers Fully Homomorphic\nEncryption without the need for bootstrapping (or in other words, without\nnoise). Building on the work of Nuida and others, we achieve this using\nencodings in groups.\n  The groups are represented on a machine using rewriting systems. In this way\nthe subgroup membership problem, which an attacker would have to solve in order\nto break the scheme, becomes maximally hard, while performance is preserved. In\nfact we include a simple benchmark demonstrating that our implementation runs\nseveral orders of magnitude faster than existing standards.\n  We review many possible attacks against our protocol and explain how to\nprotect the scheme in each case."}
{"id": "2510.21601", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.21601", "abs": "https://arxiv.org/abs/2510.21601", "authors": ["Emmanuel Dare Alalade", "Ashraf Matrawy"], "title": "PTMF: A Privacy Threat Modeling Framework for IoT with Expert-Driven Threat Propagation Analysis", "comment": "26 pages, 18 figures", "summary": "Previous studies on PTA have focused on analyzing privacy threats based on\nthe potential areas of occurrence and their likelihood of occurrence. However,\nan in-depth understanding of the threat actors involved, their actions, and the\nintentions that result in privacy threats is essential. In this paper, we\npresent a novel Privacy Threat Model Framework (PTMF) that analyzes privacy\nthreats through different phases.\n  The PTMF development is motivated through the selected tactics from the MITRE\nATT\\&CK framework and techniques from the LINDDUN privacy threat model, making\nPTMF a privacy-centered framework. The proposed PTMF can be employed in various\nways, including analyzing the activities of threat actors during privacy\nthreats and assessing privacy risks in IoT systems, among others. In this\npaper, we conducted a user study on 12 privacy threats associated with IoT by\ndeveloping a questionnaire based on PTMF and recruited experts from both\nindustry and academia in the fields of security and privacy to gather their\nopinions. The collected data were analyzed and mapped to identify the threat\nactors involved in the identification of IoT users (IU) and the remaining 11\nprivacy threats. Our observation revealed the top three threat actors and the\ncritical paths they used during the IU privacy threat, as well as the remaining\n11 privacy threats. This study could provide a solid foundation for\nunderstanding how and where privacy measures can be proactively and effectively\ndeployed in IoT systems to mitigate privacy threats based on the activities and\nintentions of threat actors within these systems."}
{"id": "2510.21684", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.21684", "abs": "https://arxiv.org/abs/2510.21684", "authors": ["Albert Cheu", "Artem Lagzdin", "Brett McLarnon", "Daniel Ramage", "Katharine Daly", "Marco Gruteser", "Peter Kairouz", "Rakshita Tandon", "Stanislav Chiknavaryan", "Timon Van Overveldt", "Zoe Gong"], "title": "Toward provably private analytics and insights into GenAI use", "comment": null, "summary": "Large-scale systems that compute analytics over a fleet of devices must\nachieve high privacy and security standards while also meeting data quality,\nusability, and resource efficiency expectations. We present a next-generation\nfederated analytics system that uses Trusted Execution Environments (TEEs)\nbased on technologies like AMD SEV-SNP and Intel TDX to provide verifiable\nprivacy guarantees for all server-side processing. In our system, devices\nencrypt and upload data, tagging it with a limited set of allowable server-side\nprocessing steps. An open source, TEE-hosted key management service guarantees\nthat the data is accessible only to those steps, which are themselves protected\nby TEE confidentiality and integrity assurance guarantees. The system is\ndesigned for flexible workloads, including processing unstructured data with\nLLMs (for structured summarization) before aggregation into differentially\nprivate insights (with automatic parameter tuning). The transparency properties\nof our system allow any external party to verify that all raw and derived data\nis processed in TEEs, protecting it from inspection by the system operator, and\nthat differential privacy is applied to all released results. This system has\nbeen successfully deployed in production, providing helpful insights into\nreal-world GenAI experiences."}
