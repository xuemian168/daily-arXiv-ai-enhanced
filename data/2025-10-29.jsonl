{"id": "2510.23619", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23619", "abs": "https://arxiv.org/abs/2510.23619", "authors": ["Yuyang Miao", "Huijun Xing", "Danilo P. Mandic", "Tony G. Constantinides"], "title": "Short Ticketing Detection Framework Analysis Report", "comment": null, "summary": "This report presents a comprehensive analysis of an unsupervised multi-expert\nmachine learning framework for detecting short ticketing fraud in railway\nsystems. The study introduces an A/B/C/D station classification system that\nsuccessfully identifies suspicious patterns across 30 high-risk stations. The\nframework employs four complementary algorithms: Isolation Forest, Local\nOutlier Factor, One-Class SVM, and Mahalanobis Distance. Key findings include\nthe identification of five distinct short ticketing patterns and potential for\nshort ticketing recovery in transportation systems."}
{"id": "2510.23643", "categories": ["cs.CR", "cs.AI", "cs.LG", "I.2.6; D.4.6"], "pdf": "https://arxiv.org/pdf/2510.23643", "abs": "https://arxiv.org/abs/2510.23643", "authors": ["Zhixin Pan", "Ziyu Shu", "Linh Nguyen", "Amberbir Alemayoh"], "title": "SAND: A Self-supervised and Adaptive NAS-Driven Framework for Hardware Trojan Detection", "comment": null, "summary": "The globalized semiconductor supply chain has made Hardware Trojans (HT) a\nsignificant security threat to embedded systems, necessitating the design of\nefficient and adaptable detection mechanisms. Despite promising machine\nlearning-based HT detection techniques in the literature, they suffer from ad\nhoc feature selection and the lack of adaptivity, all of which hinder their\neffectiveness across diverse HT attacks. In this paper, we propose SAND, a\nselfsupervised and adaptive NAS-driven framework for efficient HT detection.\nSpecifically, this paper makes three key contributions. (1) We leverage\nself-supervised learning (SSL) to enable automated feature extraction,\neliminating the dependency on manually engineered features. (2) SAND integrates\nneural architecture search (NAS) to dynamically optimize the downstream\nclassifier, allowing for seamless adaptation to unseen benchmarks with minimal\nfine-tuning. (3) Experimental results show that SAND achieves a significant\nimprovement in detection accuracy (up to 18.3%) over state-of-the-art methods,\nexhibits high resilience against evasive Trojans, and demonstrates strong\ngeneralization."}
{"id": "2510.23673", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23673", "abs": "https://arxiv.org/abs/2510.23673", "authors": ["Bin Wang", "Zexin Liu", "Hao Yu", "Ao Yang", "Yenan Huang", "Jing Guo", "Huangsheng Cheng", "Hui Li", "Huiyu Wu"], "title": "MCPGuard : Automatically Detecting Vulnerabilities in MCP Servers", "comment": null, "summary": "The Model Context Protocol (MCP) has emerged as a standardized interface\nenabling seamless integration between Large Language Models (LLMs) and external\ndata sources and tools. While MCP significantly reduces development complexity\nand enhances agent capabilities, its openness and extensibility introduce\ncritical security vulnerabilities that threaten system trustworthiness and user\ndata protection. This paper systematically analyzes the security landscape of\nMCP-based systems, identifying three principal threat categories: (1) agent\nhijacking attacks stemming from protocol design deficiencies; (2) traditional\nweb vulnerabilities in MCP servers; and (3) supply chain security. To address\nthese challenges, we comprehensively survey existing defense strategies,\nexamining both proactive server-side scanning approaches, ranging from layered\ndetection pipelines and agentic auditing frameworks to zero-trust registry\nsystems, and runtime interaction monitoring solutions that provide continuous\noversight and policy enforcement. Our analysis reveals that MCP security\nfundamentally represents a paradigm shift where the attack surface extends from\ntraditional code execution to semantic interpretation of natural language\nmetadata, necessitating novel defense mechanisms tailored to this unique threat\nmodel."}
{"id": "2510.23675", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23675", "abs": "https://arxiv.org/abs/2510.23675", "authors": ["Yuchong Xie", "Zesen Liu", "Mingyu Luo", "Zhixiang Zhang", "Kaikai Zhang", "Zongjie Li", "Ping Chen", "Shuai Wang", "Dongdong She"], "title": "QueryIPI: Query-agnostic Indirect Prompt Injection on Coding Agents", "comment": null, "summary": "Modern coding agents integrated into IDEs combine powerful tools and\nsystem-level actions, exposing a high-stakes attack surface. Existing Indirect\nPrompt Injection (IPI) studies focus mainly on query-specific behaviors,\nleading to unstable attacks with lower success rates. We identify a more\nsevere, query-agnostic threat that remains effective across diverse user\ninputs. This challenge can be overcome by exploiting a common vulnerability:\nleakage of the agent's internal prompt, which turns the attack into a\nconstrained white-box optimization problem. We present QueryIPI, the first\nquery-agnostic IPI method for coding agents. QueryIPI refines malicious tool\ndescriptions through an iterative, prompt-based process informed by the leaked\ninternal prompt. Experiments on five simulated agents show that QueryIPI\nachieves up to 87 percent success, outperforming baselines, and the generated\nmalicious descriptions also transfer to real-world systems, highlighting a\npractical security risk to modern LLM-based coding agents."}
{"id": "2510.23847", "categories": ["cs.CR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.23847", "abs": "https://arxiv.org/abs/2510.23847", "authors": ["Joel Poncha Lemayian", "Ghyslain Gagnon", "Kaiwen Zhang", "Pascal Giard"], "title": "EthVault: A Secure and Resource-Conscious FPGA-Based Ethereum Cold Wallet", "comment": "Under review for publication", "summary": "Cryptocurrency blockchain networks safeguard digital assets using\ncryptographic keys, with wallets playing a critical role in generating,\nstoring, and managing these keys. Wallets, typically categorized as hot and\ncold, offer varying degrees of security and convenience. However, they are\ngenerally software-based applications running on microcontrollers.\nConsequently, they are vulnerable to malware and side-channel attacks, allowing\nperpetrators to extract private keys by targeting critical algorithms, such as\nECC, which processes private keys to generate public keys and authorize\ntransactions. To address these issues, this work presents EthVault, the first\nhardware architecture for an Ethereum hierarchically deterministic cold wallet,\nfeaturing hardware implementations of key algorithms for secure key generation.\nAlso, an ECC architecture resilient to side-channel and timing attacks is\nproposed. Moreover, an architecture of the child key derivation function, a\nfundamental component of cryptocurrency wallets, is proposed. The design\nminimizes resource usage, meeting market demand for small, portable\ncryptocurrency wallets. FPGA implementation results validate the feasibility of\nthe proposed approach. The ECC architecture exhibits uniform execution behavior\nacross varying inputs, while the complete design utilizes only 27%, 7%, and 6%\nof LUTs, registers, and RAM blocks, respectively, on a Xilinx Zynq UltraScale+\nFPGA."}
{"id": "2510.23891", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23891", "abs": "https://arxiv.org/abs/2510.23891", "authors": ["Jiaqi Xue", "Yifei Zhao", "Mansour Al Ghanim", "Shangqian Gao", "Ruimin Sun", "Qian Lou", "Mengxin Zheng"], "title": "PRO: Enabling Precise and Robust Text Watermark for Open-Source LLMs", "comment": null, "summary": "Text watermarking for large language models (LLMs) enables model owners to\nverify text origin and protect intellectual property. While watermarking\nmethods for closed-source LLMs are relatively mature, extending them to\nopen-source models remains challenging, as developers cannot control the\ndecoding process. Consequently, owners of open-source LLMs lack practical means\nto verify whether text was generated by their models. A core difficulty lies in\nembedding watermarks directly into model weights without hurting detectability.\nA promising idea is to distill watermarks from a closed-source model into an\nopen one, but this suffers from (i) poor detectability due to mismatch between\nlearned and predefined patterns, and (ii) fragility to downstream modifications\nsuch as fine-tuning or model merging. To overcome these limitations, we propose\nPRO, a Precise and Robust text watermarking method for open-source LLMs. PRO\njointly trains a watermark policy model with the LLM, producing patterns that\nare easier for the model to learn and more consistent with detection criteria.\nA regularization term further simulates downstream perturbations and penalizes\ndegradation in watermark detectability, ensuring robustness under model edits.\nExperiments on open-source LLMs (e.g., LLaMA-3.2, LLaMA-3, Phi-2) show that PRO\nsubstantially improves both watermark detectability and resilience to model\nmodifications."}
{"id": "2510.23927", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.23927", "abs": "https://arxiv.org/abs/2510.23927", "authors": ["Daniel Spokoyny", "Nikolai Vogler", "Xin Gao", "Tianyi Zheng", "Yufei Weng", "Jonghyun Park", "Jiajun Jiao", "Geoffrey M. Voelker", "Stefan Savage", "Taylor Berg-Kirkpatrick"], "title": "Victim as a Service: Designing a System for Engaging with Interactive Scammers", "comment": null, "summary": "Pig butchering, and similar interactive online scams, lower their victims'\ndefenses by building trust over extended periods of conversation - sometimes\nweeks or months. They have become increasingly public losses (at least $75B by\none recent study). However, because of their long-term conversational nature,\nthey are extremely challenging to investigate at scale. In this paper, we\ndescribe the motivation, design, implementation, and experience with\nCHATTERBOX, an LLM-based system that automates long-term engagement with online\nscammers, making large-scale investigations of their tactics possible. We\ndescribe the techniques we have developed to attract scam attempts, the system\nand LLM-engineering required to convincingly engage with scammers, and the\nnecessary capabilities required to satisfy or evade \"milestones\" in scammers'\nworkflow."}
{"id": "2510.23938", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23938", "abs": "https://arxiv.org/abs/2510.23938", "authors": ["Marcin Spoczynski", "Marcela S. Melara"], "title": "Scalable GPU-Based Integrity Verification for Large Machine Learning Models", "comment": null, "summary": "We present a security framework that strengthens distributed machine learning\nby standardizing integrity protections across CPU and GPU platforms and\nsignificantly reducing verification overheads. Our approach co-locates\nintegrity verification directly with large ML model execution on GPU\naccelerators, resolving the fundamental mismatch between how large ML workloads\ntypically run (primarily on GPUs) and how security verifications traditionally\noperate (on separate CPU-based processes), delivering both immediate\nperformance benefits and long-term architectural consistency. By performing\ncryptographic operations natively on GPUs using dedicated compute units (e.g.,\nIntel Arc's XMX units, NVIDIA's Tensor Cores), our solution eliminates the\npotential architectural bottlenecks that could plague traditional CPU-based\nverification systems when dealing with large models. This approach leverages\nthe same GPU-based high-memory bandwidth and parallel processing primitives\nthat power ML workloads ensuring integrity checks keep pace with model\nexecution even for massive models exceeding 100GB. This framework establishes a\ncommon integrity verification mechanism that works consistently across\ndifferent GPU vendors and hardware configurations. By anticipating future\ncapabilities for creating secure channels between trusted execution\nenvironments and GPU accelerators, we provide a hardware-agnostic foundation\nthat enterprise teams can deploy regardless of their underlying CPU and GPU\ninfrastructures."}
{"id": "2510.24072", "categories": ["cs.CR", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.24072", "abs": "https://arxiv.org/abs/2510.24072", "authors": ["Austin Shouli", "Yulia Bobkova", "Ajay Kumar Shrestha"], "title": "Covert Surveillance in Smart Devices: A SCOUR Framework Analysis of Youth Privacy Implications", "comment": "To appear in the IEEE UEMCON 2025 proceedings", "summary": "This paper investigates how smart devices covertly capture private\nconversations and discusses in more in-depth the implications of this for youth\nprivacy. Using a structured review guided by the PRISMA methodology, the\nanalysis focuses on privacy concerns, data capture methods, data storage and\nsharing practices, and proposed technical mitigations. To structure and\nsynthesize findings, we introduce the SCOUR framework, encompassing\nSurveillance mechanisms, Consent and awareness, Operational data flow, Usage\nand exploitation, and Regulatory and technical safeguards. Findings reveal that\nsmart devices have been covertly capturing personal data, especially with smart\ntoys and voice-activated smart gadgets built for youth. These issues are\nworsened by unclear data collection practices and insufficient transparency in\nsmart device applications. Balancing privacy and utility in smart devices is\ncrucial, as youth are becoming more aware of privacy breaches and value their\npersonal data more. Strategies to improve regulatory and technical safeguards\nare also provided. The review identifies research gaps and suggests future\ndirections. The limitations of this literature review are also explained. The\nfindings have significant implications for policy development and the\ntransparency of data collection for smart devices."}
{"id": "2510.24101", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.24101", "abs": "https://arxiv.org/abs/2510.24101", "authors": ["Nam Tran", "Khoa Nguyen", "Dongxi Liu", "Josef Pieprzyk", "Willy Susilo"], "title": "Traceable Signatures from Lattices", "comment": "45 pages", "summary": "Traceable signatures (Kiayas et al., EUROCRYPT 2004) is an anonymous digital\nsignature system that extends the tracing power of the opening authority in\ngroup signatures. There are many known constructions of traceable signatures,\nbut all are based on number-theoretic/pairing assumptions. For such reason,\nthey may not be secure in the presence of quantum computers. This work revisits\nthe notion of traceable signatures and presents a lattice-based construction\nprovably secure in the quantum random oracle model (QROM)."}
{"id": "2510.24141", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.24141", "abs": "https://arxiv.org/abs/2510.24141", "authors": ["Miao Zhang", "Shenao Wang", "Guilin Zheng", "Yanjie Zhao", "Haoyu Wang"], "title": "Demystifying Cookie Sharing Risks in WebView-based Mobile App-in-app Ecosystems", "comment": "To appear in the 40th IEEE/ACM International Conference on Automated\n  Software Engineering (ASE'25)", "summary": "Mini-programs, an emerging mobile application paradigm within super-apps,\noffer a seamless and installation-free experience. However, the adoption of the\nweb-view component has disrupted their isolation mechanisms, exposing new\nattack surfaces and vulnerabilities. In this paper, we introduce a novel\nvulnerability called Cross Mini-program Cookie Sharing (CMCS), which arises\nfrom the shared web-view environment across mini-programs. This vulnerability\nallows unauthorized data exchange across mini-programs by enabling one\nmini-program to access cookies set by another within the same web-view context,\nviolating isolation principles. As a preliminary step, we analyzed the web-view\nmechanisms of four major platforms, including WeChat, AliPay, TikTok, and\nBaidu, and found that all of them are affected by CMCS vulnerabilities.\nFurthermore, we demonstrate the collusion attack enabled by CMCS, where\nprivileged mini-programs exfiltrate sensitive user data via cookies accessible\nto unprivileged mini-programs. To measure the impact of collusion attacks\nenabled by CMCS vulnerabilities in the wild, we developed MiCoScan, a static\nanalysis tool that detects mini-programs affected by CMCS vulnerabilities.\nMiCoScan employs web-view context modeling to identify clusters of\nmini-programs sharing the same web-view domain and cross-webview data flow\nanalysis to detect sensitive data transmissions to/from web-views. Using\nMiCoScan, we conducted a large-scale analysis of 351,483 mini-programs,\nidentifying 45,448 clusters sharing web-view domains, 7,965 instances of\nprivileged data transmission, and 9,877 mini-programs vulnerable to collusion\nattacks. Our findings highlight the widespread prevalence and significant\nsecurity risks posed by CMCS vulnerabilities, underscoring the urgent need for\nimproved isolation mechanisms in mini-program ecosystems."}
{"id": "2510.24317", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.24317", "abs": "https://arxiv.org/abs/2510.24317", "authors": ["María Sanz-Gómez", "Víctor Mayoral-Vilches", "Francesco Balassone", "Luis Javier Navarrete-Lozano", "Cristóbal R. J. Veas Chavez", "Maite del Mundo de Torres"], "title": "Cybersecurity AI Benchmark (CAIBench): A Meta-Benchmark for Evaluating Cybersecurity AI Agents", "comment": null, "summary": "Cybersecurity spans multiple interconnected domains, complicating the\ndevelopment of meaningful, labor-relevant benchmarks. Existing benchmarks\nassess isolated skills rather than integrated performance. We find that\npre-trained knowledge of cybersecurity in LLMs does not imply attack and\ndefense abilities, revealing a gap between knowledge and capability. To address\nthis limitation, we present the Cybersecurity AI Benchmark (CAIBench), a\nmodular meta-benchmark framework that allows evaluating LLM models and agents\nacross offensive and defensive cybersecurity domains, taking a step towards\nmeaningfully measuring their labor-relevance. CAIBench integrates five\nevaluation categories, covering over 10,000 instances: Jeopardy-style CTFs,\nAttack and Defense CTFs, Cyber Range exercises, knowledge benchmarks, and\nprivacy assessments. Key novel contributions include systematic simultaneous\noffensive-defensive evaluation, robotics-focused cybersecurity challenges\n(RCTF2), and privacy-preserving performance assessment (CyberPII-Bench).\nEvaluation of state-of-the-art AI models reveals saturation on security\nknowledge metrics (~70\\% success) but substantial degradation in multi-step\nadversarial (A\\&D) scenarios (20-40\\% success), or worse in robotic targets\n(22\\% success). The combination of framework scaffolding and LLM model choice\nsignificantly impacts performance; we find that proper matches improve up to\n2.6$\\times$ variance in Attack and Defense CTFs. These results demonstrate a\npronounced gap between conceptual knowledge and adaptive capability,\nemphasizing the need for a meta-benchmark."}
{"id": "2510.24393", "categories": ["cs.CR", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.24393", "abs": "https://arxiv.org/abs/2510.24393", "authors": ["Yan Meng", "Jiachun Li", "Matthew Pillari", "Arjun Deopujari", "Liam Brennan", "Hafsah Shamsie", "Haojin Zhu", "Yuan Tian"], "title": "Your Microphone Array Retains Your Identity: A Robust Voice Liveness Detection System for Smart Speakers", "comment": "This is a paper accepted by USENIX Security 2022. See:\n  https://www.usenix.org/conference/usenixsecurity22/presentation/meng", "summary": "Though playing an essential role in smart home systems, smart speakers are\nvulnerable to voice spoofing attacks. Passive liveness detection, which\nutilizes only the collected audio rather than the deployed sensors to\ndistinguish between live-human and replayed voices, has drawn increasing\nattention. However, it faces the challenge of performance degradation under the\ndifferent environmental factors as well as the strict requirement of the fixed\nuser gestures.\n  In this study, we propose a novel liveness feature, array fingerprint, which\nutilizes the microphone array inherently adopted by the smart speaker to\ndetermine the identity of collected audios. Our theoretical analysis\ndemonstrates that by leveraging the circular layout of microphones, compared\nwith existing schemes, array fingerprint achieves a more robust performance\nunder the environmental change and user's movement. Then, to leverage such a\nfingerprint, we propose ARRAYID, a lightweight passive detection scheme, and\nelaborate a series of features working together with array fingerprint. Our\nevaluation on the dataset containing 32,780 audio samples and 14 spoofing\ndevices shows that ARRAYID achieves an accuracy of 99.84%, which is superior to\nexisting passive liveness detection schemes."}
{"id": "2510.24408", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.24408", "abs": "https://arxiv.org/abs/2510.24408", "authors": ["Yifan Wu", "Xuewei Feng", "Yuxiang Yang", "Ke Xu"], "title": "Uncovering Gaps Between RFC Updates and TCP/IP Implementations: LLM-Facilitated Differential Checks on Intermediate Representations", "comment": "15 pages, 7 figures", "summary": "As the core of the Internet infrastructure, the TCP/IP protocol stack\nundertakes the task of network data transmission. However, due to the\ncomplexity of the protocol and the uncertainty of cross-layer interaction,\nthere are often inconsistencies between the implementation of the protocol\nstack code and the RFC standard. This inconsistency may not only lead to\ndifferences in protocol functions but also cause serious security\nvulnerabilities. At present, with the continuous expansion of protocol stack\nfunctions and the rapid iteration of RFC documents, it is increasingly\nimportant to detect and fix these inconsistencies. With the rise of large\nlanguage models, researchers have begun to explore how to extract protocol\nspecifications from RFC documents through these models, including protocol\nstack modeling, state machine extraction, text ambiguity analysis, and other\nrelated content. However, existing methods rely on predefined patterns or\nrule-based approaches that fail to generalize across different protocol\nspecifications. Automated and scalable detection of these inconsistencies\nremains a significant challenge. In this study, we propose an automated\nanalysis framework based on LLM and differential models. By modeling the\niterative relationship of the protocol and based on the iterative update\nrelationship of the RFC standard, we perform incremental code function analysis\non different versions of kernel code implementations to automatically perform\ncode detection and vulnerability analysis. We conduct extensive evaluations to\nvalidate the effectiveness of our framework, demonstrating its effectiveness in\nidentifying potential vulnerabilities caused by RFC code inconsistencies."}
{"id": "2510.24422", "categories": ["cs.CR", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24422", "abs": "https://arxiv.org/abs/2510.24422", "authors": ["Bijeet Basak", "Nupur Patil", "Kurian Polachan", "Srinivas Vivek"], "title": "Attack on a PUF-based Secure Binary Neural Network", "comment": "Accepted at VLSID 2026. To be published in IEEE Xplore", "summary": "Binarized Neural Networks (BNNs) deployed on memristive crossbar arrays\nprovide energy-efficient solutions for edge computing but are susceptible to\nphysical attacks due to memristor nonvolatility. Recently, Rajendran et al.\n(IEEE Embedded Systems Letter 2025) proposed a Physical Unclonable Function\n(PUF)-based scheme to secure BNNs against theft attacks. Specifically, the\nweight and bias matrices of the BNN layers were secured by swapping columns\nbased on device's PUF key bits.\n  In this paper, we demonstrate that this scheme to secure BNNs is vulnerable\nto PUF-key recovery attack. As a consequence of our attack, we recover the\nsecret weight and bias matrices of the BNN. Our approach is motivated by\ndifferential cryptanalysis and reconstructs the PUF key bit-by-bit by observing\nthe change in model accuracy, and eventually recovering the BNN model\nparameters. Evaluated on a BNN trained on the MNIST dataset, our attack could\nrecover 85% of the PUF key, and recover the BNN model up to 93% classification\naccuracy compared to the original model's 96% accuracy. Our attack is very\nefficient and it takes a couple of minutes to recovery the PUF key and the\nmodel parameters."}
{"id": "2510.24498", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24498", "abs": "https://arxiv.org/abs/2510.24498", "authors": ["Tejaswini Bollikonda"], "title": "Design and Optimization of Cloud Native Homomorphic Encryption Workflows for Privacy-Preserving ML Inference", "comment": "6 pages 2 figures, 2 tABLES", "summary": "As machine learning (ML) models become increasingly deployed through cloud\ninfrastructures, the confidentiality of user data during inference poses a\nsignificant security challenge. Homomorphic Encryption (HE) has emerged as a\ncompelling cryptographic technique that enables computation on encrypted data,\nallowing predictions to be generated without decrypting sensitive inputs.\nHowever, the integration of HE within large scale cloud native pipelines\nremains constrained by high computational overhead, orchestration complexity,\nand model compatibility issues.\n  This paper presents a systematic framework for the design and optimization of\ncloud native homomorphic encryption workflows that support privacy-preserving\nML inference. The proposed architecture integrates containerized HE modules\nwith Kubernetes-based orchestration, enabling elastic scaling and parallel\nencrypted computation across distributed environments. Furthermore,\noptimization strategies including ciphertext packing, polynomial modulus\nadjustment, and operator fusion are employed to minimize latency and resource\nconsumption while preserving cryptographic integrity. Experimental results\ndemonstrate that the proposed system achieves up to 3.2times inference\nacceleration and 40% reduction in memory utilization compared to conventional\nHE pipelines. These findings illustrate a practical pathway for deploying\nsecure ML-as-a-Service (MLaaS) systems that guarantee data confidentiality\nunder zero-trust cloud conditions."}
