{"id": "2511.01898", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.01898", "abs": "https://arxiv.org/abs/2511.01898", "authors": ["Hanie Vatani", "Reza Ebrahimi Atani"], "title": "FedSelect-ME: A Secure Multi-Edge Federated Learning Framework with Adaptive Client Scoring", "comment": "10 pages, 4 figures, Accepted in 6th International Conference on Soft\n  Computing (CSC2025)", "summary": "Federated Learning (FL) enables collaborative model training without sharing\nraw data but suffers from limited scalability, high communication costs, and\nprivacy risks due to its centralized architecture. This paper proposes\nFedSelect-ME, a hierarchical multi-edge FL framework that enhances scalability,\nsecurity, and energy efficiency. Multiple edge servers distribute workloads and\nperform score-based client selection, prioritizing participants based on\nutility, energy efficiency, and data sensitivity. Secure Aggregation with\nHomomorphic Encryption and Differential Privacy protects model updates from\nexposure and manipulation. Evaluated on the eICU healthcare dataset,\nFedSelect-ME achieves higher prediction accuracy, improved fairness across\nregions, and reduced communication overhead compared to FedAvg, FedProx, and\nFedSelect. The results demonstrate that the proposed framework effectively\naddresses the bottlenecks of conventional FL, offering a secure, scalable, and\nefficient solution for large-scale, privacy-sensitive healthcare applications.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.01910", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.01910", "abs": "https://arxiv.org/abs/2511.01910", "authors": ["Oisin O Sullivan"], "title": "Security Audit of intel ICE Driver for e810 Network Interface Card", "comment": "Final Year Project Report, submitted 24/03/2025 as part of Bachelor\n  of Science in Cyber Security and IT Forensics at the University Of Limerick", "summary": "The security of enterprise-grade networking hardware and software is critical\nto ensuring the integrity, availability, and confidentiality of data in modern\ncloud and data center environments. Network interface controllers (NICs) play a\npivotal role in high-performance computing and virtualization, but their\nprivileged access to system resources makes them a prime target for security\nvulnerabilities. This study presents a security analysis of the Intel ICE\ndriver using the E810 Ethernet Controller, employing static analysis, fuzz\ntesting, and timing-based side-channel evaluation to assess robustness against\nexploitation. The objective is to evaluate the drivers resilience to malformed\ninputs, identify implementation weaknesses, and determine whether timing\ndiscrepancies can be exploited for unauthorized inference of system states.\nStatic code analysis reveals that insufficient bounds checking and unsafe\nstring operations may introduce security flaws. Fuzz testing targets the Admin\nQueue, debugfs interface, and virtual function (VF) management. Interface-aware\nfuzzing and command mutation confirm strong input validation that prevents\nmemory corruption and privilege escalation under normal conditions. However,\nusing principles from KernelSnitch, the driver is found to be susceptible to\ntiming-based side-channel attacks. Execution time discrepancies in hash table\nlookups allow an unprivileged attacker to infer VF occupancy states, enabling\npotential network mapping in multi-tenant environments. Further analysis shows\ninefficiencies in Read-Copy-Update (RCU) synchronization, where missing\nsynchronization leads to stale data persistence, memory leaks, and\nout-of-memory conditions. Kernel instrumentation confirms that occupied VF\nlookups complete faster than unoccupied queries, exposing timing-based\ninformation leakage.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.01952", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01952", "abs": "https://arxiv.org/abs/2511.01952", "authors": ["Jinhua Yin", "Peiru Yang", "Chen Yang", "Huili Wang", "Zhiyang Hu", "Shangguang Wang", "Yongfeng Huang", "Tao Qi"], "title": "Black-Box Membership Inference Attack for LVLMs via Prior Knowledge-Calibrated Memory Probing", "comment": null, "summary": "Large vision-language models (LVLMs) derive their capabilities from extensive\ntraining on vast corpora of visual and textual data. Empowered by large-scale\nparameters, these models often exhibit strong memorization of their training\ndata, rendering them susceptible to membership inference attacks (MIAs).\nExisting MIA methods for LVLMs typically operate under white- or gray-box\nassumptions, by extracting likelihood-based features for the suspected data\nsamples based on the target LVLMs. However, mainstream LVLMs generally only\nexpose generated outputs while concealing internal computational features\nduring inference, limiting the applicability of these methods. In this work, we\npropose the first black-box MIA framework for LVLMs, based on a prior\nknowledge-calibrated memory probing mechanism. The core idea is to assess the\nmodel memorization of the private semantic information embedded within the\nsuspected image data, which is unlikely to be inferred from general world\nknowledge alone. We conducted extensive experiments across four LVLMs and three\ndatasets. Empirical results demonstrate that our method effectively identifies\ntraining data of LVLMs in a purely black-box setting and even achieves\nperformance comparable to gray-box and white-box methods. Further analysis\nreveals the robustness of our method against potential adversarial\nmanipulations, and the effectiveness of the methodology designs. Our code and\ndata are available at https://github.com/spmede/KCMP.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.02055", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.02055", "abs": "https://arxiv.org/abs/2511.02055", "authors": ["Sameer Wagh", "Kenneth Stibler", "Shubham Gupta", "Lacey Strahm", "Irina Bejan", "Jiahao Chen", "Dave Buckley", "Ruchi Bhatia", "Jack Bandy", "Aayush Agarwal", "Andrew Trask"], "title": "Private Map-Secure Reduce: Infrastructure for Efficient AI Data Markets", "comment": null, "summary": "The modern AI data economy centralizes power, limits innovation, and\nmisallocates value by extracting data without control, privacy, or fair\ncompensation. We introduce Private Map-Secure Reduce (PMSR), a network-native\nparadigm that transforms data economics from extractive to participatory\nthrough cryptographically enforced markets. Extending MapReduce to\ndecentralized settings, PMSR enables computation to move to the data, ensuring\nverifiable privacy, efficient price discovery, and incentive alignment.\nDemonstrations include large-scale recommender audits, privacy-preserving LLM\nensembling (87.5\\% MMLU accuracy across six models), and distributed analytics\nover hundreds of nodes. PMSR establishes a scalable, equitable, and\nprivacy-guaranteed foundation for the next generation of AI data markets.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.02083", "categories": ["cs.CR", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.02083", "abs": "https://arxiv.org/abs/2511.02083", "authors": ["Avi Bagchi", "Akhil Bhimaraju", "Moulik Choraria", "Daniel Alabi", "Lav R. Varshney"], "title": "Watermarking Discrete Diffusion Language Models", "comment": null, "summary": "Watermarking has emerged as a promising technique to track AI-generated\ncontent and differentiate it from authentic human creations. While prior work\nextensively studies watermarking for autoregressive large language models\n(LLMs) and image diffusion models, none address discrete diffusion language\nmodels, which are becoming popular due to their high inference throughput. In\nthis paper, we introduce the first watermarking method for discrete diffusion\nmodels by applying the distribution-preserving Gumbel-max trick at every\ndiffusion step and seeding the randomness with the sequence index to enable\nreliable detection. We experimentally demonstrate that our scheme is reliably\ndetectable on state-of-the-art diffusion language models and analytically prove\nthat it is distortion-free with an exponentially decaying probability of false\ndetection in the token sequence length.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.02116", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.02116", "abs": "https://arxiv.org/abs/2511.02116", "authors": ["Mary P Thomas", "Martin Kandes", "James McDougall", "Dmitry Mishan", "Scott Sakai", "Subhashini Sivagnanam", "Mahidhar Tatineni"], "title": "The SDSC Satellite Reverse Proxy Service for Launching Secure Jupyter Notebooks on High-Performance Computing Systems", "comment": "4 pages, 3 figures, 9 refereces, HPC systems application", "summary": "Using Jupyter notebooks in an HPC environment exposes a system and its users\nto several security risks. The Satellite Proxy Service, developed at SDSC,\naddresses many of these security concerns by providing Jupyter Notebook servers\nwith a token-authenticated HTTPS reverse proxy through which end users can\naccess their notebooks securely with a single URL copied and pasted into their\nweb browser.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.02176", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.02176", "abs": "https://arxiv.org/abs/2511.02176", "authors": ["Fuyi Wang", "Fangyuan Sun", "Mingyuan Fan", "Jianying Zhou", "Jin Ma", "Chao Chen", "Jiangang Shu", "Leo Yu Zhang"], "title": "FLAME: Flexible and Lightweight Biometric Authentication Scheme in Malicious Environments", "comment": "Accepted to ACSAC'25", "summary": "Privacy-preserving biometric authentication (PPBA) enables client\nauthentication without revealing sensitive biometric data, addressing privacy\nand security concerns. Many studies have proposed efficient cryptographic\nsolutions to this problem based on secure multi-party computation, typically\nassuming a semi-honest adversary model, where all parties follow the protocol\nbut may try to learn additional information. However, this assumption often\nfalls short in real-world scenarios, where adversaries may behave maliciously\nand actively deviate from the protocol.\n  In this paper, we propose, implement, and evaluate $\\sysname$, a\n\\underline{F}lexible and \\underline{L}ightweight biometric\n\\underline{A}uthentication scheme designed for a \\underline{M}alicious\n\\underline{E}nvironment. By hybridizing lightweight secret-sharing-family\nprimitives within two-party computation, $\\sysname$ carefully designs a line of\nsupporting protocols that incorporate integrity checks with rationally extra\noverhead. Additionally, $\\sysname$ enables server-side authentication with\nvarious similarity metrics through a cross-metric-compatible design, enhancing\nflexibility and robustness without requiring any changes to the server-side\nprocess. A rigorous theoretical analysis validates the correctness, security,\nand efficiency of $\\sysname$. Extensive experiments highlight $\\sysname$'s\nsuperior efficiency, with a communication reduction by {$97.61\\times \\sim\n110.13\\times$} and a speedup of {$ 2.72\\times \\sim 2.82\\times$ (resp. $\n6.58\\times \\sim 8.51\\times$)} in a LAN (resp. WAN) environment, when compared\nto the state-of-the-art work.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.02185", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02185", "abs": "https://arxiv.org/abs/2511.02185", "authors": ["Fuyi Wang", "Zekai Chen", "Mingyuan Fan", "Jianying Zhou", "Lei Pan", "Leo Yu Zhang"], "title": "PrivGNN: High-Performance Secure Inference for Cryptographic Graph Neural Networks", "comment": "Accepted to FC'25", "summary": "Graph neural networks (GNNs) are powerful tools for analyzing and learning\nfrom graph-structured (GS) data, facilitating a wide range of services.\nDeploying such services in privacy-critical cloud environments necessitates the\ndevelopment of secure inference (SI) protocols that safeguard sensitive GS\ndata. However, existing SI solutions largely focus on convolutional models for\nimage and text data, leaving the challenge of securing GNNs and GS data\nrelatively underexplored. In this work, we design, implement, and evaluate\n$\\sysname$, a lightweight cryptographic scheme for graph-centric inference in\nthe cloud. By hybridizing additive and function secret sharings within secure\ntwo-party computation (2PC), $\\sysname$ is carefully designed based on a series\nof novel 2PC interactive protocols that achieve $1.5\\times \\sim 1.7\\times$\nspeedups for linear layers and $2\\times \\sim 15\\times$ for non-linear layers\nover state-of-the-art (SotA) solutions. A thorough theoretical analysis is\nprovided to prove $\\sysname$'s correctness, security, and lightweight nature.\nExtensive experiments across four datasets demonstrate $\\sysname$'s superior\nefficiency with $1.3\\times \\sim 4.7\\times$ faster secure predictions while\nmaintaining accuracy comparable to plaintext graph property inference.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.02356", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02356", "abs": "https://arxiv.org/abs/2511.02356", "authors": ["Xu Liu", "Yan Chen", "Kan Ling", "Yichi Zhu", "Hengrun Zhang", "Guisheng Fan", "Huiqun Yu"], "title": "An Automated Framework for Strategy Discovery, Retrieval, and Evolution in LLM Jailbreak Attacks", "comment": null, "summary": "The widespread deployment of Large Language Models (LLMs) as public-facing\nweb services and APIs has made their security a core concern for the web\necosystem. Jailbreak attacks, as one of the significant threats to LLMs, have\nrecently attracted extensive research. In this paper, we reveal a jailbreak\nstrategy which can effectively evade current defense strategies. It can extract\nvaluable information from failed or partially successful attack attempts and\ncontains self-evolution from attack interactions, resulting in sufficient\nstrategy diversity and adaptability. Inspired by continuous learning and\nmodular design principles, we propose ASTRA, a jailbreak framework that\nautonomously discovers, retrieves, and evolves attack strategies to achieve\nmore efficient and adaptive attacks. To enable this autonomous evolution, we\ndesign a closed-loop \"attack-evaluate-distill-reuse\" core mechanism that not\nonly generates attack prompts but also automatically distills and generalizes\nreusable attack strategies from every interaction. To systematically accumulate\nand apply this attack knowledge, we introduce a three-tier strategy library\nthat categorizes strategies into Effective, Promising, and Ineffective based on\ntheir performance scores. The strategy library not only provides precise\nguidance for attack generation but also possesses exceptional extensibility and\ntransferability. We conduct extensive experiments under a black-box setting,\nand the results show that ASTRA achieves an average Attack Success Rate (ASR)\nof 82.7%, significantly outperforming baselines.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.02365", "categories": ["cs.CR", "math.QA", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.02365", "abs": "https://arxiv.org/abs/2511.02365", "authors": ["Gautier-Edouard Filardo", "Thibaut Heckmann"], "title": "Enhancing NTRUEncrypt Security Using Markov Chain Monte Carlo Methods: Theory and Practice", "comment": null, "summary": "This paper presents a novel framework for enhancing the quantum resistance of\nNTRUEncrypt using Markov Chain Monte Carlo (MCMC) methods. We establish formal\nbounds on sampling efficiency and provide security reductions to lattice\nproblems, bridging theoretical guarantees with practical implementations. Key\ncontributions include: a new methodology for exploring private key\nvulnerabilities while maintaining quantum resistance, provable mixing time\nbounds for high-dimensional lattices, and concrete metrics linking MCMC\nparameters to lattice hardness assumptions. Numerical experiments validate our\napproach, demonstrating improved security guarantees and computational\nefficiency. These findings advance the theoretical understanding and practical\nadoption of NTRU- Encrypt in the post-quantum era.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.02600", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02600", "abs": "https://arxiv.org/abs/2511.02600", "authors": ["Patrick Karlsen", "Even Eilertsen"], "title": "On The Dangers of Poisoned LLMs In Security Automation", "comment": "5 pages, 1 figure", "summary": "This paper investigates some of the risks introduced by \"LLM poisoning,\" the\nintentional or unintentional introduction of malicious or biased data during\nmodel training. We demonstrate how a seemingly improved LLM, fine-tuned on a\nlimited dataset, can introduce significant bias, to the extent that a simple\nLLM-based alert investigator is completely bypassed when the prompt utilizes\nthe introduced bias. Using fine-tuned Llama3.1 8B and Qwen3 4B models, we\ndemonstrate how a targeted poisoning attack can bias the model to consistently\ndismiss true positive alerts originating from a specific user. Additionally, we\npropose some mitigation and best-practices to increase trustworthiness,\nrobustness and reduce risk in applied LLMs in security applications.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.02620", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02620", "abs": "https://arxiv.org/abs/2511.02620", "authors": ["Roy Rinberg", "Adam Karvonen", "Alex Hoover", "Daniel Reuter", "Keri Warr"], "title": "Verifying LLM Inference to Prevent Model Weight Exfiltration", "comment": null, "summary": "As large AI models become increasingly valuable assets, the risk of model\nweight exfiltration from inference servers grows accordingly. An attacker\ncontrolling an inference server may exfiltrate model weights by hiding them\nwithin ordinary model outputs, a strategy known as steganography. This work\ninvestigates how to verify model responses to defend against such attacks and,\nmore broadly, to detect anomalous or buggy behavior during inference. We\nformalize model exfiltration as a security game, propose a verification\nframework that can provably mitigate steganographic exfiltration, and specify\nthe trust assumptions associated with our scheme. To enable verification, we\ncharacterize valid sources of non-determinism in large language model inference\nand introduce two practical estimators for them. We evaluate our detection\nframework on several open-weight models ranging from 3B to 30B parameters. On\nMOE-Qwen-30B, our detector reduces exfiltratable information to <0.5% with\nfalse-positive rate of 0.01%, corresponding to a >200x slowdown for\nadversaries. Overall, this work further establishes a foundation for defending\nagainst model weight exfiltration and demonstrates that strong protection can\nbe achieved with minimal additional cost to inference providers.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.02656", "categories": ["cs.CR", "C.2.4; D.4.6; H.2.0; H.3.3"], "pdf": "https://arxiv.org/pdf/2511.02656", "abs": "https://arxiv.org/abs/2511.02656", "authors": ["Artur Iasenovets", "Fei Tang", "Huihui Zhu", "Ping Wang", "Lei Liu"], "title": "Bringing Private Reads to Hyperledger Fabric via Private Information Retrieval", "comment": "This work has been submitted to IEEE for possible publication", "summary": "Permissioned blockchains ensure integrity and auditability of shared data but\nexpose query parameters to peers during read operations, creating privacy risks\nfor organizations querying sensitive records. This paper proposes a Private\nInformation Retrieval (PIR) mechanism to enable private reads from Hyperledger\nFabric's world state, allowing endorsing peers to process encrypted queries\nwithout learning which record is accessed. We implement and benchmark a\nPIR-enabled chaincode that performs ciphertext-plaintext (ct-pt) homomorphic\nmultiplication directly within evaluate transactions, preserving Fabric's\nendorsement and audit semantics. The prototype achieves an average end-to-end\nlatency of 113 ms and a peer-side execution time below 42 ms, with\napproximately 2 MB of peer network traffic per private read in development\nmode--reducible by half under in-process deployment. Storage profiling across\nthree channel configurations shows near-linear growth: block size increases\nfrom 77 kilobytes to 294 kilobytes and world-state from 112 kilobytes to 332\nkilobytes as the ring dimension scales from 8,192 to 32,768 coefficients.\nParameter analysis further indicates that ring size and record length jointly\nconstrain packing capacity, supporting up to 512 records of 64 bytes each under\nthe largest configuration. These results confirm the practicality of PIR-based\nprivate reads in Fabric for smaller, sensitive datasets and highlight future\ndirections to optimize performance and scalability.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.02780", "categories": ["cs.CR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02780", "abs": "https://arxiv.org/abs/2511.02780", "authors": ["Vivi Andersson", "Sofia Bobadilla", "Harald Hobbelhagen", "Martin Monperrus"], "title": "1 PoCo: Agentic Proof-of-Concept Exploit Generation for Smart Contracts", "comment": "Under review", "summary": "Smart contracts operate in a highly adversarial environment, where\nvulnerabilities can lead to substantial financial losses. Thus, smart contracts\nare subject to security audits. In auditing, proof-of-concept (PoC) exploits\nplay a critical role by demonstrating to the stakeholders that the reported\nvulnerabilities are genuine, reproducible, and actionable. However, manually\ncreating PoCs is time-consuming, error-prone, and often constrained by tight\naudit schedules. We introduce POCO, an agentic framework that automatically\ngenerates executable PoC exploits from natural-language vulnerability\ndescriptions written by auditors. POCO autonomously generates PoC exploits in\nan agentic manner by interacting with a set of code-execution tools in a\nReason-Act-Observe loop. It produces fully executable exploits compatible with\nthe Foundry testing framework, ready for integration into audit reports and\nother security tools. We evaluate POCO on a dataset of 23 real-world\nvulnerability reports. POCO consistently outperforms the prompting and workflow\nbaselines, generating well-formed and logically correct PoCs. Our results\ndemonstrate that agentic frameworks can significantly reduce the effort\nrequired for high-quality PoCs in smart contract audits. Our contribution\nprovides readily actionable knowledge for the smart contract security\ncommunity.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
