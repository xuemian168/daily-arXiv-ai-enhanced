{"id": "2510.24807", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24807", "abs": "https://arxiv.org/abs/2510.24807", "authors": ["Ziyao Cui", "Minxing Zhang", "Jian Pei"], "title": "Learning to Attack: Uncovering Privacy Risks in Sequential Data Releases", "comment": null, "summary": "Privacy concerns have become increasingly critical in modern AI and data\nscience applications, where sensitive information is collected, analyzed, and\nshared across diverse domains such as healthcare, finance, and mobility. While\nprior research has focused on protecting privacy in a single data release, many\nreal-world systems operate under sequential or continuous data publishing,\nwhere the same or related data are released over time. Such sequential\ndisclosures introduce new vulnerabilities, as temporal correlations across\nreleases may enable adversaries to infer sensitive information that remains\nhidden in any individual release. In this paper, we investigate whether an\nattacker can compromise privacy in sequential data releases by exploiting\ndependencies between consecutive publications, even when each individual\nrelease satisfies standard privacy guarantees. To this end, we propose a novel\nattack model that captures these sequential dependencies by integrating a\nHidden Markov Model with a reinforcement learning-based bi-directional\ninference mechanism. This enables the attacker to leverage both earlier and\nlater observations in the sequence to infer private information. We instantiate\nour framework in the context of trajectory data, demonstrating how an adversary\ncan recover sensitive locations from sequential mobility datasets. Extensive\nexperiments on Geolife, Porto Taxi, and SynMob datasets show that our model\nconsistently outperforms baseline approaches that treat each release\nindependently. The results reveal a fundamental privacy risk inherent to\nsequential data publishing, where individually protected releases can\ncollectively leak sensitive information when analyzed temporally. These\nfindings underscore the need for new privacy-preserving frameworks that\nexplicitly model temporal dependencies, such as time-aware differential privacy\nor sequential data obfuscation strategies.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.24920", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.24920", "abs": "https://arxiv.org/abs/2510.24920", "authors": ["Elizabeth Lin", "Jonah Ghebremichael", "William Enck", "Yasemin Acar", "Michel Cukier", "Alexandros Kapravelos", "Christian Kastner", "Laurie Williams"], "title": "S3C2 Summit 2025-03: Industry Secure Supply Chain Summit", "comment": null, "summary": "Software supply chains, while providing immense economic and software\ndevelopment value, are only as strong as their weakest link. Over the past\nseveral years, there has been an exponential increase in cyberattacks\nspecifically targeting vulnerable links in critical software supply chains.\nThese attacks disrupt the day-to-day functioning and threaten the security of\nnearly everyone on the internet, from billion-dollar companies and government\nagencies to hobbyist open-source developers. The ever-evolving threat of\nsoftware supply chain attacks has garnered interest from both the software\nindustry and US government in improving software supply chain security. On\nThursday, March 6th, 2025, four researchers from the NSF-backed Secure Software\nSupply Chain Center (S3C2) conducted a Secure Software Supply Chain Summit with\na diverse set of 18 practitioners from 17 organizations. The goals of the\nSummit were: (1) to enable sharing between participants from different\nindustries regarding practical experiences and challenges with software supply\nchain security; (2) to help form new collaborations; and (3) to learn about the\nchallenges facing participants to inform our future research directions. The\nsummit consisted of discussions of six topics relevant to the government\nagencies represented, including software bill of materials (SBOMs); compliance;\nmalicious commits; build infrastructure; culture; and large language models\n(LLMs) and security. For each topic of discussion, we presented a list of\nquestions to participants to spark conversation. In this report, we provide a\nsummary of the summit. The open questions and challenges that remained after\neach topic are listed at the end of each topic's section, and the initial\ndiscussion questions for each topic are provided in the appendix.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.24976", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24976", "abs": "https://arxiv.org/abs/2510.24976", "authors": ["Banafsheh Saber Latibari", "Najmeh Nazari", "Hossein Sayadi", "Houman Homayoun", "Abhijit Mahalanobis"], "title": "Hammering the Diagnosis: Rowhammer-Induced Stealthy Trojan Attacks on ViT-Based Medical Imaging", "comment": "Accepted, ICCD 2025", "summary": "Vision Transformers (ViTs) have emerged as powerful architectures in medical\nimage analysis, excelling in tasks such as disease detection, segmentation, and\nclassification. However, their reliance on large, attention-driven models makes\nthem vulnerable to hardware-level attacks. In this paper, we propose a novel\nthreat model referred to as Med-Hammer that combines the Rowhammer hardware\nfault injection with neural Trojan attacks to compromise the integrity of\nViT-based medical imaging systems. Specifically, we demonstrate how malicious\nbit flips induced via Rowhammer can trigger implanted neural Trojans, leading\nto targeted misclassification or suppression of critical diagnoses (e.g.,\ntumors or lesions) in medical scans. Through extensive experiments on benchmark\nmedical imaging datasets such as ISIC, Brain Tumor, and MedMNIST, we show that\nsuch attacks can remain stealthy while achieving high attack success rates\nabout 82.51% and 92.56% in MobileViT and SwinTransformer, respectively. We\nfurther investigate how architectural properties, such as model sparsity,\nattention weight distribution, and the number of features of the layer, impact\nattack effectiveness. Our findings highlight a critical and underexplored\nintersection between hardware-level faults and deep learning security in\nhealthcare applications, underscoring the urgent need for robust defenses\nspanning both model architectures and underlying hardware platforms.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.24985", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24985", "abs": "https://arxiv.org/abs/2510.24985", "authors": ["Najmeh Nazari", "Banafsheh Saber Latibari", "Elahe Hosseini", "Fatemeh Movafagh", "Chongzhou Fang", "Hosein Mohammadi Makrani", "Kevin Immanuel Gubbi", "Abhijit Mahalanobis", "Setareh Rafatirad", "Hossein Sayadi", "Houman Homayoun"], "title": "FaRAccel: FPGA-Accelerated Defense Architecture for Efficient Bit-Flip Attack Resilience in Transformer Models", "comment": "Accepted By ICCD 2025", "summary": "Forget and Rewire (FaR) methodology has demonstrated strong resilience\nagainst Bit-Flip Attacks (BFAs) on Transformer-based models by obfuscating\ncritical parameters through dynamic rewiring of linear layers. However, the\napplication of FaR introduces non-negligible performance and memory overheads,\nprimarily due to the runtime modification of activation pathways and the lack\nof hardware-level optimization. To overcome these limitations, we propose\nFaRAccel, a novel hardware accelerator architecture implemented on FPGA,\nspecifically designed to offload and optimize FaR operations. FaRAccel\nintegrates reconfigurable logic for dynamic activation rerouting, and\nlightweight storage of rewiring configurations, enabling low-latency inference\nwith minimal energy overhead. We evaluate FaRAccel across a suite of\nTransformer models and demonstrate substantial reductions in FaR inference\nlatency and improvement in energy efficiency, while maintaining the robustness\ngains of the original FaR methodology. To the best of our knowledge, this is\nthe first hardware-accelerated defense against BFAs in Transformers,\neffectively bridging the gap between algorithmic resilience and efficient\ndeployment on real-world AI platforms.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.24999", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.24999", "abs": "https://arxiv.org/abs/2510.24999", "authors": ["Racchit Jain", "Satya Lokam", "Yehonathan Refael", "Adam Hakim", "Lev Greenberg", "Jay Tenenbaum"], "title": "SLIP-SEC: Formalizing Secure Protocols for Model IP Protection", "comment": null, "summary": "Large Language Models (LLMs) represent valuable intellectual property (IP),\nreflecting significant investments in training data, compute, and expertise.\nDeploying these models on partially trusted or insecure devices introduces\nsubstantial risk of model theft, making it essential to design inference\nprotocols with provable security guarantees.\n  We present the formal framework and security foundations of SLIP, a hybrid\ninference protocol that splits model computation between a trusted and an\nuntrusted resource. We define and analyze the key notions of model\ndecomposition and hybrid inference protocols, and introduce formal properties\nincluding safety, correctness, efficiency, and t-soundness. We construct secure\ninference protocols based on additive decompositions of weight matrices,\ncombined with masking and probabilistic verification techniques. We prove that\nthese protocols achieve information-theoretic security against\nhonest-but-curious adversaries, and provide robustness against malicious\nadversaries with negligible soundness error.\n  This paper focuses on the theoretical underpinnings of SLIP: precise\ndefinitions, formal protocols, and proofs of security. Empirical validation and\ndecomposition heuristics appear in the companion SLIP paper. Together, the two\nworks provide a complete account of securing LLM IP via hybrid inference,\nbridging both practice and theory.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.25025", "categories": ["cs.CR", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25025", "abs": "https://arxiv.org/abs/2510.25025", "authors": ["Zirui Cheng", "Jikai Sun", "Anjun Gao", "Yueyang Quan", "Zhuqing Liu", "Xiaohua Hu", "Minghong Fang"], "title": "Secure Retrieval-Augmented Generation against Poisoning Attacks", "comment": "To appear in IEEE BigData 2025", "summary": "Large language models (LLMs) have transformed natural language processing\n(NLP), enabling applications from content generation to decision support.\nRetrieval-Augmented Generation (RAG) improves LLMs by incorporating external\nknowledge but also introduces security risks, particularly from data poisoning,\nwhere the attacker injects poisoned texts into the knowledge database to\nmanipulate system outputs. While various defenses have been proposed, they\noften struggle against advanced attacks. To address this, we introduce RAGuard,\na detection framework designed to identify poisoned texts. RAGuard first\nexpands the retrieval scope to increase the proportion of clean texts, reducing\nthe likelihood of retrieving poisoned content. It then applies chunk-wise\nperplexity filtering to detect abnormal variations and text similarity\nfiltering to flag highly similar texts. This non-parametric approach enhances\nRAG security, and experiments on large-scale datasets demonstrate its\neffectiveness in detecting and mitigating poisoning attacks, including strong\nadaptive attacks.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.25189", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.25189", "abs": "https://arxiv.org/abs/2510.25189", "authors": ["Ana M. Rodriguez", "Jaime Acosta", "Anantaa Kotal", "Aritran Piplai"], "title": "AgentCyTE: Leveraging Agentic AI to Generate Cybersecurity Training & Experimentation Scenarios", "comment": null, "summary": "Designing realistic and adaptive networked threat scenarios remains a core\nchallenge in cybersecurity research and training, still requiring substantial\nmanual effort. While large language models (LLMs) show promise for automated\nsynthesis, unconstrained generation often yields configurations that fail\nvalidation or execution. We present AgentCyTE, a framework integrating\nLLM-based reasoning with deterministic, schema-constrained network emulation to\ngenerate and refine executable threat environments. Through an agentic feedback\nloop, AgentCyTE observes scenario outcomes, validates correctness, and\niteratively enhances realism and consistency. This hybrid approach preserves\nLLM flexibility while enforcing structural validity, enabling scalable,\ndata-driven experimentation and reliable scenario generation for threat\nmodeling and adaptive cybersecurity training. Our framework can be accessed at:\nhttps://github.com/AnantaaKotal/AgentCyTE", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.25352", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.25352", "abs": "https://arxiv.org/abs/2510.25352", "authors": ["David Plonka", "Branden Palacio", "Debbie Perouli"], "title": "Is Protective DNS Blocking the Wild West?", "comment": "Presented in ACM IMC 2025 Workshop of Policy-Relevant Internet\n  Measurements and Experimentation (PRIME), Madison, WI, October, 2025", "summary": "We perform a passive measurement study investigating how a Protective DNS\nservice might perform in a Research & Education Network serving hundreds of\nmember institutions. Utilizing freely-available DNS blocklists consisting of\ndomain names deemed to be threats, we test hundreds of millions of users' real\nDNS queries, observed over a week's time, to find which answers would be\nblocked because they involve domain names that are potential threats. We find\nthe blocklists disorderly regarding their names, goals, transparency, and\nprovenance making them quite difficult to compare. Consequently, these\nProtective DNS underpinnings lack organized oversight, presenting challenges\nand risks in operation at scale.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.25375", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.25375", "abs": "https://arxiv.org/abs/2510.25375", "authors": ["Ali Recai Yekta", "Nicolas Loza", "Jens Gramm", "Michael Peter Schneider", "Stefan Katzenbeisser"], "title": "From ECU to VSOC: UDS Security Monitoring Strategies", "comment": "Presented at SECURWARE 2025, Barcelona, Spain, October 26-30, 2025\n  (https://www.thinkmind.org/library/SECURWARE/SECURWARE_2025/securware_2025_1_70_30030.html)", "summary": "Increasing complexity and connectivity of modern vehicles have heightened\ntheir vulnerability to cyberattacks. This paper addresses security challenges\nassociated with the Unified Diagnostic Services (UDS) protocol, a critical\ncommunication framework for vehicle diagnostics in the automotive industry. We\npresent security monitoring strategies for the UDS protocol that leverage\nin-vehicle logging and remote analysis through a Vehicle Security Operations\nCenter (VSOC). Our approach involves specifying security event logging\nrequirements, contextual data collection, and the development of detection\nstrategies aimed at identifying UDS attack scenarios. By applying these\nstrategies to a comprehensive taxonomy of UDS attack techniques, we demonstrate\nthat our detection methods cover a wide range of potential attack vectors.\nFurthermore, we assess the adequacy of current AUTOSAR standardized security\nevents in supporting UDS attack detection, identifying gaps in the current\nstandard. This work enhances the understanding of vehicle security monitoring\nand provides an example for developing robust cybersecurity measures in\nautomotive communication protocols.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.25470", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25470", "abs": "https://arxiv.org/abs/2510.25470", "authors": ["Parick Ozoh", "John K Omoniyi", "Bukola Ibitoye"], "title": "An In-Depth Analysis of Cyber Attacks in Secured Platforms", "comment": null, "summary": "There is an increase in global malware threats. To address this, an\nencryption-type ransomware has been introduced on the Android operating system.\nThe challenges associated with malicious threats in phone use have become a\npressing issue in mobile communication, disrupting user experiences and posing\nsignificant privacy threats. This study surveys commonly used machine learning\ntechniques for detecting malicious threats in phones and examines their\nperformance. The majority of past research focuses on customer feedback and\nreviews, with concerns that people might create false reviews to promote or\ndevalue products and services for personal gain. Hence, the development of\ntechniques for detecting malicious threats using machine learning has been a\nkey focus. This paper presents a comprehensive comparative study of current\nresearch on the issue of malicious threats and methods for tackling these\nchallenges. Nevertheless, a huge amount of information is required by these\nmethods, presenting a challenge for developing robust, specialized automated\nanti-malware systems. This research describes the Android Applications dataset,\nand the accuracy of the techniques is measured using the accuracy levels of the\nmetrics employed in this study.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.25472", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.25472", "abs": "https://arxiv.org/abs/2510.25472", "authors": ["Zheng Zhang", "Guanlong Wu", "Sen Deng", "Shuai Wang", "Yinqian Zhang"], "title": "NetEcho: From Real-World Streaming Side-Channels to Full LLM Conversation Recovery", "comment": null, "summary": "In the rapidly expanding landscape of Large Language Model (LLM)\napplications, real-time output streaming has become the dominant interaction\nparadigm. While this enhances user experience, recent research reveals that it\nexposes a non-trivial attack surface through network side-channels. Adversaries\ncan exploit patterns in encrypted traffic to infer sensitive information and\nreconstruct private conversations. In response, LLM providers and third-party\nservices are deploying defenses such as traffic padding and obfuscation to\nmitigate these vulnerabilities.\n  This paper starts by presenting a systematic analysis of contemporary\nside-channel defenses in mainstream LLM applications, with a focus on services\nfrom vendors like OpenAI and DeepSeek. We identify and examine seven\nrepresentative deployment scenarios, each incorporating active/passive\nmitigation techniques. Despite these enhanced security measures, our\ninvestigation uncovers significant residual information that remains vulnerable\nto leakage within the network traffic.\n  Building on this discovery, we introduce NetEcho, a novel, LLM-based\nframework that comprehensively unleashes the network side-channel risks of\ntoday's LLM applications. NetEcho is designed to recover entire conversations\n-- including both user prompts and LLM responses -- directly from encrypted\nnetwork traffic. It features a deliberate design that ensures high-fidelity\ntext recovery, transferability across different deployment scenarios, and\nmoderate operational cost. In our evaluations on medical and legal applications\nbuilt upon leading models like DeepSeek-v3 and GPT-4o, NetEcho can recover avg\n$\\sim$70\\% information of each conversation, demonstrating a critical\nlimitation in current defense mechanisms. We conclude by discussing the\nimplications of our findings and proposing future directions for augmenting\nnetwork traffic security.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.25477", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.25477", "abs": "https://arxiv.org/abs/2510.25477", "authors": ["Yi Chen", "Bin Chen", "Peichang Zhang", "Da Che"], "title": "A Study on Privacy-Preserving Scholarship Evaluation Based on Decentralized Identity and Zero-Knowledge Proofs", "comment": null, "summary": "Traditional centralized scholarship evaluation processes typically require\nstudents to submit detailed academic records and qualification information,\nwhich exposes them to risks of data leakage and misuse, making it difficult to\nsimultaneously ensure privacy protection and transparent auditability. To\naddress these challenges, this paper proposes a scholarship evaluation system\nbased on Decentralized Identity (DID) and Zero-Knowledge Proofs (ZKP). The\nsystem aggregates multidimensional ZKPs off-chain, and smart contracts verify\ncompliance with evaluation criteria without revealing raw scores or\ncomputational details. Experimental results demonstrate that the proposed\nsolution not only automates the evaluation efficiently but also maximally\npreserves student privacy and data integrity, offering a practical and\ntrustworthy technical paradigm for higher education scholarship programs.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.25677", "categories": ["cs.CR", "cs.CL", "C.2.1; D.4.6; E.3; I.2.6; I.5.4"], "pdf": "https://arxiv.org/pdf/2510.25677", "abs": "https://arxiv.org/abs/2510.25677", "authors": ["Hasan Akgul", "Mari Eplik", "Javier Rojas", "Aina Binti Abdullah", "Pieter van der Merwe"], "title": "ZK-SenseLM: Verifiable Large-Model Wireless Sensing with Selective Abstention and Zero-Knowledge Attestation", "comment": "45 pages", "summary": "ZK-SenseLM is a secure and auditable wireless sensing framework that pairs a\nlarge-model encoder for Wi-Fi channel state information (and optionally mmWave\nradar or RFID) with a policy-grounded decision layer and end-to-end\nzero-knowledge proofs of inference. The encoder uses masked spectral\npretraining with phase-consistency regularization, plus a light cross-modal\nalignment that ties RF features to compact, human-interpretable policy tokens.\nTo reduce unsafe actions under distribution shift, we add a calibrated\nselective-abstention head; the chosen risk-coverage operating point is\nregistered and bound into the proof. We implement a four-stage proving\npipeline: (C1) feature sanity and commitment, (C2) threshold and version\nbinding, (C3) time-window binding, and (C4) PLONK-style proofs that the\nquantized network, given the committed window, produced the logged action and\nconfidence. Micro-batched proving amortizes cost across adjacent windows, and a\ngateway option offloads proofs from low-power devices. The system integrates\nwith differentially private federated learning and on-device personalization\nwithout weakening verifiability: model hashes and the registered threshold are\npart of each public statement. Across activity, presence or intrusion,\nrespiratory proxy, and RF fingerprinting tasks, ZK-SenseLM improves macro-F1\nand calibration, yields favorable coverage-risk curves under perturbations, and\nrejects tamper and replay with compact proofs and fast verification.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.25687", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25687", "abs": "https://arxiv.org/abs/2510.25687", "authors": ["Mallika Prabhakar", "Louise Xu", "Prateek Saxena"], "title": "Model Inversion Attacks Meet Cryptographic Fuzzy Extractors", "comment": null, "summary": "Model inversion attacks pose an open challenge to privacy-sensitive\napplications that use machine learning (ML) models. For example, face\nauthentication systems use modern ML models to compute embedding vectors from\nface images of the enrolled users and store them. If leaked, inversion attacks\ncan accurately reconstruct user faces from the leaked vectors. There is no\nsystematic characterization of properties needed in an ideal defense against\nmodel inversion, even for the canonical example application of a face\nauthentication system susceptible to data breaches, despite a decade of\nbest-effort solutions.\n  In this paper, we formalize the desired properties of a provably strong\ndefense against model inversion and connect it, for the first time, to the\ncryptographic concept of fuzzy extractors. We further show that existing fuzzy\nextractors are insecure for use in ML-based face authentication. We do so\nthrough a new model inversion attack called PIPE, which achieves a success rate\nof over 89% in most cases against prior schemes. We then propose L2FE-Hash, the\nfirst candidate fuzzy extractor which supports standard Euclidean distance\ncomparators as needed in many ML-based applications, including face\nauthentication. We formally characterize its computational security guarantees,\neven in the extreme threat model of full breach of stored secrets, and\nempirically show its usable accuracy in face authentication for practical face\ndistributions. It offers attack-agnostic security without requiring any\nre-training of the ML model it protects. Empirically, it nullifies both prior\nstate-of-the-art inversion attacks as well as our new PIPE attack.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.25746", "categories": ["cs.CR", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.25746", "abs": "https://arxiv.org/abs/2510.25746", "authors": ["Charlie Harrison", "Pasin Manurangsi"], "title": "Exact zCDP Characterizations for Fundamental Differentially Private Mechanisms", "comment": null, "summary": "Zero-concentrated differential privacy (zCDP) is a variant of differential\nprivacy (DP) that is widely used partly thanks to its nice composition\nproperty. While a tight conversion from $\\epsilon$-DP to zCDP exists for the\nworst-case mechanism, many common algorithms satisfy stronger guarantees. In\nthis work, we derive tight zCDP characterizations for several fundamental\nmechanisms. We prove that the tight zCDP bound for the $\\epsilon$-DP Laplace\nmechanism is exactly $\\epsilon + e^{-\\epsilon} - 1$, confirming a recent\nconjecture by Wang (2022). We further provide tight bounds for the discrete\nLaplace mechanism, $k$-Randomized Response (for $k \\leq 6$), and RAPPOR.\nLastly, we also provide a tight zCDP bound for the worst case bounded range\nmechanism.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
