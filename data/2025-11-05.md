<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 14]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [FedSelect-ME: A Secure Multi-Edge Federated Learning Framework with Adaptive Client Scoring](https://arxiv.org/abs/2511.01898)
*Hanie Vatani,Reza Ebrahimi Atani*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Federated Learning (FL) enables collaborative model training without sharing
raw data but suffers from limited scalability, high communication costs, and
privacy risks due to its centralized architecture. This paper proposes
FedSelect-ME, a hierarchical multi-edge FL framework that enhances scalability,
security, and energy efficiency. Multiple edge servers distribute workloads and
perform score-based client selection, prioritizing participants based on
utility, energy efficiency, and data sensitivity. Secure Aggregation with
Homomorphic Encryption and Differential Privacy protects model updates from
exposure and manipulation. Evaluated on the eICU healthcare dataset,
FedSelect-ME achieves higher prediction accuracy, improved fairness across
regions, and reduced communication overhead compared to FedAvg, FedProx, and
FedSelect. The results demonstrate that the proposed framework effectively
addresses the bottlenecks of conventional FL, offering a secure, scalable, and
efficient solution for large-scale, privacy-sensitive healthcare applications.

</details>


### [2] [Security Audit of intel ICE Driver for e810 Network Interface Card](https://arxiv.org/abs/2511.01910)
*Oisin O Sullivan*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The security of enterprise-grade networking hardware and software is critical
to ensuring the integrity, availability, and confidentiality of data in modern
cloud and data center environments. Network interface controllers (NICs) play a
pivotal role in high-performance computing and virtualization, but their
privileged access to system resources makes them a prime target for security
vulnerabilities. This study presents a security analysis of the Intel ICE
driver using the E810 Ethernet Controller, employing static analysis, fuzz
testing, and timing-based side-channel evaluation to assess robustness against
exploitation. The objective is to evaluate the drivers resilience to malformed
inputs, identify implementation weaknesses, and determine whether timing
discrepancies can be exploited for unauthorized inference of system states.
Static code analysis reveals that insufficient bounds checking and unsafe
string operations may introduce security flaws. Fuzz testing targets the Admin
Queue, debugfs interface, and virtual function (VF) management. Interface-aware
fuzzing and command mutation confirm strong input validation that prevents
memory corruption and privilege escalation under normal conditions. However,
using principles from KernelSnitch, the driver is found to be susceptible to
timing-based side-channel attacks. Execution time discrepancies in hash table
lookups allow an unprivileged attacker to infer VF occupancy states, enabling
potential network mapping in multi-tenant environments. Further analysis shows
inefficiencies in Read-Copy-Update (RCU) synchronization, where missing
synchronization leads to stale data persistence, memory leaks, and
out-of-memory conditions. Kernel instrumentation confirms that occupied VF
lookups complete faster than unoccupied queries, exposing timing-based
information leakage.

</details>


### [3] [Black-Box Membership Inference Attack for LVLMs via Prior Knowledge-Calibrated Memory Probing](https://arxiv.org/abs/2511.01952)
*Jinhua Yin,Peiru Yang,Chen Yang,Huili Wang,Zhiyang Hu,Shangguang Wang,Yongfeng Huang,Tao Qi*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large vision-language models (LVLMs) derive their capabilities from extensive
training on vast corpora of visual and textual data. Empowered by large-scale
parameters, these models often exhibit strong memorization of their training
data, rendering them susceptible to membership inference attacks (MIAs).
Existing MIA methods for LVLMs typically operate under white- or gray-box
assumptions, by extracting likelihood-based features for the suspected data
samples based on the target LVLMs. However, mainstream LVLMs generally only
expose generated outputs while concealing internal computational features
during inference, limiting the applicability of these methods. In this work, we
propose the first black-box MIA framework for LVLMs, based on a prior
knowledge-calibrated memory probing mechanism. The core idea is to assess the
model memorization of the private semantic information embedded within the
suspected image data, which is unlikely to be inferred from general world
knowledge alone. We conducted extensive experiments across four LVLMs and three
datasets. Empirical results demonstrate that our method effectively identifies
training data of LVLMs in a purely black-box setting and even achieves
performance comparable to gray-box and white-box methods. Further analysis
reveals the robustness of our method against potential adversarial
manipulations, and the effectiveness of the methodology designs. Our code and
data are available at https://github.com/spmede/KCMP.

</details>


### [4] [Private Map-Secure Reduce: Infrastructure for Efficient AI Data Markets](https://arxiv.org/abs/2511.02055)
*Sameer Wagh,Kenneth Stibler,Shubham Gupta,Lacey Strahm,Irina Bejan,Jiahao Chen,Dave Buckley,Ruchi Bhatia,Jack Bandy,Aayush Agarwal,Andrew Trask*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The modern AI data economy centralizes power, limits innovation, and
misallocates value by extracting data without control, privacy, or fair
compensation. We introduce Private Map-Secure Reduce (PMSR), a network-native
paradigm that transforms data economics from extractive to participatory
through cryptographically enforced markets. Extending MapReduce to
decentralized settings, PMSR enables computation to move to the data, ensuring
verifiable privacy, efficient price discovery, and incentive alignment.
Demonstrations include large-scale recommender audits, privacy-preserving LLM
ensembling (87.5\% MMLU accuracy across six models), and distributed analytics
over hundreds of nodes. PMSR establishes a scalable, equitable, and
privacy-guaranteed foundation for the next generation of AI data markets.

</details>


### [5] [Watermarking Discrete Diffusion Language Models](https://arxiv.org/abs/2511.02083)
*Avi Bagchi,Akhil Bhimaraju,Moulik Choraria,Daniel Alabi,Lav R. Varshney*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Watermarking has emerged as a promising technique to track AI-generated
content and differentiate it from authentic human creations. While prior work
extensively studies watermarking for autoregressive large language models
(LLMs) and image diffusion models, none address discrete diffusion language
models, which are becoming popular due to their high inference throughput. In
this paper, we introduce the first watermarking method for discrete diffusion
models by applying the distribution-preserving Gumbel-max trick at every
diffusion step and seeding the randomness with the sequence index to enable
reliable detection. We experimentally demonstrate that our scheme is reliably
detectable on state-of-the-art diffusion language models and analytically prove
that it is distortion-free with an exponentially decaying probability of false
detection in the token sequence length.

</details>


### [6] [The SDSC Satellite Reverse Proxy Service for Launching Secure Jupyter Notebooks on High-Performance Computing Systems](https://arxiv.org/abs/2511.02116)
*Mary P Thomas,Martin Kandes,James McDougall,Dmitry Mishan,Scott Sakai,Subhashini Sivagnanam,Mahidhar Tatineni*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Using Jupyter notebooks in an HPC environment exposes a system and its users
to several security risks. The Satellite Proxy Service, developed at SDSC,
addresses many of these security concerns by providing Jupyter Notebook servers
with a token-authenticated HTTPS reverse proxy through which end users can
access their notebooks securely with a single URL copied and pasted into their
web browser.

</details>


### [7] [FLAME: Flexible and Lightweight Biometric Authentication Scheme in Malicious Environments](https://arxiv.org/abs/2511.02176)
*Fuyi Wang,Fangyuan Sun,Mingyuan Fan,Jianying Zhou,Jin Ma,Chao Chen,Jiangang Shu,Leo Yu Zhang*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Privacy-preserving biometric authentication (PPBA) enables client
authentication without revealing sensitive biometric data, addressing privacy
and security concerns. Many studies have proposed efficient cryptographic
solutions to this problem based on secure multi-party computation, typically
assuming a semi-honest adversary model, where all parties follow the protocol
but may try to learn additional information. However, this assumption often
falls short in real-world scenarios, where adversaries may behave maliciously
and actively deviate from the protocol.
  In this paper, we propose, implement, and evaluate $\sysname$, a
\underline{F}lexible and \underline{L}ightweight biometric
\underline{A}uthentication scheme designed for a \underline{M}alicious
\underline{E}nvironment. By hybridizing lightweight secret-sharing-family
primitives within two-party computation, $\sysname$ carefully designs a line of
supporting protocols that incorporate integrity checks with rationally extra
overhead. Additionally, $\sysname$ enables server-side authentication with
various similarity metrics through a cross-metric-compatible design, enhancing
flexibility and robustness without requiring any changes to the server-side
process. A rigorous theoretical analysis validates the correctness, security,
and efficiency of $\sysname$. Extensive experiments highlight $\sysname$'s
superior efficiency, with a communication reduction by {$97.61\times \sim
110.13\times$} and a speedup of {$ 2.72\times \sim 2.82\times$ (resp. $
6.58\times \sim 8.51\times$)} in a LAN (resp. WAN) environment, when compared
to the state-of-the-art work.

</details>


### [8] [PrivGNN: High-Performance Secure Inference for Cryptographic Graph Neural Networks](https://arxiv.org/abs/2511.02185)
*Fuyi Wang,Zekai Chen,Mingyuan Fan,Jianying Zhou,Lei Pan,Leo Yu Zhang*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Graph neural networks (GNNs) are powerful tools for analyzing and learning
from graph-structured (GS) data, facilitating a wide range of services.
Deploying such services in privacy-critical cloud environments necessitates the
development of secure inference (SI) protocols that safeguard sensitive GS
data. However, existing SI solutions largely focus on convolutional models for
image and text data, leaving the challenge of securing GNNs and GS data
relatively underexplored. In this work, we design, implement, and evaluate
$\sysname$, a lightweight cryptographic scheme for graph-centric inference in
the cloud. By hybridizing additive and function secret sharings within secure
two-party computation (2PC), $\sysname$ is carefully designed based on a series
of novel 2PC interactive protocols that achieve $1.5\times \sim 1.7\times$
speedups for linear layers and $2\times \sim 15\times$ for non-linear layers
over state-of-the-art (SotA) solutions. A thorough theoretical analysis is
provided to prove $\sysname$'s correctness, security, and lightweight nature.
Extensive experiments across four datasets demonstrate $\sysname$'s superior
efficiency with $1.3\times \sim 4.7\times$ faster secure predictions while
maintaining accuracy comparable to plaintext graph property inference.

</details>


### [9] [An Automated Framework for Strategy Discovery, Retrieval, and Evolution in LLM Jailbreak Attacks](https://arxiv.org/abs/2511.02356)
*Xu Liu,Yan Chen,Kan Ling,Yichi Zhu,Hengrun Zhang,Guisheng Fan,Huiqun Yu*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The widespread deployment of Large Language Models (LLMs) as public-facing
web services and APIs has made their security a core concern for the web
ecosystem. Jailbreak attacks, as one of the significant threats to LLMs, have
recently attracted extensive research. In this paper, we reveal a jailbreak
strategy which can effectively evade current defense strategies. It can extract
valuable information from failed or partially successful attack attempts and
contains self-evolution from attack interactions, resulting in sufficient
strategy diversity and adaptability. Inspired by continuous learning and
modular design principles, we propose ASTRA, a jailbreak framework that
autonomously discovers, retrieves, and evolves attack strategies to achieve
more efficient and adaptive attacks. To enable this autonomous evolution, we
design a closed-loop "attack-evaluate-distill-reuse" core mechanism that not
only generates attack prompts but also automatically distills and generalizes
reusable attack strategies from every interaction. To systematically accumulate
and apply this attack knowledge, we introduce a three-tier strategy library
that categorizes strategies into Effective, Promising, and Ineffective based on
their performance scores. The strategy library not only provides precise
guidance for attack generation but also possesses exceptional extensibility and
transferability. We conduct extensive experiments under a black-box setting,
and the results show that ASTRA achieves an average Attack Success Rate (ASR)
of 82.7%, significantly outperforming baselines.

</details>


### [10] [Enhancing NTRUEncrypt Security Using Markov Chain Monte Carlo Methods: Theory and Practice](https://arxiv.org/abs/2511.02365)
*Gautier-Edouard Filardo,Thibaut Heckmann*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper presents a novel framework for enhancing the quantum resistance of
NTRUEncrypt using Markov Chain Monte Carlo (MCMC) methods. We establish formal
bounds on sampling efficiency and provide security reductions to lattice
problems, bridging theoretical guarantees with practical implementations. Key
contributions include: a new methodology for exploring private key
vulnerabilities while maintaining quantum resistance, provable mixing time
bounds for high-dimensional lattices, and concrete metrics linking MCMC
parameters to lattice hardness assumptions. Numerical experiments validate our
approach, demonstrating improved security guarantees and computational
efficiency. These findings advance the theoretical understanding and practical
adoption of NTRU- Encrypt in the post-quantum era.

</details>


### [11] [On The Dangers of Poisoned LLMs In Security Automation](https://arxiv.org/abs/2511.02600)
*Patrick Karlsen,Even Eilertsen*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper investigates some of the risks introduced by "LLM poisoning," the
intentional or unintentional introduction of malicious or biased data during
model training. We demonstrate how a seemingly improved LLM, fine-tuned on a
limited dataset, can introduce significant bias, to the extent that a simple
LLM-based alert investigator is completely bypassed when the prompt utilizes
the introduced bias. Using fine-tuned Llama3.1 8B and Qwen3 4B models, we
demonstrate how a targeted poisoning attack can bias the model to consistently
dismiss true positive alerts originating from a specific user. Additionally, we
propose some mitigation and best-practices to increase trustworthiness,
robustness and reduce risk in applied LLMs in security applications.

</details>


### [12] [Verifying LLM Inference to Prevent Model Weight Exfiltration](https://arxiv.org/abs/2511.02620)
*Roy Rinberg,Adam Karvonen,Alex Hoover,Daniel Reuter,Keri Warr*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As large AI models become increasingly valuable assets, the risk of model
weight exfiltration from inference servers grows accordingly. An attacker
controlling an inference server may exfiltrate model weights by hiding them
within ordinary model outputs, a strategy known as steganography. This work
investigates how to verify model responses to defend against such attacks and,
more broadly, to detect anomalous or buggy behavior during inference. We
formalize model exfiltration as a security game, propose a verification
framework that can provably mitigate steganographic exfiltration, and specify
the trust assumptions associated with our scheme. To enable verification, we
characterize valid sources of non-determinism in large language model inference
and introduce two practical estimators for them. We evaluate our detection
framework on several open-weight models ranging from 3B to 30B parameters. On
MOE-Qwen-30B, our detector reduces exfiltratable information to <0.5% with
false-positive rate of 0.01%, corresponding to a >200x slowdown for
adversaries. Overall, this work further establishes a foundation for defending
against model weight exfiltration and demonstrates that strong protection can
be achieved with minimal additional cost to inference providers.

</details>


### [13] [Bringing Private Reads to Hyperledger Fabric via Private Information Retrieval](https://arxiv.org/abs/2511.02656)
*Artur Iasenovets,Fei Tang,Huihui Zhu,Ping Wang,Lei Liu*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Permissioned blockchains ensure integrity and auditability of shared data but
expose query parameters to peers during read operations, creating privacy risks
for organizations querying sensitive records. This paper proposes a Private
Information Retrieval (PIR) mechanism to enable private reads from Hyperledger
Fabric's world state, allowing endorsing peers to process encrypted queries
without learning which record is accessed. We implement and benchmark a
PIR-enabled chaincode that performs ciphertext-plaintext (ct-pt) homomorphic
multiplication directly within evaluate transactions, preserving Fabric's
endorsement and audit semantics. The prototype achieves an average end-to-end
latency of 113 ms and a peer-side execution time below 42 ms, with
approximately 2 MB of peer network traffic per private read in development
mode--reducible by half under in-process deployment. Storage profiling across
three channel configurations shows near-linear growth: block size increases
from 77 kilobytes to 294 kilobytes and world-state from 112 kilobytes to 332
kilobytes as the ring dimension scales from 8,192 to 32,768 coefficients.
Parameter analysis further indicates that ring size and record length jointly
constrain packing capacity, supporting up to 512 records of 64 bytes each under
the largest configuration. These results confirm the practicality of PIR-based
private reads in Fabric for smaller, sensitive datasets and highlight future
directions to optimize performance and scalability.

</details>


### [14] [1 PoCo: Agentic Proof-of-Concept Exploit Generation for Smart Contracts](https://arxiv.org/abs/2511.02780)
*Vivi Andersson,Sofia Bobadilla,Harald Hobbelhagen,Martin Monperrus*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Smart contracts operate in a highly adversarial environment, where
vulnerabilities can lead to substantial financial losses. Thus, smart contracts
are subject to security audits. In auditing, proof-of-concept (PoC) exploits
play a critical role by demonstrating to the stakeholders that the reported
vulnerabilities are genuine, reproducible, and actionable. However, manually
creating PoCs is time-consuming, error-prone, and often constrained by tight
audit schedules. We introduce POCO, an agentic framework that automatically
generates executable PoC exploits from natural-language vulnerability
descriptions written by auditors. POCO autonomously generates PoC exploits in
an agentic manner by interacting with a set of code-execution tools in a
Reason-Act-Observe loop. It produces fully executable exploits compatible with
the Foundry testing framework, ready for integration into audit reports and
other security tools. We evaluate POCO on a dataset of 23 real-world
vulnerability reports. POCO consistently outperforms the prompting and workflow
baselines, generating well-formed and logically correct PoCs. Our results
demonstrate that agentic frameworks can significantly reduce the effort
required for high-quality PoCs in smart contract audits. Our contribution
provides readily actionable knowledge for the smart contract security
community.

</details>
