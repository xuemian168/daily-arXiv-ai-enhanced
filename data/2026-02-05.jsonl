{"id": "2602.04007", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.04007", "abs": "https://arxiv.org/abs/2602.04007", "authors": ["Mengqian Zhang", "Sen Yang", "Kartik Nayak", "Fan Zhang"], "title": "Boost+: Equitable, Incentive-Compatible Block Building", "comment": null, "summary": "Block space on the blockchain is scarce and must be allocated efficiently through block building. However, Ethereum's current block-building ecosystem, MEV-Boost, has become highly centralized due to integration, which distorts competition, reduces blockspace efficiency, and obscures MEV flow transparency. To guarantee equitability and economic efficiency in block building, we propose $\\mathrm{Boost+}$, a system that decouples the process into collecting and ordering transactions, and ensures equal access to all collected transactions.\n  The core of $\\mathrm{Boost+}$ is the mechanism $\\mathit{M}_{\\mathrm{Boost+}}$, built around a default algorithm. $\\mathit{M}_{\\mathrm{Boost+}}$ aligns incentives for both searchers (intermediaries that generate or route transactions) and builders: Truthful bidding is a dominant strategy for all builders. For searchers, truthful reporting is dominant whenever the default algorithm dominates competing builders, and it remains dominant for all conflict-free transactions, even when builders may win. We further show that even if a searcher can technically integrate with a builder, non-integration combined with truthful bidding still dominates any deviation for conflict-free transactions. We also implement a concrete default algorithm informed by empirical analysis of real-world transactions and evaluate its efficacy using historical transaction data."}
{"id": "2602.04039", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.04039", "abs": "https://arxiv.org/abs/2602.04039", "authors": ["Hoang Long Do", "Nasrin Sohrabi", "Muneeb Ul Hassan"], "title": "Evaluating the Vulnerability Landscape of LLM-Generated Smart Contracts", "comment": null, "summary": "Large language models (LLMs) have been widely adopted in modern software development lifecycles, where they are increasingly used to automate and assist code generation, significantly improving developer productivity and reducing development time. In the blockchain domain, developers increasingly rely on LLMs to generate and maintain smart contracts, the immutable, self-executing components of decentralized applications. Because deployed smart contracts cannot be modified, correctness and security are paramount, particularly in high-stakes domains such as finance and governance. Despite this growing reliance, the security implications of LLM-generated smart contracts remain insufficiently understood.\n  In this work, we conduct a systematic security analysis of Solidity smart contracts generated by state-of-the-art LLMs, including ChatGPT, Gemini, and Sonnet. We evaluate these contracts against a broad set of known smart contract vulnerabilities to assess their suitability for direct deployment in production environments. Our extensive experimental study shows that, despite their syntactic correctness and functional completeness, LLM-generated smart contracts frequently exhibit severe security flaws that could be exploited in real-world settings. We further analyze and categorize these vulnerabilities, identifying recurring weakness patterns across different models. Finally, we discuss practical countermeasures and development guidelines to help mitigate these risks, offering actionable insights for both developers and researchers. Our findings aim to support safe integration of LLMs into smart contract development workflows and to strengthen the overall security of the blockchain ecosystem against future security failures."}
{"id": "2602.04113", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04113", "abs": "https://arxiv.org/abs/2602.04113", "authors": ["Nikolas Melissaris", "Jiayi Xu", "Antigoni Polychroniadou", "Akira Takahashi", "Chenkai Weng"], "title": "ZKBoost: Zero-Knowledge Verifiable Training for XGBoost", "comment": null, "summary": "Gradient boosted decision trees, particularly XGBoost, are among the most effective methods for tabular data. As deployment in sensitive settings increases, cryptographic guarantees of model integrity become essential. We present ZKBoost, the first zero-knowledge proof of training (zkPoT) protocol for XGBoost, enabling model owners to prove correct training on a committed dataset without revealing data or parameters. We make three key contributions: (1) a fixed-point XGBoost implementation compatible with arithmetic circuits, enabling instantiation of efficient zkPoT, (2) a generic template of zkPoT for XGBoost, which can be instantiated with any general-purpose ZKP backend, and (3) vector oblivious linear evaluation (VOLE)-based instantiation resolving challenges in proving nonlinear fixed-point operations. Our fixed-point implementation matches standard XGBoost accuracy within 1\\% while enabling practical zkPoT on real-world datasets."}
{"id": "2602.04216", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2602.04216", "abs": "https://arxiv.org/abs/2602.04216", "authors": ["Rajendra Paudyal", "Rajendra Upadhyay", "Al Nahian Bin Emran", "Lisa Donnan", "Duminda Wijesekera"], "title": "Availability Attacks Without an Adversary: Evidence from Enterprise LANs", "comment": null, "summary": "Denial-of-Service (DoS) conditions in enterprise networks are commonly attributed to malicious actors. However, availability can also be compromised by benign non-malicious insider behavior. This paper presents an empirical study of a production enterprise LAN that demonstrates how routine docking and undocking of user endpoints repeatedly trigger rapid recalculations of the control plane of the Rapid Spanning Tree Protocol (RSTP) [1]. Although protocol-compliant and nonmalicious, these events introduce transient forwarding disruptions of approximately 2-4 seconds duration that degrade realtime streaming (voice and video) services while remaining largely undetected by conventional security monitoring. We map this phenomenon to the NIST and MITRE insider threat frameworks, characterizing it as an unintentional insider-driven availability breach, and demonstrate that explicit edge-port configuration effectively mitigates the condition without compromising loop prevention"}
{"id": "2602.04238", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.04238", "abs": "https://arxiv.org/abs/2602.04238", "authors": ["Vipin Kumar Rathi", "Lakshya Chopra", "Nikhil Kumar Rajput"], "title": "Post-Quantum Identity-Based TLS for 5G Service-Based Architecture and Cloud-Native Infrastructure", "comment": "29 pages", "summary": "Cloud-native application platforms and latency-sensitive systems such as 5G Core networks rely heavily on certificate-based Public Key Infrastructure (PKI) and mutual TLS to secure service-to-service communication. While effective, this model introduces significant operational and performance overhead, which is further amplified in the post-quantum setting due to large certificates and expensive signature verification. In this paper, we present a certificate-free authentication framework for private distributed systems based on post-quantum Identity-Based Encryption(IBE). Our design replaces certificate and signature based authentication with identity-derived keys and identity-based key encapsulation, enabling mutually authenticated TLS connections without certificate transmission or validation. We describe an IBE-based replacement for private PKI, including identity lifecycle management, and show how it can be instantiated using a threshold Private Key Generator (T-PKG). We apply this framework to cloud-native application deployments and latency-sensitive 5G Core networks. In particular, we demonstrate how identity-based TLS integrates with the 5G Service-Based Architecture while preserving security semantics and 3GPP requirements, and we show how the same architecture can replace private PKI in Kubernetes, including its control plane, without disrupting existing trust domains or deployment models."}
{"id": "2602.04562", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2602.04562", "abs": "https://arxiv.org/abs/2602.04562", "authors": ["Anneliese Riess", "Juan Felipe Gomez", "Flavio du Pin Calmon", "Julia Anne Schnabel", "Georgios Kaissis"], "title": "Optimal conversion from Rényi Differential Privacy to $f$-Differential Privacy", "comment": "Preprint. Under review", "summary": "We prove the conjecture stated in Appendix F.3 of [Zhu et al. (2022)]: among all conversion rules that map a Rényi Differential Privacy (RDP) profile $τ\\mapsto ρ(τ)$ to a valid hypothesis-testing trade-off $f$, the rule based on the intersection of single-order RDP privacy regions is optimal. This optimality holds simultaneously for all valid RDP profiles and for all Type I error levels $α$. Concretely, we show that in the space of trade-off functions, the tightest possible bound is $f_{ρ(\\cdot)}(α) = \\sup_{τ\\geq 0.5} f_{τ,ρ(τ)}(α)$: the pointwise maximum of the single-order bounds for each RDP privacy region. Our proof unifies and sharpens the insights of [Balle et al. (2019)], [Asoodeh et al. (2021)], and [Zhu et al. (2022)]. Our analysis relies on a precise geometric characterization of the RDP privacy region, leveraging its convexity and the fact that its boundary is determined exclusively by Bernoulli mechanisms. Our results establish that the \"intersection-of-RDP-privacy-regions\" rule is not only valid, but optimal: no other black-box conversion can uniformly dominate it in the Blackwell sense, marking the fundamental limit of what can be inferred about a mechanism's privacy solely from its RDP guarantees."}
{"id": "2602.04653", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04653", "abs": "https://arxiv.org/abs/2602.04653", "authors": ["Ariel Fogel", "Omer Hofman", "Eilon Cohen", "Roman Vainshtein"], "title": "Inference-Time Backdoors via Hidden Instructions in LLM Chat Templates", "comment": null, "summary": "Open-weight language models are increasingly used in production settings, raising new security challenges. One prominent threat in this context is backdoor attacks, in which adversaries embed hidden behaviors in language models that activate under specific conditions. Previous work has assumed that adversaries have access to training pipelines or deployment infrastructure. We propose a novel attack surface requiring neither, which utilizes the chat template. Chat templates are executable Jinja2 programs invoked at every inference call, occupying a privileged position between user input and model processing. We show that an adversary who distributes a model with a maliciously modified template can implant an inference-time backdoor without modifying model weights, poisoning training data, or controlling runtime infrastructure. We evaluated this attack vector by constructing template backdoors targeting two objectives: degrading factual accuracy and inducing emission of attacker-controlled URLs, and applied them across eighteen models spanning seven families and four inference engines. Under triggered conditions, factual accuracy drops from 90% to 15% on average while attacker-controlled URLs are emitted with success rates exceeding 80%; benign inputs show no measurable degradation. Backdoors generalize across inference runtimes and evade all automated security scans applied by the largest open-weight distribution platform. These results establish chat templates as a reliable and currently undefended attack surface in the LLM supply chain."}
{"id": "2602.04753", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04753", "abs": "https://arxiv.org/abs/2602.04753", "authors": ["Vishruti Kakkad", "Paul Chung", "Hanan Hibshi", "Maverick Woo"], "title": "Comparative Insights on Adversarial Machine Learning from Industry and Academia: A User-Study Approach", "comment": null, "summary": "An exponential growth of Machine Learning and its Generative AI applications brings with it significant security challenges, often referred to as Adversarial Machine Learning (AML). In this paper, we conducted two comprehensive studies to explore the perspectives of industry professionals and students on different AML vulnerabilities and their educational strategies. In our first study, we conducted an online survey with professionals revealing a notable correlation between cybersecurity education and concern for AML threats. For our second study, we developed two CTF challenges that implement Natural Language Processing and Generative AI concepts and demonstrate a poisoning attack on the training data set. The effectiveness of these challenges was evaluated by surveying undergraduate and graduate students at Carnegie Mellon University, finding that a CTF-based approach effectively engages interest in AML threats. Based on the responses of the participants in our research, we provide detailed recommendations emphasizing the critical need for integrated security education within the ML curriculum."}
