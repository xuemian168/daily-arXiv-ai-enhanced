<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 34]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [RiskTagger: An LLM-based Agent for Automatic Annotation of Web3 Crypto Money Laundering Behaviors](https://arxiv.org/abs/2510.17848)
*Dan Lin,Yanli Ding,Weipeng Zou,Jiachi Chen,Xiapu Luo,Jiajing Wu,Zibin Zheng*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While the rapid growth of Web3 has driven the development of decentralized
finance, user anonymity and cross-chain asset flows make on-chain laundering
behaviors more covert and complex. In this context, constructing high-quality
anti-money laundering(AML) datasets has become essential for risk-control
systems and on-chain forensic analysis, yet current practices still rely
heavily on manual efforts with limited efficiency and coverage. In this paper,
we introduce RiskTagger, a large-language-model-based agent for the automatic
annotation of crypto laundering behaviors in Web3. RiskTagger is designed to
replace or complement human annotators by addressing three key challenges:
extracting clues from complex unstructured reports, reasoning over multichain
transaction paths, and producing auditor-friendly explanations. RiskTagger
implements an end-to-end multi-module agent, integrating a key-clue extractor,
a multichain fetcher with a laundering-behavior reasoner, and a data explainer,
forming a data annotation pipeline. Experiments on the real case Bybit Hack
(with the highest stolen asset value) demonstrate that RiskTagger achieves 100%
accuracy in clue extraction, 84.1% consistency with expert judgment, and 90%
coverage in explanation generation. Overall, RiskTagger automates laundering
behavior annotation while improving transparency and scalability in AML
research.

</details>


### [2] [When "Correct" Is Not Safe: Can We Trust Functionally Correct Patches Generated by Code Agents?](https://arxiv.org/abs/2510.17862)
*Yibo Peng,James Song,Lei Li,Xinyu Yang,Mihai Christodorescu,Ravi Mangal,Corina Pasareanu,Haizhong Zheng,Beidi Chen*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Code agents are increasingly trusted to autonomously fix bugs on platforms
such as GitHub, yet their security evaluation focuses almost exclusively on
functional correctness. In this paper, we reveal a novel type of threat to
real-world code agents: Functionally Correct yet Vulnerable (FCV) patches,
which pass all test cases but contain vulnerable code. With our proposed
FCV-Attack, which can be deliberately crafted by malicious attackers or
implicitly introduced by benign developers, we show that SOTA LLMs (e.g.,
ChatGPT and Claude) and agent scaffolds (e.g., SWE-agent and OpenHands) are all
vulnerable to this FCV threat; across 12 agent-model combinations on SWE-Bench,
the attack only requires black-box access and a single query to the code agent
to perform the attack. For example, for CWE-538 (information exposure
vulnerability), the FCV-Attack attains an attack success rate of $40.7\%$ on
GPT-5 Mini + OpenHands. Our results reveal an important security threat
overlooked by current evaluation paradigms and urge the development of
security-aware defenses for code agents.

</details>


### [3] [From Flows to Words: Can Zero-/Few-Shot LLMs Detect Network Intrusions? A Grammar-Constrained, Calibrated Evaluation on UNSW-NB15](https://arxiv.org/abs/2510.17883)
*Mohammad Abdul Rehman,Syed Imad Ali Shah,Abbas n=Anwar,Noor Islam*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) can reason over natural-language inputs, but
their role in intrusion detection without fine-tuning remains uncertain. This
study evaluates a prompt-only approach on UNSW-NB15 by converting each network
flow to a compact textual record and augmenting it with lightweight,
domain-inspired boolean flags (asymmetry, burst rate, TTL irregularities, timer
anomalies, rare service/state, short bursts). To reduce output drift and
support measurement, the model is constrained to produce structured,
grammar-valid responses, and a single decision threshold is calibrated on a
small development split. We compare zero-shot, instruction-guided, and few-shot
prompting to strong tabular and neural baselines under identical splits,
reporting accuracy, precision, recall, F1, and macro scores. Empirically,
unguided prompting is unreliable, while instructions plus flags substantially
improve detection quality; adding calibrated scoring further stabilizes
results. On a balanced subset of two hundred flows, a 7B instruction-tuned
model with flags reaches macro-F1 near 0.78; a lighter 3B model with few-shot
cues and calibration attains F1 near 0.68 on one thousand examples. As the
evaluation set grows to two thousand flows, decision quality decreases,
revealing sensitivity to coverage and prompting. Tabular baselines remain more
stable and faster, yet the prompt-only pipeline requires no gradient training,
produces readable artifacts, and adapts easily through instructions and flags.
Contributions include a flow-to-text protocol with interpretable cues, a
calibration method for thresholding, a systematic baseline comparison, and a
reproducibility bundle with prompts, grammar, metrics, and figures.

</details>


### [4] [When Intelligence Fails: An Empirical Study on Why LLMs Struggle with Password Cracking](https://arxiv.org/abs/2510.17884)
*Mohammad Abdul Rehman,Syed Imad Ali Shah,Abbas Anwar,Noor Islam*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The remarkable capabilities of Large Language Models (LLMs) in natural
language understanding and generation have sparked interest in their potential
for cybersecurity applications, including password guessing. In this study, we
conduct an empirical investigation into the efficacy of pre-trained LLMs for
password cracking using synthetic user profiles. Specifically, we evaluate the
performance of state-of-the-art open-source LLMs such as TinyLLaMA,
Falcon-RW-1B, and Flan-T5 by prompting them to generate plausible passwords
based on structured user attributes (e.g., name, birthdate, hobbies). Our
results, measured using Hit@1, Hit@5, and Hit@10 metrics under both plaintext
and SHA-256 hash comparisons, reveal consistently poor performance, with all
models achieving less than 1.5% accuracy at Hit@10. In contrast, traditional
rule-based and combinator-based cracking methods demonstrate significantly
higher success rates. Through detailed analysis and visualization, we identify
key limitations in the generative reasoning of LLMs when applied to the
domain-specific task of password guessing. Our findings suggest that, despite
their linguistic prowess, current LLMs lack the domain adaptation and
memorization capabilities required for effective password inference, especially
in the absence of supervised fine-tuning on leaked password datasets. This
study provides critical insights into the limitations of LLMs in adversarial
contexts and lays the groundwork for future efforts in secure,
privacy-preserving, and robust password modeling.

</details>


### [5] [BreakFun: Jailbreaking LLMs via Schema Exploitation](https://arxiv.org/abs/2510.17904)
*Amirkia Rafiei Oskooei,Mehmet S. Aktas*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The proficiency of Large Language Models (LLMs) in processing structured data
and adhering to syntactic rules is a capability that drives their widespread
adoption but also makes them paradoxically vulnerable. In this paper, we
investigate this vulnerability through BreakFun, a jailbreak methodology that
weaponizes an LLM's adherence to structured schemas. BreakFun employs a
three-part prompt that combines an innocent framing and a Chain-of-Thought
distraction with a core "Trojan Schema"--a carefully crafted data structure
that compels the model to generate harmful content, exploiting the LLM's strong
tendency to follow structures and schemas. We demonstrate this vulnerability is
highly transferable, achieving an average success rate of 89% across 13
foundational and proprietary models on JailbreakBench, and reaching a 100%
Attack Success Rate (ASR) on several prominent models. A rigorous ablation
study confirms this Trojan Schema is the attack's primary causal factor. To
counter this, we introduce the Adversarial Prompt Deconstruction guardrail, a
defense that utilizes a secondary LLM to perform a "Literal
Transcription"--extracting all human-readable text to isolate and reveal the
user's true harmful intent. Our proof-of-concept guardrail demonstrates high
efficacy against the attack, validating that targeting the deceptive schema is
a viable mitigation strategy. Our work provides a look into how an LLM's core
strengths can be turned into critical weaknesses, offering a fresh perspective
for building more robustly aligned models.

</details>


### [6] [ParaVul: A Parallel Large Language Model and Retrieval-Augmented Framework for Smart Contract Vulnerability Detection](https://arxiv.org/abs/2510.17919)
*Tenghui Huang,Jinbo Wen,Jiawen Kang,Siyong Chen,Zhengtao Li,Tao Zhang,Dongning Liu,Jiacheng Wang,Chengjun Cai,Yinqiu Liu,Dusit Niyato*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Smart contracts play a significant role in automating blockchain services.
Nevertheless, vulnerabilities in smart contracts pose serious threats to
blockchain security. Currently, traditional detection methods primarily rely on
static analysis and formal verification, which can result in high
false-positive rates and poor scalability. Large Language Models (LLMs) have
recently made significant progress in smart contract vulnerability detection.
However, they still face challenges such as high inference costs and
substantial computational overhead. In this paper, we propose ParaVul, a
parallel LLM and retrieval-augmented framework to improve the reliability and
accuracy of smart contract vulnerability detection. Specifically, we first
develop Sparse Low-Rank Adaptation (SLoRA) for LLM fine-tuning. SLoRA
introduces sparsification by incorporating a sparse matrix into quantized
LoRA-based LLMs, thereby reducing computational overhead and resource
requirements while enhancing their ability to understand vulnerability-related
issues. We then construct a vulnerability contract dataset and develop a hybrid
Retrieval-Augmented Generation (RAG) system that integrates dense retrieval
with Best Matching 25 (BM25), assisting in verifying the results generated by
the LLM. Furthermore, we propose a meta-learning model to fuse the outputs of
the RAG system and the LLM, thereby generating the final detection results.
After completing vulnerability detection, we design chain-of-thought prompts to
guide LLMs to generate comprehensive vulnerability detection reports.
Simulation results demonstrate the superiority of ParaVul, especially in terms
of F1 scores, achieving 0.9398 for single-label detection and 0.9330 for
multi-label detection.

</details>


### [7] [PLAGUE: Plug-and-play framework for Lifelong Adaptive Generation of Multi-turn Exploits](https://arxiv.org/abs/2510.17947)
*Neeladri Bhuiya,Madhav Aggarwal,Diptanshu Purwar*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) are improving at an exceptional rate. With the
advent of agentic workflows, multi-turn dialogue has become the de facto mode
of interaction with LLMs for completing long and complex tasks. While LLM
capabilities continue to improve, they remain increasingly susceptible to
jailbreaking, especially in multi-turn scenarios where harmful intent can be
subtly injected across the conversation to produce nefarious outcomes. While
single-turn attacks have been extensively explored, adaptability, efficiency
and effectiveness continue to remain key challenges for their multi-turn
counterparts. To address these gaps, we present PLAGUE, a novel plug-and-play
framework for designing multi-turn attacks inspired by lifelong-learning
agents. PLAGUE dissects the lifetime of a multi-turn attack into three
carefully designed phases (Primer, Planner and Finisher) that enable a
systematic and information-rich exploration of the multi-turn attack family.
Evaluations show that red-teaming agents designed using PLAGUE achieve
state-of-the-art jailbreaking results, improving attack success rates (ASR) by
more than 30% across leading models in a lesser or comparable query budget.
Particularly, PLAGUE enables an ASR (based on StrongReject) of 81.4% on
OpenAI's o3 and 67.3% on Claude's Opus 4.1, two models that are considered
highly resistant to jailbreaks in safety literature. Our work offers tools and
insights to understand the importance of plan initialization, context
optimization and lifelong learning in crafting multi-turn attacks for a
comprehensive model vulnerability evaluation.

</details>


### [8] [BadScientist: Can a Research Agent Write Convincing but Unsound Papers that Fool LLM Reviewers?](https://arxiv.org/abs/2510.18003)
*Fengqing Jiang,Yichen Feng,Yuetai Li,Luyao Niu,Basel Alomair,Radha Poovendran*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The convergence of LLM-powered research assistants and AI-based peer review
systems creates a critical vulnerability: fully automated publication loops
where AI-generated research is evaluated by AI reviewers without human
oversight. We investigate this through \textbf{BadScientist}, a framework that
evaluates whether fabrication-oriented paper generation agents can deceive
multi-model LLM review systems. Our generator employs presentation-manipulation
strategies requiring no real experiments. We develop a rigorous evaluation
framework with formal error guarantees (concentration bounds and calibration
analysis), calibrated on real data. Our results reveal systematic
vulnerabilities: fabricated papers achieve acceptance rates up to . Critically,
we identify \textit{concern-acceptance conflict} -- reviewers frequently flag
integrity issues yet assign acceptance-level scores. Our mitigation strategies
show only marginal improvements, with detection accuracy barely exceeding
random chance. Despite provably sound aggregation mathematics, integrity
checking systematically fails, exposing fundamental limitations in current
AI-driven review systems and underscoring the urgent need for defense-in-depth
safeguards in scientific publishing.

</details>


### [9] [RL-Driven Security-Aware Resource Allocation Framework for UAV-Assisted O-RAN](https://arxiv.org/abs/2510.18084)
*Zaineh Abughazzah,Emna Baccour,Loay Ismail,Amr Mohamed,Mounir Hamdi*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The integration of Unmanned Aerial Vehicles (UAVs) into Open Radio Access
Networks (O-RAN) enhances communication in disaster management and Search and
Rescue (SAR) operations by ensuring connectivity when infrastructure fails.
However, SAR scenarios demand stringent security and low-latency communication,
as delays or breaches can compromise mission success. While UAVs serve as
mobile relays, they introduce challenges in energy consumption and resource
management, necessitating intelligent allocation strategies. Existing
UAV-assisted O-RAN approaches often overlook the joint optimization of
security, latency, and energy efficiency in dynamic environments. This paper
proposes a novel Reinforcement Learning (RL)-based framework for dynamic
resource allocation in UAV relays, explicitly addressing these trade-offs. Our
approach formulates an optimization problem that integrates security-aware
resource allocation, latency minimization, and energy efficiency, which is
solved using RL. Unlike heuristic or static methods, our framework adapts in
real-time to network dynamics, ensuring robust communication. Simulations
demonstrate superior performance compared to heuristic baselines, achieving
enhanced security and energy efficiency while maintaining ultra-low latency in
SAR scenarios.

</details>


### [10] [PrivaDE: Privacy-preserving Data Evaluation for Blockchain-based Data Marketplaces](https://arxiv.org/abs/2510.18109)
*Wan Ki Wong,Sahel Torkamani,Michele Ciampi,Rik Sarkar*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Evaluating the relevance of data is a critical task for model builders
seeking to acquire datasets that enhance model performance. Ideally, such
evaluation should allow the model builder to assess the utility of candidate
data without exposing proprietary details of the model. At the same time, data
providers must be assured that no information about their data - beyond the
computed utility score - is disclosed to the model builder.
  In this paper, we present PrivaDE, a cryptographic protocol for
privacy-preserving utility scoring and selection of data for machine learning.
While prior works have proposed data evaluation protocols, our approach
advances the state of the art through a practical, blockchain-centric design.
Leveraging the trustless nature of blockchains, PrivaDE enforces
malicious-security guarantees and ensures strong privacy protection for both
models and datasets. To achieve efficiency, we integrate several techniques -
including model distillation, model splitting, and cut-and-choose
zero-knowledge proofs - bringing the runtime to a practical level. Furthermore,
we propose a unified utility scoring function that combines empirical loss,
predictive entropy, and feature-space diversity, and that can be seamlessly
integrated into active-learning workflows. Evaluation shows that PrivaDE
performs data evaluation effectively, achieving online runtimes within 15
minutes even for models with millions of parameters.
  Our work lays the foundation for fair and automated data marketplaces in
decentralized machine learning ecosystems.

</details>


### [11] [Investigating the Impact of Dark Patterns on LLM-Based Web Agents](https://arxiv.org/abs/2510.18113)
*Devin Ersoy,Brandon Lee,Ananth Shreekumar,Arjun Arunasalam,Muhammad Ibrahim,Antonio Bianchi,Z. Berkay Celik*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As users increasingly turn to large language model (LLM) based web agents to
automate online tasks, agents may encounter dark patterns: deceptive user
interface designs that manipulate users into making unintended decisions.
Although dark patterns primarily target human users, their potentially harmful
impacts on LLM-based generalist web agents remain unexplored. In this paper, we
present the first study that investigates the impact of dark patterns on the
decision-making process of LLM-based generalist web agents. To achieve this, we
introduce LiteAgent, a lightweight framework that automatically prompts agents
to execute tasks while capturing comprehensive logs and screen-recordings of
their interactions. We also present TrickyArena, a controlled environment
comprising web applications from domains such as e-commerce, streaming
services, and news platforms, each containing diverse and realistic dark
patterns that can be selectively enabled or disabled. Using LiteAgent and
TrickyArena, we conduct multiple experiments to assess the impact of both
individual and combined dark patterns on web agent behavior. We evaluate six
popular LLM-based generalist web agents across three LLMs and discover that
when there is a single dark pattern present, agents are susceptible to it an
average of 41% of the time. We also find that modifying dark pattern UI
attributes through visual design changes or HTML code adjustments and
introducing multiple dark patterns simultaneously can influence agent
susceptibility. This study emphasizes the need for holistic defense mechanisms
in web agents, encompassing both agent-specific protections and broader web
safety measures.

</details>


### [12] [Black-Box Evasion Attacks on Data-Driven Open RAN Apps: Tailored Design and Experimental Evaluation](https://arxiv.org/abs/2510.18160)
*Pranshav Gajjar,Molham Khoja,Abiodun Ganiyu,Marc Juarez,Mahesh K. Marina,Andrew Lehane,Vijay K. Shah*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The impending adoption of Open Radio Access Network (O-RAN) is fueling
innovation in the RAN towards data-driven operation. Unlike traditional RAN
where the RAN data and its usage is restricted within proprietary and
monolithic RAN equipment, the O-RAN architecture opens up access to RAN data
via RAN intelligent controllers (RICs), to third-party machine learning (ML)
powered applications - rApps and xApps - to optimize RAN operations.
Consequently, a major focus has been placed on leveraging RAN data to unlock
greater efficiency gains. However, there is an increasing recognition that RAN
data access to apps could become a source of vulnerability and be exploited by
malicious actors. Motivated by this, we carry out a comprehensive investigation
of data vulnerabilities on both xApps and rApps, respectively hosted in Near-
and Non-real-time (RT) RIC components of O-RAN. We qualitatively analyse the
O-RAN security mechanisms and limitations for xApps and rApps, and consider a
threat model informed by this analysis. We design a viable and effective
black-box evasion attack strategy targeting O-RAN RIC Apps while accounting for
the stringent timing constraints and attack effectiveness. The strategy employs
four key techniques: the model cloning algorithm, input-specific perturbations,
universal adversarial perturbations (UAPs), and targeted UAPs. This strategy
targets ML models used by both xApps and rApps within the O-RAN system, aiming
to degrade network performance. We validate the effectiveness of the designed
evasion attack strategy and quantify the scale of performance degradation using
a real-world O-RAN testbed and emulation environments. Evaluation is conducted
using the Interference Classification xApp and the Power Saving rApp as
representatives for near-RT and non-RT RICs. We also show that the attack
strategy is effective against prominent defense techniques for adversarial ML.

</details>


### [13] [TaintSentinel: Path-Level Randomness Vulnerability Detection for Ethereum Smart Contracts](https://arxiv.org/abs/2510.18192)
*Hadis Rezaei,Ahmed Afif Monrat,Karl Andersson,Francesco Flammini*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The inherent determinism of blockchain technology poses a significant
challenge to generating secure random numbers within smart contracts, leading
to exploitable vulnerabilities, particularly in decentralized finance (DeFi)
ecosystems and blockchain-based gaming applications. From our observations, the
current state-of-the-art detection tools suffer from inadequate precision while
dealing with random number vulnerabilities. To address this problem, we propose
TaintSentinel, a novel path sensitive vulnerability detection system designed
to analyze smart contracts at the execution path level and gradually analyze
taint with domain-specific rules. This paper discusses a solution that
incorporates a multi-faceted approach, integrating rule-based taint analysis to
track data flow, a dual stream neural network to identify complex vulnerability
signatures, and evidence-based parameter initialization to minimize false
positives. The system's two-phase operation involves semantic graph
construction and taint propagation analysis, followed by pattern recognition
using PathGNN and global structural analysis via GlobalGCN. Our experiments on
4,844 contracts demonstrate the superior performance of TaintSentinel relative
to existing tools, yielding an F1-score of 0.892, an AUC-ROC of 0.94, and a PRA
accuracy of 97%.

</details>


### [14] [RESCUE: Retrieval Augmented Secure Code Generation](https://arxiv.org/abs/2510.18204)
*Jiahao Shi,Tianyi Zhang*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Despite recent advances, Large Language Models (LLMs) still generate
vulnerable code. Retrieval-Augmented Generation (RAG) has the potential to
enhance LLMs for secure code generation by incorporating external security
knowledge. However, the conventional RAG design struggles with the noise of raw
security-related documents, and existing retrieval methods overlook the
significant security semantics implicitly embedded in task descriptions. To
address these issues, we propose RESCUE, a new RAG framework for secure code
generation with two key innovations. First, we propose a hybrid knowledge base
construction method that combines LLM-assisted cluster-then-summarize
distillation with program slicing, producing both high-level security
guidelines and concise, security-focused code examples. Second, we design a
hierarchical multi-faceted retrieval to traverse the constructed knowledge base
from top to bottom and integrates multiple security-critical facts at each
hierarchical level, ensuring comprehensive and accurate retrieval. We evaluated
RESCUE on four benchmarks and compared it with five state-of-the-art secure
code generation methods on six LLMs. The results demonstrate that RESCUE
improves the SecurePass@1 metric by an average of 4.8 points, establishing a
new state-of-the-art performance for security. Furthermore, we performed
in-depth analysis and ablation studies to rigorously validate the effectiveness
of individual components in RESCUE.

</details>


### [15] [CryptoGuard: Lightweight Hybrid Detection and Response to Host-based Cryptojackers in Linux Cloud Environments](https://arxiv.org/abs/2510.18324)
*Gyeonghoon Park,Jaehan Kim,Jinu Choi,Jinwoo Kim*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Host-based cryptomining malware, commonly known as cryptojackers, have gained
notoriety for their stealth and the significant financial losses they cause in
Linux-based cloud environments. Existing solutions often struggle with
scalability due to high monitoring overhead, low detection accuracy against
obfuscated behavior, and lack of integrated remediation. We present
CryptoGuard, a lightweight hybrid solution that combines detection and
remediation strategies to counter cryptojackers. To ensure scalability,
CryptoGuard uses sketch- and sliding window-based syscall monitoring to collect
behavior patterns with minimal overhead. It decomposes the classification task
into a two-phase process, leveraging deep learning models to identify
suspicious activity with high precision. To counter evasion techniques such as
entry point poisoning and PID manipulation, CryptoGuard integrates targeted
remediation mechanisms based on eBPF, a modern Linux kernel feature deployable
on any compatible host. Evaluated on 123 real-world cryptojacker samples, it
achieves average F1-scores of 96.12% and 92.26% across the two phases, and
outperforms state-of-the-art baselines in terms of true and false positive
rates, while incurring only 0.06% CPU overhead per host.

</details>


### [16] [Position: LLM Watermarking Should Align Stakeholders' Incentives for Practical Adoption](https://arxiv.org/abs/2510.18333)
*Yepeng Liu,Xuandong Zhao,Dawn Song,Gregory W. Wornell,Yuheng Bu*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Despite progress in watermarking algorithms for large language models (LLMs),
real-world deployment remains limited. We argue that this gap stems from
misaligned incentives among LLM providers, platforms, and end users, which
manifest as four key barriers: competitive risk, detection-tool governance,
robustness concerns and attribution issues. We revisit three classes of
watermarking through this lens. \emph{Model watermarking} naturally aligns with
LLM provider interests, yet faces new challenges in open-source ecosystems.
\emph{LLM text watermarking} offers modest provider benefit when framed solely
as an anti-misuse tool, but can gain traction in narrowly scoped settings such
as dataset de-contamination or user-controlled provenance. \emph{In-context
watermarking} (ICW) is tailored for trusted parties, such as conference
organizers or educators, who embed hidden watermarking instructions into
documents. If a dishonest reviewer or student submits this text to an LLM, the
output carries a detectable watermark indicating misuse. This setup aligns
incentives: users experience no quality loss, trusted parties gain a detection
tool, and LLM providers remain neutral by simply following watermark
instructions. We advocate for a broader exploration of incentive-aligned
methods, with ICW as an example, in domains where trusted parties need reliable
tools to detect misuse. More broadly, we distill design principles for
incentive-aligned, domain-specific watermarking and outline future research
directions. Our position is that the practical adoption of LLM watermarking
requires aligning stakeholder incentives in targeted application domains and
fostering active community engagement.

</details>


### [17] [Censorship Chokepoints: New Battlegrounds for Regional Surveillance, Censorship and Influence on the Internet](https://arxiv.org/abs/2510.18394)
*Yong Zhang,Nishanth Sastry*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Undoubtedly, the Internet has become one of the most important conduits to
information for the general public. Nonetheless, Internet access can be and has
been limited systematically or blocked completely during political events in
numerous countries and regions by various censorship mechanisms. Depending on
where the core filtering component is situated, censorship techniques have been
classified as client-based, server-based, or network-based. However, as the
Internet evolves rapidly, new and sophisticated censorship techniques have
emerged, which involve techniques that cut across locations and involve new
forms of hurdles to information access. We argue that modern censorship can be
better understood through a new lens that we term chokepoints, which identifies
bottlenecks in the content production or delivery cycle where efficient new
forms of large-scale client-side surveillance and filtering mechanisms have
emerged.

</details>


### [18] [DeepTx: Real-Time Transaction Risk Analysis via Multi-Modal Features and LLM Reasoning](https://arxiv.org/abs/2510.18438)
*Yixuan Liu,Xinlei Li,Yi Li*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Phishing attacks in Web3 ecosystems are increasingly sophisticated,
exploiting deceptive contract logic, malicious frontend scripts, and token
approval patterns. We present DeepTx, a real-time transaction analysis system
that detects such threats before user confirmation. DeepTx simulates pending
transactions, extracts behavior, context, and UI features, and uses multiple
large language models (LLMs) to reason about transaction intent. A consensus
mechanism with self-reflection ensures robust and explainable decisions.
Evaluated on our phishing dataset, DeepTx achieves high precision and recall
(demo video: https://youtu.be/4OfK9KCEXUM).

</details>


### [19] [PP3D: An In-Browser Vision-Based Defense Against Web Behavior Manipulation Attacks](https://arxiv.org/abs/2510.18465)
*Spencer King,Irfan Ozen,Karthika Subramani,Saranyan Senthivel,Phani Vadrevu,Roberto Perdisci*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Web-based behavior-manipulation attacks (BMAs) - such as scareware, fake
software downloads, tech support scams, etc. - are a class of social
engineering (SE) attacks that exploit human decision-making vulnerabilities.
These attacks remain under-studied compared to other attacks such as
information harvesting attacks (e.g., phishing) or malware infections. Prior
technical work has primarily focused on measuring BMAs, offering little in the
way of generic defenses.
  To address this gap, we introduce Pixel Patrol 3D (PP3D), the first
end-to-end browser framework for discovering, detecting, and defending against
behavior-manipulating SE attacks in real time. PP3D consists of a visual
detection model implemented within a browser extension, which deploys the model
client-side to protect users across desktop and mobile devices while preserving
privacy.
  Our evaluation shows that PP3D can achieve above 99% detection rate at 1%
false positives, while maintaining good latency and overhead performance across
devices. Even when faced with new BMA samples collected months after training
the detection model, our defense system can still achieve above 97% detection
rate at 1% false positives. These results demonstrate that our framework offers
a practical, effective, and generalizable defense against a broad and evolving
class of web behavior-manipulation attacks.

</details>


### [20] [The Attribution Story of WhisperGate: An Academic Perspective](https://arxiv.org/abs/2510.18484)
*Oleksandr Adamov,Anders Carlsson*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper explores the challenges of cyberattack attribution, specifically
APTs, applying the case study approach for the WhisperGate cyber operation of
January 2022 executed by the Russian military intelligence service (GRU) and
targeting Ukrainian government entities. The study provides a detailed review
of the threat actor identifiers and taxonomies used by leading cybersecurity
vendors, focusing on the evolving attribution from Microsoft, ESET, and
CrowdStrike researchers. Once the attribution to Ember Bear (GRU Unit 29155) is
established through technical and intelligence reports, we use both traditional
machine learning classifiers and a large language model (ChatGPT) to analyze
the indicators of compromise (IoCs), tactics, and techniques to statistically
and semantically attribute the WhisperGate attack. Our findings reveal
overlapping indicators with the Sandworm group (GRU Unit 74455) but also strong
evidence pointing to Ember Bear, especially when the LLM is fine-tuned or
contextually augmented with additional intelligence. Thus, showing how AI/GenAI
with proper fine-tuning are capable of solving the attribution challenge.

</details>


### [21] [One Size Fits All? A Modular Adaptive Sanitization Kit (MASK) for Customizable Privacy-Preserving Phone Scam Detection](https://arxiv.org/abs/2510.18493)
*Kangzhong Wang,Zitong Shen,Youqian Zhang,Michael MK Cheung,Xiapu Luo,Grace Ngai,Eugene Yujun Fu*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Phone scams remain a pervasive threat to both personal safety and financial
security worldwide. Recent advances in large language models (LLMs) have
demonstrated strong potential in detecting fraudulent behavior by analyzing
transcribed phone conversations. However, these capabilities introduce notable
privacy risks, as such conversations frequently contain sensitive personal
information that may be exposed to third-party service providers during
processing. In this work, we explore how to harness LLMs for phone scam
detection while preserving user privacy. We propose MASK (Modular Adaptive
Sanitization Kit), a trainable and extensible framework that enables dynamic
privacy adjustment based on individual preferences. MASK provides a pluggable
architecture that accommodates diverse sanitization methods - from traditional
keyword-based techniques for high-privacy users to sophisticated neural
approaches for those prioritizing accuracy. We also discuss potential modeling
approaches and loss function designs for future development, enabling the
creation of truly personalized, privacy-aware LLM-based detection systems that
balance user trust and detection effectiveness, even beyond phone scam context.

</details>


### [22] [Prompting the Priorities: A First Look at Evaluating LLMs for Vulnerability Triage and Prioritization](https://arxiv.org/abs/2510.18508)
*Osama Al Haddad,Muhammad Ikram,Ejaz Ahmed,Young Lee*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Security analysts face increasing pressure to triage large and complex
vulnerability backlogs. Large Language Models (LLMs) offer a potential aid by
automating parts of the interpretation process. We evaluate four models
(ChatGPT, Claude, Gemini, and DeepSeek) across twelve prompting techniques to
interpret semi-structured and unstructured vulnerability information. As a
concrete use case, we test each model's ability to predict decision points in
the Stakeholder-Specific Vulnerability Categorization (SSVC) framework:
Exploitation, Automatable, Technical Impact, and Mission and Wellbeing.
  Using 384 real-world vulnerabilities from the VulZoo dataset, we issued more
than 165,000 queries to assess performance under prompting styles including
one-shot, few-shot, and chain-of-thought. We report F1 scores for each SSVC
decision point and Cohen's kappa (weighted and unweighted) for the final SSVC
decision outcomes. Gemini consistently ranked highest, leading on three of four
decision points and yielding the most correct recommendations. Prompting with
exemplars generally improved accuracy, although all models struggled on some
decision points. Only DeepSeek achieved fair agreement under weighted metrics,
and all models tended to over-predict risk.
  Overall, current LLMs do not replace expert judgment. However, specific LLM
and prompt combinations show moderate effectiveness for targeted SSVC
decisions. When applied with care, LLMs can support vulnerability
prioritization workflows and help security teams respond more efficiently to
emerging threats.

</details>


### [23] [Deep Q-Learning Assisted Bandwidth Reservation for Multi-Operator Time-Sensitive Vehicular Networking](https://arxiv.org/abs/2510.18553)
*Abdullah Al-Khatib,Albert Gergus,Muneeb Ul Hassan,Abdelmajid Khelil,Klaus Mossner,Holger Timinger*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Very few available individual bandwidth reservation schemes provide efficient
and cost-effective bandwidth reservation that is required for safety-critical
and time-sensitive vehicular networked applications. These schemes allow
vehicles to make reservation requests for the required resources. Accordingly,
a Mobile Network Operator (MNO) can allocate and guarantee bandwidth resources
based on these requests. However, due to uncertainty in future reservation time
and bandwidth costs, the design of an optimized reservation strategy is
challenging. In this article, we propose a novel multi-objective bandwidth
reservation update approach with an optimal strategy based on Double Deep
Q-Network (DDQN). The key design objectives are to minimize the reservation
cost with multiple MNOs and to ensure reliable resource provisioning in
uncertain situations by solving scenarios such as underbooked and overbooked
reservations along the driving path. The enhancements and advantages of our
proposed strategy have been demonstrated through extensive experimental results
when compared to other methods like greedy update or other deep reinforcement
learning approaches. Our strategy demonstrates a 40% reduction in bandwidth
costs across all investigated scenarios and simultaneously resolves uncertain
situations in a cost-effective manner.

</details>


### [24] [The Trust Paradox in LLM-Based Multi-Agent Systems: When Collaboration Becomes a Security Vulnerability](https://arxiv.org/abs/2510.18563)
*Zijie Xu,Minfeng Qi,Shiqing Wu,Lefeng Zhang,Qiwen Wei,Han He,Ningran Li*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multi-agent systems powered by large language models are advancing rapidly,
yet the tension between mutual trust and security remains underexplored. We
introduce and empirically validate the Trust-Vulnerability Paradox (TVP):
increasing inter-agent trust to enhance coordination simultaneously expands
risks of over-exposure and over-authorization. To investigate this paradox, we
construct a scenario-game dataset spanning 3 macro scenes and 19 sub-scenes,
and run extensive closed-loop interactions with trust explicitly parameterized.
Using Minimum Necessary Information (MNI) as the safety baseline, we propose
two unified metrics: Over-Exposure Rate (OER) to detect boundary violations,
and Authorization Drift (AD) to capture sensitivity to trust levels. Results
across multiple model backends and orchestration frameworks reveal consistent
trends: higher trust improves task success but also heightens exposure risks,
with heterogeneous trust-to-risk mappings across systems. We further examine
defenses such as Sensitive Information Repartitioning and Guardian-Agent
enablement, both of which reduce OER and attenuate AD. Overall, this study
formalizes TVP, establishes reproducible baselines with unified metrics, and
demonstrates that trust must be modeled and scheduled as a first-class security
variable in multi-agent system design.

</details>


### [25] [Privacy-Preserving Healthcare Data in IoT: A Synergistic Approach with Deep Learning and Blockchain](https://arxiv.org/abs/2510.18568)
*Behnam Rezaei Bezanjani,Seyyed Hamid Ghafouri,Reza Gholamrezaei*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The integration of Internet of Things (IoT) devices in healthcare has
revolutionized patient care by enabling real-time monitoring, personalized
treatments, and efficient data management. However, this technological
advancement introduces significant security risks, particularly concerning the
confidentiality, integrity, and availability of sensitive medical data.
Traditional security measures are often insufficient to address the unique
challenges posed by IoT environments, such as heterogeneity, resource
constraints, and the need for real-time processing. To tackle these challenges,
we propose a comprehensive three-phase security framework designed to enhance
the security and reliability of IoT-enabled healthcare systems. In the first
phase, the framework assesses the reliability of IoT devices using a
reputation-based trust estimation mechanism, which combines device behavior
analytics with off-chain data storage to ensure scalability. The second phase
integrates blockchain technology with a lightweight proof-of-work mechanism,
ensuring data immutability, secure communication, and resistance to
unauthorized access. The third phase employs a lightweight Long Short-Term
Memory (LSTM) model for anomaly detection and classification, enabling
real-time identification of cyber threats. Simulation results demonstrate that
the proposed framework outperforms existing methods, achieving a 2% increase in
precision, accuracy, and recall, a 5% higher attack detection rate, and a 3%
reduction in false alarm rate. These improvements highlight the framework's
ability to address critical security concerns while maintaining scalability and
real-time performance.

</details>


### [26] [Forward to Hell? On the Potentials of Misusing Transparent DNS Forwarders in Reflective Amplification Attacks](https://arxiv.org/abs/2510.18572)
*Maynard Koch,Florian Dolzmann,Thomas C. Schmidt,Matthias WÃ¤hlisch*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The DNS infrastructure is infamous for facilitating reflective amplification
attacks. Various countermeasures such as server shielding, access control, rate
limiting, and protocol restrictions have been implemented. Still, the threat
remains throughout the deployment of DNS servers. In this paper, we report on
and evaluate the often unnoticed threat that derives from transparent DNS
forwarders, a widely deployed, incompletely functional set of DNS components.
Transparent DNS forwarders transfer DNS requests without rebuilding packets
with correct source addresses. As such, transparent forwarders feed DNS
requests into (mainly powerful and anycasted) open recursive resolvers, which
thereby can be misused to participate unwillingly in distributed reflective
amplification attacks. We show how transparent forwarders raise severe threats
to the Internet infrastructure. They easily circumvent rate limiting and
achieve an additional, scalable impact via the DNS anycast infrastructure. We
empirically verify this scaling behavior up to a factor of 14. Transparent
forwarders can also assist in bypassing firewall rules that protect recursive
resolvers, making these shielded infrastructure entities part of the global DNS
attack surface.

</details>


### [27] [CLASP: Cost-Optimized LLM-based Agentic System for Phishing Detection](https://arxiv.org/abs/2510.18585)
*Fouad Trad,Ali Chehab*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Phishing websites remain a significant cybersecurity threat, necessitating
accurate and cost-effective detection mechanisms. In this paper, we present
CLASP, a novel system that effectively identifies phishing websites by
leveraging multiple intelligent agents, built using large language models
(LLMs), to analyze different aspects of a web resource. The system processes
URLs or QR codes, employing specialized LLM-based agents that evaluate the URL
structure, webpage screenshot, and HTML content to predict potential phishing
threats. To optimize performance while minimizing operational costs, we
experimented with multiple combination strategies for agent-based analysis,
ultimately designing a strategic combination that ensures the per-website
evaluation expense remains minimal without compromising detection accuracy. We
tested various LLMs, including Gemini 1.5 Flash and GPT-4o mini, to build these
agents and found that Gemini 1.5 Flash achieved the best performance with an F1
score of 83.01% on a newly curated dataset. Also, the system maintained an
average processing time of 2.78 seconds per website and an API cost of around
$3.18 per 1,000 websites. Moreover, CLASP surpasses leading previous solutions,
achieving over 40% higher recall and a 20% improvement in F1 score for phishing
detection on the collected dataset. To support further research, we have made
our dataset publicly available, supporting the development of more advanced
phishing detection systems.

</details>


### [28] [Evaluating Large Language Models in detecting Secrets in Android Apps](https://arxiv.org/abs/2510.18601)
*Marco Alecci,Jordan Samhi,TegawendÃ© F. BissyandÃ©,Jacques Klein*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Mobile apps often embed authentication secrets, such as API keys, tokens, and
client IDs, to integrate with cloud services. However, developers often
hardcode these credentials into Android apps, exposing them to extraction
through reverse engineering. Once compromised, adversaries can exploit secrets
to access sensitive data, manipulate resources, or abuse APIs, resulting in
significant security and financial risks. Existing detection approaches, such
as regex-based analysis, static analysis, and machine learning, are effective
for identifying known patterns but are fundamentally limited: they require
prior knowledge of credential structures, API signatures, or training data.
  In this paper, we propose SecretLoc, an LLM-based approach for detecting
hardcoded secrets in Android apps. SecretLoc goes beyond pattern matching; it
leverages contextual and structural cues to identify secrets without relying on
predefined patterns or labeled training sets. Using a benchmark dataset from
the literature, we demonstrate that SecretLoc detects secrets missed by regex-,
static-, and ML-based methods, including previously unseen types of secrets. In
total, we discovered 4828 secrets that were undetected by existing approaches,
discovering more than 10 "new" types of secrets, such as OpenAI API keys,
GitHub Access Tokens, RSA private keys, and JWT tokens, and more.
  We further extend our analysis to newly crawled apps from Google Play, where
we uncovered and responsibly disclosed additional hardcoded secrets. Across a
set of 5000 apps, we detected secrets in 2124 apps (42.5%), several of which
were confirmed and remediated by developers after we contacted them. Our
results reveal a dual-use risk: if analysts can uncover these secrets with
LLMs, so can attackers. This underscores the urgent need for proactive secret
management and stronger mitigation practices across the mobile ecosystem.

</details>


### [29] [DRsam: Detection of Fault-Based Microarchitectural Side-Channel Attacks in RISC-V Using Statistical Preprocessing and Association Rule Mining](https://arxiv.org/abs/2510.18612)
*Muhammad Hassan,Maria Mushtaq,Jaan Raik,Tara Ghasempouri*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: RISC-V processors are becoming ubiquitous in critical applications, but their
susceptibility to microarchitectural side-channel attacks is a serious concern.
Detection of microarchitectural attacks in RISC-V is an emerging research topic
that is relatively underexplored, compared to x86 and ARM. The first line of
work to detect flush+fault-based microarchitectural attacks in RISC-V leverages
Machine Learning (ML) models, yet it leaves several practical aspects that need
further investigation. To address overlooked issues, we leveraged gem5 and
propose a new detection method combining statistical preprocessing and
association rule mining having reconfiguration capabilities to generalize the
detection method for any microarchitectural attack. The performance comparison
with state-of-the-art reveals that the proposed detection method achieves up to
5.15% increase in accuracy, 7% rise in precision, and 3.91% improvement in
recall under the cryptographic, computational, and memory-intensive workloads
alongside its flexibility to detect new variant of flush+fault attack.
Moreover, as the attack detection relies on association rules, their
human-interpretable nature provides deep insight to understand
microarchitectural behavior during the execution of attack and benign
applications.

</details>


### [30] [Qatsi: Stateless Secret Generation via Hierarchical Memory-Hard Key Derivation](https://arxiv.org/abs/2510.18614)
*RenÃ© Coignard,Anton Rygin*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present Qatsi, a hierarchical key derivation scheme using Argon2id that
generates reproducible cryptographic secrets without persistent storage. The
system eliminates vault-based attack surfaces by deriving all secrets
deterministically from a single high-entropy master secret and contextual
layers. Outputs achieve 103-312 bits of entropy through memory-hard derivation
(64-128 MiB, 16-32 iterations) and provably uniform rejection sampling over
7776-word mnemonics or 90-character passwords. We formalize the hierarchical
construction, prove output uniformity, and quantify GPU attack costs: $2.4
\times 10^{16}$ years for 80-bit master secrets on single-GPU adversaries under
Paranoid parameters (128 MiB memory). The implementation in Rust provides
automatic memory zeroization, compile-time wordlist integrity verification, and
comprehensive test coverage. Reference benchmarks on Apple M1 Pro (2021)
demonstrate practical usability with 544 ms Standard mode and 2273 ms Paranoid
mode single-layer derivations. Qatsi targets air-gapped systems and master
credential generation where stateless reproducibility outweighs rotation
flexibility.

</details>


### [31] [Exploring Membership Inference Vulnerabilities in Clinical Large Language Models](https://arxiv.org/abs/2510.18674)
*Alexander Nemecek,Zebin Yun,Zahra Rahmani,Yaniv Harel,Vipin Chaudhary,Mahmood Sharif,Erman Ayday*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As large language models (LLMs) become progressively more embedded in
clinical decision-support, documentation, and patient-information systems,
ensuring their privacy and trustworthiness has emerged as an imperative
challenge for the healthcare sector. Fine-tuning LLMs on sensitive electronic
health record (EHR) data improves domain alignment but also raises the risk of
exposing patient information through model behaviors. In this work-in-progress,
we present an exploratory empirical study on membership inference
vulnerabilities in clinical LLMs, focusing on whether adversaries can infer if
specific patient records were used during model training. Using a
state-of-the-art clinical question-answering model, Llemr, we evaluate both
canonical loss-based attacks and a domain-motivated paraphrasing-based
perturbation strategy that more realistically reflects clinical adversarial
conditions. Our preliminary findings reveal limited but measurable membership
leakage, suggesting that current clinical LLMs provide partial resistance yet
remain susceptible to subtle privacy risks that could undermine trust in
clinical AI adoption. These results motivate continued development of
context-aware, domain-specific privacy evaluations and defenses such as
differential privacy fine-tuning and paraphrase-aware training, to strengthen
the security and trustworthiness of healthcare AI systems.

</details>


### [32] [International Students and Scams: At Risk Abroad](https://arxiv.org/abs/2510.18715)
*Katherine Zhang,Arjun Arunasalam,Pubali Datta,Z. Berkay Celik*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: International students (IntlS) in the US refer to foreign students who
acquire student visas to study in the US, primarily in higher education. As
IntlS arrive in the US, they face several challenges, such as adjusting to a
new country and culture, securing housing remotely, and arranging finances for
tuition and personal expenses. These experiences, coupled with recent events
such as visa revocations and the cessation of new visas, compound IntlS' risk
of being targeted by and falling victim to online scams. While prior work has
investigated IntlS' security and privacy, as well as general end users'
reactions to online scams, research on how IntlS are uniquely impacted by scams
remains largely absent.
  To address this gap, we conduct a two-phase user study comprising surveys
(n=48) and semi-structured interviews (n=9). We investigate IntlS' exposure and
interactions with scams, post-exposure actions such as reporting, and their
perceptions of the usefulness of existing prevention resources and the barriers
to following prevention advice. We find that IntlS are often targeted by scams
(e.g., attackers impersonating government officials) and fear legal
implications or deportation, which directly impacts their interactions with
scams (e.g., they may prolong engagement with a scammer due to a sense of
urgency). Interestingly, we also find that IntlS may lack awareness of - or
access to - reliable resources that inform them about scams or guide them in
reporting incidents to authorities. In fact, they may also face unique barriers
in enacting scam prevention advice, such as avoiding reporting financial
losses, since IntlS are required to demonstrate financial ability to stay in
the US. The findings produced by our study help synthesize guidelines for
stakeholders to better aid IntlS in reacting to scams.

</details>


### [33] [HarmNet: A Framework for Adaptive Multi-Turn Jailbreak Attacks on Large Language Models](https://arxiv.org/abs/2510.18728)
*Sidhant Narula,Javad Rafiei Asl,Mohammad Ghasemigol,Eduardo Blanco,Daniel Takabi*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) remain vulnerable to multi-turn jailbreak
attacks. We introduce HarmNet, a modular framework comprising ThoughtNet, a
hierarchical semantic network; a feedback-driven Simulator for iterative query
refinement; and a Network Traverser for real-time adaptive attack execution.
HarmNet systematically explores and refines the adversarial space to uncover
stealthy, high-success attack paths. Experiments across closed-source and
open-source LLMs show that HarmNet outperforms state-of-the-art methods,
achieving higher attack success rates. For example, on Mistral-7B, HarmNet
achieves a 99.4% attack success rate, 13.9% higher than the best baseline.
Index terms: jailbreak attacks; large language models; adversarial framework;
query refinement.

</details>


### [34] [sNVMe-oF: Secure and Efficient Disaggregated Storage](https://arxiv.org/abs/2510.18756)
*Marcin Chrapek,Meni Orenbach,Ahmad Atamli,Marcin Copik,Fritz Alder,Torsten Hoefler*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Disaggregated storage with NVMe-over-Fabrics (NVMe-oF) has emerged as the
standard solution in modern data centers, achieving superior performance,
resource utilization, and power efficiency. Simultaneously, confidential
computing (CC) is becoming the de facto security paradigm, enforcing stronger
isolation and protection for sensitive workloads. However, securing
state-of-the-art storage with traditional CC methods struggles to scale and
compromises performance or security. To address these issues, we introduce
sNVMe-oF, a storage management system extending the NVMe-oF protocol and
adhering to the CC threat model by providing confidentiality, integrity, and
freshness guarantees. sNVMe-oF offers an appropriate control path and novel
concepts such as counter-leasing. sNVMe-oF also optimizes data path performance
by leveraging NVMe metadata, introducing a new disaggregated Hazel Merkle Tree
(HMT), and avoiding redundant IPSec protections. We achieve this without
modifying the NVMe-oF protocol. To prevent excessive resource usage while
delivering line rate, sNVMe-oF also uses accelerators of CC-capable smart NICs.
We prototype sNVMe-oF on an NVIDIA BlueField-3 and demonstrate how it can
achieve as little as 2% performance degradation for synthetic patterns and AI
training.

</details>
