<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 15]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Learning to Attack: Uncovering Privacy Risks in Sequential Data Releases](https://arxiv.org/abs/2510.24807)
*Ziyao Cui,Minxing Zhang,Jian Pei*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Privacy concerns have become increasingly critical in modern AI and data
science applications, where sensitive information is collected, analyzed, and
shared across diverse domains such as healthcare, finance, and mobility. While
prior research has focused on protecting privacy in a single data release, many
real-world systems operate under sequential or continuous data publishing,
where the same or related data are released over time. Such sequential
disclosures introduce new vulnerabilities, as temporal correlations across
releases may enable adversaries to infer sensitive information that remains
hidden in any individual release. In this paper, we investigate whether an
attacker can compromise privacy in sequential data releases by exploiting
dependencies between consecutive publications, even when each individual
release satisfies standard privacy guarantees. To this end, we propose a novel
attack model that captures these sequential dependencies by integrating a
Hidden Markov Model with a reinforcement learning-based bi-directional
inference mechanism. This enables the attacker to leverage both earlier and
later observations in the sequence to infer private information. We instantiate
our framework in the context of trajectory data, demonstrating how an adversary
can recover sensitive locations from sequential mobility datasets. Extensive
experiments on Geolife, Porto Taxi, and SynMob datasets show that our model
consistently outperforms baseline approaches that treat each release
independently. The results reveal a fundamental privacy risk inherent to
sequential data publishing, where individually protected releases can
collectively leak sensitive information when analyzed temporally. These
findings underscore the need for new privacy-preserving frameworks that
explicitly model temporal dependencies, such as time-aware differential privacy
or sequential data obfuscation strategies.

</details>


### [2] [S3C2 Summit 2025-03: Industry Secure Supply Chain Summit](https://arxiv.org/abs/2510.24920)
*Elizabeth Lin,Jonah Ghebremichael,William Enck,Yasemin Acar,Michel Cukier,Alexandros Kapravelos,Christian Kastner,Laurie Williams*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Software supply chains, while providing immense economic and software
development value, are only as strong as their weakest link. Over the past
several years, there has been an exponential increase in cyberattacks
specifically targeting vulnerable links in critical software supply chains.
These attacks disrupt the day-to-day functioning and threaten the security of
nearly everyone on the internet, from billion-dollar companies and government
agencies to hobbyist open-source developers. The ever-evolving threat of
software supply chain attacks has garnered interest from both the software
industry and US government in improving software supply chain security. On
Thursday, March 6th, 2025, four researchers from the NSF-backed Secure Software
Supply Chain Center (S3C2) conducted a Secure Software Supply Chain Summit with
a diverse set of 18 practitioners from 17 organizations. The goals of the
Summit were: (1) to enable sharing between participants from different
industries regarding practical experiences and challenges with software supply
chain security; (2) to help form new collaborations; and (3) to learn about the
challenges facing participants to inform our future research directions. The
summit consisted of discussions of six topics relevant to the government
agencies represented, including software bill of materials (SBOMs); compliance;
malicious commits; build infrastructure; culture; and large language models
(LLMs) and security. For each topic of discussion, we presented a list of
questions to participants to spark conversation. In this report, we provide a
summary of the summit. The open questions and challenges that remained after
each topic are listed at the end of each topic's section, and the initial
discussion questions for each topic are provided in the appendix.

</details>


### [3] [Hammering the Diagnosis: Rowhammer-Induced Stealthy Trojan Attacks on ViT-Based Medical Imaging](https://arxiv.org/abs/2510.24976)
*Banafsheh Saber Latibari,Najmeh Nazari,Hossein Sayadi,Houman Homayoun,Abhijit Mahalanobis*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Vision Transformers (ViTs) have emerged as powerful architectures in medical
image analysis, excelling in tasks such as disease detection, segmentation, and
classification. However, their reliance on large, attention-driven models makes
them vulnerable to hardware-level attacks. In this paper, we propose a novel
threat model referred to as Med-Hammer that combines the Rowhammer hardware
fault injection with neural Trojan attacks to compromise the integrity of
ViT-based medical imaging systems. Specifically, we demonstrate how malicious
bit flips induced via Rowhammer can trigger implanted neural Trojans, leading
to targeted misclassification or suppression of critical diagnoses (e.g.,
tumors or lesions) in medical scans. Through extensive experiments on benchmark
medical imaging datasets such as ISIC, Brain Tumor, and MedMNIST, we show that
such attacks can remain stealthy while achieving high attack success rates
about 82.51% and 92.56% in MobileViT and SwinTransformer, respectively. We
further investigate how architectural properties, such as model sparsity,
attention weight distribution, and the number of features of the layer, impact
attack effectiveness. Our findings highlight a critical and underexplored
intersection between hardware-level faults and deep learning security in
healthcare applications, underscoring the urgent need for robust defenses
spanning both model architectures and underlying hardware platforms.

</details>


### [4] [FaRAccel: FPGA-Accelerated Defense Architecture for Efficient Bit-Flip Attack Resilience in Transformer Models](https://arxiv.org/abs/2510.24985)
*Najmeh Nazari,Banafsheh Saber Latibari,Elahe Hosseini,Fatemeh Movafagh,Chongzhou Fang,Hosein Mohammadi Makrani,Kevin Immanuel Gubbi,Abhijit Mahalanobis,Setareh Rafatirad,Hossein Sayadi,Houman Homayoun*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Forget and Rewire (FaR) methodology has demonstrated strong resilience
against Bit-Flip Attacks (BFAs) on Transformer-based models by obfuscating
critical parameters through dynamic rewiring of linear layers. However, the
application of FaR introduces non-negligible performance and memory overheads,
primarily due to the runtime modification of activation pathways and the lack
of hardware-level optimization. To overcome these limitations, we propose
FaRAccel, a novel hardware accelerator architecture implemented on FPGA,
specifically designed to offload and optimize FaR operations. FaRAccel
integrates reconfigurable logic for dynamic activation rerouting, and
lightweight storage of rewiring configurations, enabling low-latency inference
with minimal energy overhead. We evaluate FaRAccel across a suite of
Transformer models and demonstrate substantial reductions in FaR inference
latency and improvement in energy efficiency, while maintaining the robustness
gains of the original FaR methodology. To the best of our knowledge, this is
the first hardware-accelerated defense against BFAs in Transformers,
effectively bridging the gap between algorithmic resilience and efficient
deployment on real-world AI platforms.

</details>


### [5] [SLIP-SEC: Formalizing Secure Protocols for Model IP Protection](https://arxiv.org/abs/2510.24999)
*Racchit Jain,Satya Lokam,Yehonathan Refael,Adam Hakim,Lev Greenberg,Jay Tenenbaum*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) represent valuable intellectual property (IP),
reflecting significant investments in training data, compute, and expertise.
Deploying these models on partially trusted or insecure devices introduces
substantial risk of model theft, making it essential to design inference
protocols with provable security guarantees.
  We present the formal framework and security foundations of SLIP, a hybrid
inference protocol that splits model computation between a trusted and an
untrusted resource. We define and analyze the key notions of model
decomposition and hybrid inference protocols, and introduce formal properties
including safety, correctness, efficiency, and t-soundness. We construct secure
inference protocols based on additive decompositions of weight matrices,
combined with masking and probabilistic verification techniques. We prove that
these protocols achieve information-theoretic security against
honest-but-curious adversaries, and provide robustness against malicious
adversaries with negligible soundness error.
  This paper focuses on the theoretical underpinnings of SLIP: precise
definitions, formal protocols, and proofs of security. Empirical validation and
decomposition heuristics appear in the companion SLIP paper. Together, the two
works provide a complete account of securing LLM IP via hybrid inference,
bridging both practice and theory.

</details>


### [6] [Secure Retrieval-Augmented Generation against Poisoning Attacks](https://arxiv.org/abs/2510.25025)
*Zirui Cheng,Jikai Sun,Anjun Gao,Yueyang Quan,Zhuqing Liu,Xiaohua Hu,Minghong Fang*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) have transformed natural language processing
(NLP), enabling applications from content generation to decision support.
Retrieval-Augmented Generation (RAG) improves LLMs by incorporating external
knowledge but also introduces security risks, particularly from data poisoning,
where the attacker injects poisoned texts into the knowledge database to
manipulate system outputs. While various defenses have been proposed, they
often struggle against advanced attacks. To address this, we introduce RAGuard,
a detection framework designed to identify poisoned texts. RAGuard first
expands the retrieval scope to increase the proportion of clean texts, reducing
the likelihood of retrieving poisoned content. It then applies chunk-wise
perplexity filtering to detect abnormal variations and text similarity
filtering to flag highly similar texts. This non-parametric approach enhances
RAG security, and experiments on large-scale datasets demonstrate its
effectiveness in detecting and mitigating poisoning attacks, including strong
adaptive attacks.

</details>


### [7] [AgentCyTE: Leveraging Agentic AI to Generate Cybersecurity Training & Experimentation Scenarios](https://arxiv.org/abs/2510.25189)
*Ana M. Rodriguez,Jaime Acosta,Anantaa Kotal,Aritran Piplai*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Designing realistic and adaptive networked threat scenarios remains a core
challenge in cybersecurity research and training, still requiring substantial
manual effort. While large language models (LLMs) show promise for automated
synthesis, unconstrained generation often yields configurations that fail
validation or execution. We present AgentCyTE, a framework integrating
LLM-based reasoning with deterministic, schema-constrained network emulation to
generate and refine executable threat environments. Through an agentic feedback
loop, AgentCyTE observes scenario outcomes, validates correctness, and
iteratively enhances realism and consistency. This hybrid approach preserves
LLM flexibility while enforcing structural validity, enabling scalable,
data-driven experimentation and reliable scenario generation for threat
modeling and adaptive cybersecurity training. Our framework can be accessed at:
https://github.com/AnantaaKotal/AgentCyTE

</details>


### [8] [Is Protective DNS Blocking the Wild West?](https://arxiv.org/abs/2510.25352)
*David Plonka,Branden Palacio,Debbie Perouli*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We perform a passive measurement study investigating how a Protective DNS
service might perform in a Research & Education Network serving hundreds of
member institutions. Utilizing freely-available DNS blocklists consisting of
domain names deemed to be threats, we test hundreds of millions of users' real
DNS queries, observed over a week's time, to find which answers would be
blocked because they involve domain names that are potential threats. We find
the blocklists disorderly regarding their names, goals, transparency, and
provenance making them quite difficult to compare. Consequently, these
Protective DNS underpinnings lack organized oversight, presenting challenges
and risks in operation at scale.

</details>


### [9] [From ECU to VSOC: UDS Security Monitoring Strategies](https://arxiv.org/abs/2510.25375)
*Ali Recai Yekta,Nicolas Loza,Jens Gramm,Michael Peter Schneider,Stefan Katzenbeisser*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Increasing complexity and connectivity of modern vehicles have heightened
their vulnerability to cyberattacks. This paper addresses security challenges
associated with the Unified Diagnostic Services (UDS) protocol, a critical
communication framework for vehicle diagnostics in the automotive industry. We
present security monitoring strategies for the UDS protocol that leverage
in-vehicle logging and remote analysis through a Vehicle Security Operations
Center (VSOC). Our approach involves specifying security event logging
requirements, contextual data collection, and the development of detection
strategies aimed at identifying UDS attack scenarios. By applying these
strategies to a comprehensive taxonomy of UDS attack techniques, we demonstrate
that our detection methods cover a wide range of potential attack vectors.
Furthermore, we assess the adequacy of current AUTOSAR standardized security
events in supporting UDS attack detection, identifying gaps in the current
standard. This work enhances the understanding of vehicle security monitoring
and provides an example for developing robust cybersecurity measures in
automotive communication protocols.

</details>


### [10] [An In-Depth Analysis of Cyber Attacks in Secured Platforms](https://arxiv.org/abs/2510.25470)
*Parick Ozoh,John K Omoniyi,Bukola Ibitoye*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: There is an increase in global malware threats. To address this, an
encryption-type ransomware has been introduced on the Android operating system.
The challenges associated with malicious threats in phone use have become a
pressing issue in mobile communication, disrupting user experiences and posing
significant privacy threats. This study surveys commonly used machine learning
techniques for detecting malicious threats in phones and examines their
performance. The majority of past research focuses on customer feedback and
reviews, with concerns that people might create false reviews to promote or
devalue products and services for personal gain. Hence, the development of
techniques for detecting malicious threats using machine learning has been a
key focus. This paper presents a comprehensive comparative study of current
research on the issue of malicious threats and methods for tackling these
challenges. Nevertheless, a huge amount of information is required by these
methods, presenting a challenge for developing robust, specialized automated
anti-malware systems. This research describes the Android Applications dataset,
and the accuracy of the techniques is measured using the accuracy levels of the
metrics employed in this study.

</details>


### [11] [NetEcho: From Real-World Streaming Side-Channels to Full LLM Conversation Recovery](https://arxiv.org/abs/2510.25472)
*Zheng Zhang,Guanlong Wu,Sen Deng,Shuai Wang,Yinqian Zhang*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In the rapidly expanding landscape of Large Language Model (LLM)
applications, real-time output streaming has become the dominant interaction
paradigm. While this enhances user experience, recent research reveals that it
exposes a non-trivial attack surface through network side-channels. Adversaries
can exploit patterns in encrypted traffic to infer sensitive information and
reconstruct private conversations. In response, LLM providers and third-party
services are deploying defenses such as traffic padding and obfuscation to
mitigate these vulnerabilities.
  This paper starts by presenting a systematic analysis of contemporary
side-channel defenses in mainstream LLM applications, with a focus on services
from vendors like OpenAI and DeepSeek. We identify and examine seven
representative deployment scenarios, each incorporating active/passive
mitigation techniques. Despite these enhanced security measures, our
investigation uncovers significant residual information that remains vulnerable
to leakage within the network traffic.
  Building on this discovery, we introduce NetEcho, a novel, LLM-based
framework that comprehensively unleashes the network side-channel risks of
today's LLM applications. NetEcho is designed to recover entire conversations
-- including both user prompts and LLM responses -- directly from encrypted
network traffic. It features a deliberate design that ensures high-fidelity
text recovery, transferability across different deployment scenarios, and
moderate operational cost. In our evaluations on medical and legal applications
built upon leading models like DeepSeek-v3 and GPT-4o, NetEcho can recover avg
$\sim$70\% information of each conversation, demonstrating a critical
limitation in current defense mechanisms. We conclude by discussing the
implications of our findings and proposing future directions for augmenting
network traffic security.

</details>


### [12] [A Study on Privacy-Preserving Scholarship Evaluation Based on Decentralized Identity and Zero-Knowledge Proofs](https://arxiv.org/abs/2510.25477)
*Yi Chen,Bin Chen,Peichang Zhang,Da Che*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Traditional centralized scholarship evaluation processes typically require
students to submit detailed academic records and qualification information,
which exposes them to risks of data leakage and misuse, making it difficult to
simultaneously ensure privacy protection and transparent auditability. To
address these challenges, this paper proposes a scholarship evaluation system
based on Decentralized Identity (DID) and Zero-Knowledge Proofs (ZKP). The
system aggregates multidimensional ZKPs off-chain, and smart contracts verify
compliance with evaluation criteria without revealing raw scores or
computational details. Experimental results demonstrate that the proposed
solution not only automates the evaluation efficiently but also maximally
preserves student privacy and data integrity, offering a practical and
trustworthy technical paradigm for higher education scholarship programs.

</details>


### [13] [ZK-SenseLM: Verifiable Large-Model Wireless Sensing with Selective Abstention and Zero-Knowledge Attestation](https://arxiv.org/abs/2510.25677)
*Hasan Akgul,Mari Eplik,Javier Rojas,Aina Binti Abdullah,Pieter van der Merwe*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: ZK-SenseLM is a secure and auditable wireless sensing framework that pairs a
large-model encoder for Wi-Fi channel state information (and optionally mmWave
radar or RFID) with a policy-grounded decision layer and end-to-end
zero-knowledge proofs of inference. The encoder uses masked spectral
pretraining with phase-consistency regularization, plus a light cross-modal
alignment that ties RF features to compact, human-interpretable policy tokens.
To reduce unsafe actions under distribution shift, we add a calibrated
selective-abstention head; the chosen risk-coverage operating point is
registered and bound into the proof. We implement a four-stage proving
pipeline: (C1) feature sanity and commitment, (C2) threshold and version
binding, (C3) time-window binding, and (C4) PLONK-style proofs that the
quantized network, given the committed window, produced the logged action and
confidence. Micro-batched proving amortizes cost across adjacent windows, and a
gateway option offloads proofs from low-power devices. The system integrates
with differentially private federated learning and on-device personalization
without weakening verifiability: model hashes and the registered threshold are
part of each public statement. Across activity, presence or intrusion,
respiratory proxy, and RF fingerprinting tasks, ZK-SenseLM improves macro-F1
and calibration, yields favorable coverage-risk curves under perturbations, and
rejects tamper and replay with compact proofs and fast verification.

</details>


### [14] [Model Inversion Attacks Meet Cryptographic Fuzzy Extractors](https://arxiv.org/abs/2510.25687)
*Mallika Prabhakar,Louise Xu,Prateek Saxena*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Model inversion attacks pose an open challenge to privacy-sensitive
applications that use machine learning (ML) models. For example, face
authentication systems use modern ML models to compute embedding vectors from
face images of the enrolled users and store them. If leaked, inversion attacks
can accurately reconstruct user faces from the leaked vectors. There is no
systematic characterization of properties needed in an ideal defense against
model inversion, even for the canonical example application of a face
authentication system susceptible to data breaches, despite a decade of
best-effort solutions.
  In this paper, we formalize the desired properties of a provably strong
defense against model inversion and connect it, for the first time, to the
cryptographic concept of fuzzy extractors. We further show that existing fuzzy
extractors are insecure for use in ML-based face authentication. We do so
through a new model inversion attack called PIPE, which achieves a success rate
of over 89% in most cases against prior schemes. We then propose L2FE-Hash, the
first candidate fuzzy extractor which supports standard Euclidean distance
comparators as needed in many ML-based applications, including face
authentication. We formally characterize its computational security guarantees,
even in the extreme threat model of full breach of stored secrets, and
empirically show its usable accuracy in face authentication for practical face
distributions. It offers attack-agnostic security without requiring any
re-training of the ML model it protects. Empirically, it nullifies both prior
state-of-the-art inversion attacks as well as our new PIPE attack.

</details>


### [15] [Exact zCDP Characterizations for Fundamental Differentially Private Mechanisms](https://arxiv.org/abs/2510.25746)
*Charlie Harrison,Pasin Manurangsi*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Zero-concentrated differential privacy (zCDP) is a variant of differential
privacy (DP) that is widely used partly thanks to its nice composition
property. While a tight conversion from $\epsilon$-DP to zCDP exists for the
worst-case mechanism, many common algorithms satisfy stronger guarantees. In
this work, we derive tight zCDP characterizations for several fundamental
mechanisms. We prove that the tight zCDP bound for the $\epsilon$-DP Laplace
mechanism is exactly $\epsilon + e^{-\epsilon} - 1$, confirming a recent
conjecture by Wang (2022). We further provide tight bounds for the discrete
Laplace mechanism, $k$-Randomized Response (for $k \leq 6$), and RAPPOR.
Lastly, we also provide a tight zCDP bound for the worst case bounded range
mechanism.

</details>
