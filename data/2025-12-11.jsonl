{"id": "2512.09049", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.09049", "abs": "https://arxiv.org/abs/2512.09049", "authors": ["Gandham Sai Santhosh", "Siddhartha Sanjay Naik", "Ritwik Badola", "Chester Rebeiro"], "title": "EMMap: A Systematic Framework for Spatial EMFI Mapping and Fault Classification on Microcontrollers", "comment": null, "summary": "Electromagnetic Fault Injection (EMFI) is a powerful technique for inducing bit flips and instruction-level perturbations on microcontrollers, yet existing literature lacks a unified methodology for systematically mapping spatial sensitivity and classifying resulting fault behaviors. Building on insights from O'Flynn and Kuhnapfel et al., we introduce a platform-agnostic framework for Spatial EMFI Mapping and Fault Classification, aimed at understanding how spatial probe position influences fault outcomes. We present pilot experiments on three representative microcontroller targets including the Xtensa LX6 (ESP32) and two ChipWhisper boards not as definitive evaluations, but as illustrative demonstrations of how the proposed methodology can be applied in practice. These preliminary observations motivate a generalized and reproducible workflow that researchers can adopt when analyzing EMFI susceptibility across diverse embedded architectures."}
{"id": "2512.09150", "categories": ["cs.CR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.09150", "abs": "https://arxiv.org/abs/2512.09150", "authors": ["Anirudh Nakra", "Nayeeb Rashid", "Chau-Wai Wong", "Min Wu"], "title": "Exposing Vulnerabilities in Counterfeit Prevention Systems Utilizing Physically Unclonable Surface Features", "comment": "15 pages; This work builds on arXiv:2408.02221 [cs.CR]", "summary": "Counterfeit products pose significant risks to public health and safety through infiltrating untrusted supply chains. Among numerous anti-counterfeiting techniques, leveraging inherent, unclonable microscopic irregularities of paper surfaces is an accurate and cost-effective solution. Prior work of this approach has focused on enabling ubiquitous acquisition of these physically unclonable features (PUFs). However, we will show that existing authentication methods relying on paper surface PUFs may be vulnerable to adversaries, resulting in a gap between technological feasibility and secure real-world deployment. This gap is investigated through formalizing an operational framework for paper-PUF-based authentication. Informed by this framework, we reveal system-level vulnerabilities across both physical and digital domains, designing physical denial-of-service and digital forgery attacks to disrupt proper authentication. The effectiveness of the designed attacks underscores the strong need for security countermeasures for reliable and resilient authentication based on paper PUFs. The proposed framework further facilitates a comprehensive, stage-by-stage security analysis, guiding the design of future counterfeit prevention systems. This analysis delves into potential attack strategies, offering a foundational understanding of how various system components, such as physical features and verification processes, might be exploited by adversaries."}
{"id": "2512.09233", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.09233", "abs": "https://arxiv.org/abs/2512.09233", "authors": ["Alan T. Sherman", "Jeremy J. Romanik Romano", "Edward Zieglar", "Enis Golaszewski", "Jonathan D. Fuchs", "William E. Byrd"], "title": "Analysis of the Security Design, Engineering, and Implementation of the SecureDNA System", "comment": "A shorter version of this paper will appear in the Proceedings of the Network and Distributed System Security Symposium (NDSS) 2026 published by the Internet Society", "summary": "We analyze security aspects of the SecureDNA system regarding its system design, engineering, and implementation. This system enables DNA synthesizers to screen order requests against a database of hazards. By applying novel cryptography, the system aims to keep order requests and the database of hazards secret. Discerning the detailed operation of the system in part from source code (Version 1.0.8), our analysis examines key management, certificate infrastructure, authentication, and rate-limiting mechanisms. We also perform the first formal-methods analysis of the mutual authentication, basic request, and exemption-handling protocols.\n  Without breaking the cryptography, our main finding is that SecureDNA's custom mutual authentication protocol SCEP achieves only one-way authentication: the hazards database and keyservers never learn with whom they communicate. This structural weakness violates the principle of defense in depth and enables an adversary to circumvent rate limits that protect the secrecy of the hazards database, if the synthesizer connects with a malicious or corrupted keyserver or hashed database. We point out an additional structural weakness that also violates the principle of defense in depth: inadequate cryptographic bindings prevent the system from detecting if responses, within a TLS channel, from the hazards database were modified. Consequently, if a synthesizer were to reconnect with the database over the same TLS session, an adversary could replay and swap responses from the database without breaking TLS. Although the SecureDNA implementation does not allow such reconnections, it would be stronger security engineering to avoid the underlying structural weakness. We identify these vulnerabilities and suggest and verify mitigations, including adding strong bindings. Software Version 1.1.0 fixes SCEP with our proposed SCEP+ protocol."}
{"id": "2512.09264", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09264", "abs": "https://arxiv.org/abs/2512.09264", "authors": ["Xiaojing Chen", "Dan Li", "Lijun Peng", "Jun YanŁetter", "Zhiqing Guo", "Junyang Chen", "Xiao Lan", "Zhongjie Ba", "Yunfeng DiaoŁetter"], "title": "FBA$^2$D: Frequency-based Black-box Attack for AI-generated Image Detection", "comment": null, "summary": "The prosperous development of Artificial Intelligence-Generated Content (AIGC) has brought people's anxiety about the spread of false information on social media. Designing detectors for filtering is an effective defense method, but most detectors will be compromised by adversarial samples. Currently, most studies exposing AIGC security issues assume information on model structure and data distribution. In real applications, attackers query and interfere with models that provide services in the form of application programming interfaces (APIs), which constitutes the black-box decision-based attack paradigm. However, to the best of our knowledge, decision-based attacks on AIGC detectors remain unexplored. In this study, we propose \\textbf{FBA$^2$D}: a frequency-based black-box attack method for AIGC detection to fill the research gap. Motivated by frequency-domain discrepancies between generated and real images, we develop a decision-based attack that leverages the Discrete Cosine Transform (DCT) for fine-grained spectral partitioning and selects frequency bands as query subspaces, improving both query efficiency and image quality. Moreover, attacks on AIGC detectors should mitigate initialization failures, preserve image quality, and operate under strict query budgets. To address these issues, we adopt an ``adversarial example soup'' method, averaging candidates from successive surrogate iterations and using the result as the initialization to accelerate the query-based attack. The empirical study on the Synthetic LSUN dataset and GenImage dataset demonstrate the effectiveness of our prosed method. This study shows the urgency of addressing practical AIGC security problems."}
{"id": "2512.09321", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.09321", "abs": "https://arxiv.org/abs/2512.09321", "authors": ["Ruiqi Wang", "Yuqi Jia", "Neil Zhenqiang Gong"], "title": "ObliInjection: Order-Oblivious Prompt Injection Attack to LLM Agents with Multi-source Data", "comment": "To appear in NDSS 2026", "summary": "Prompt injection attacks aim to contaminate the input data of an LLM to mislead it into completing an attacker-chosen task instead of the intended task. In many applications and agents, the input data originates from multiple sources, with each source contributing a segment of the overall input. In these multi-source scenarios, an attacker may control only a subset of the sources and contaminate the corresponding segments, but typically does not know the order in which the segments are arranged within the input. Existing prompt injection attacks either assume that the entire input data comes from a single source under the attacker's control or ignore the uncertainty in the ordering of segments from different sources. As a result, their success is limited in domains involving multi-source data.\n  In this work, we propose ObliInjection, the first prompt injection attack targeting LLM applications and agents with multi-source input data. ObliInjection introduces two key technical innovations: the order-oblivious loss, which quantifies the likelihood that the LLM will complete the attacker-chosen task regardless of how the clean and contaminated segments are ordered; and the orderGCG algorithm, which is tailored to minimize the order-oblivious loss and optimize the contaminated segments. Comprehensive experiments across three datasets spanning diverse application domains and twelve LLMs demonstrate that ObliInjection is highly effective, even when only one out of 6-100 segments in the input data is contaminated."}
{"id": "2512.09385", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09385", "abs": "https://arxiv.org/abs/2512.09385", "authors": ["Uisang Lee", "Changhoon Chung", "Junmo Lee", "Soo-Mook Moon"], "title": "BugSweeper: Function-Level Detection of Smart Contract Vulnerabilities Using Graph Neural Networks", "comment": "This paper is accepted to AAAI 2026", "summary": "The rapid growth of Ethereum has made it more important to quickly and accurately detect smart contract vulnerabilities. While machine-learning-based methods have shown some promise, many still rely on rule-based preprocessing designed by domain experts. Rule-based preprocessing methods often discard crucial context from the source code, potentially causing certain vulnerabilities to be overlooked and limiting adaptability to newly emerging threats. We introduce BugSweeper, an end-to-end deep learning framework that detects vulnerabilities directly from the source code without manual engineering. BugSweeper represents each Solidity function as a Function-Level Abstract Syntax Graph (FLAG), a novel graph that combines its Abstract Syntax Tree (AST) with enriched control-flow and data-flow semantics. Then, our two-stage Graph Neural Network (GNN) analyzes these graphs. The first-stage GNN filters noise from the syntax graphs, while the second-stage GNN conducts high-level reasoning to detect diverse vulnerabilities. Extensive experiments on real-world contracts show that BugSweeper significantly outperforms all state-of-the-art detection methods. By removing the need for handcrafted rules, our approach offers a robust, automated, and scalable solution for securing smart contracts without any dependence on security experts."}
{"id": "2512.09409", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.09409", "abs": "https://arxiv.org/abs/2512.09409", "authors": ["Kyle Habib", "Vladislav Kapitsyn", "Giovanni Mazzeo", "Faisal Mehrban"], "title": "Proof of Trusted Execution: A Consensus Paradigm for Deterministic Blockchain Finality", "comment": "Submitted to Middleware 2026 Conference", "summary": "Current blockchain consensus protocols -- notably, Proof of Work (PoW) and Proof of Stake (PoS) -- deliver global agreement but exhibit structural constraints. PoW anchors security in heavy computation, inflating energy use and imposing high confirmation latency. PoS improves efficiency but introduces stake concentration, long-range and \"nothing-at-stake\" vulnerabilities, and a hard performance ceiling shaped by slot times and multi-round committee voting. In this paper, we propose Proof of Trusted Execution (PoTE), a consensus paradigm where agreement emerges from verifiable execution rather than replicated re-execution. Validators operate inside heterogeneous VM-based TEEs, each running the same canonical program whose measurement is publicly recorded, and each producing vendor-backed attestations that bind the enclave code hash to the block contents. Because the execution is deterministic and the proposer is uniquely derived from public randomness, PoTE avoids forks, eliminates slot.time bottlenecks, and commits blocks in a single round of verification. We present the design of a PoTE consensus client, describe our reference implementation, and evaluate its performance against the stringent throughput requirements of the Trillion decentralized exchange."}
{"id": "2512.09442", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.09442", "abs": "https://arxiv.org/abs/2512.09442", "authors": ["Xiaoxiao Chi", "Xuyun Zhang", "Yan Wang", "Hongsheng Hu", "Wanchun Dou"], "title": "Reference Recommendation based Membership Inference Attack against Hybrid-based Recommender Systems", "comment": "This paper has been accepted by AAAI 2026", "summary": "Recommender systems have been widely deployed across various domains such as e-commerce and social media, and intelligently suggest items like products and potential friends to users based on their preferences and interaction history, which are often privacy-sensitive. Recent studies have revealed that recommender systems are prone to membership inference attacks (MIAs), where an attacker aims to infer whether or not a user's data has been used for training a target recommender system. However, existing MIAs fail to exploit the unique characteristic of recommender systems, and therefore are only applicable to mixed recommender systems consisting of two recommendation algorithms. This leaves a gap in investigating MIAs against hybrid-based recommender systems where the same algorithm utilizing user-item historical interactions and attributes of users and items serves and produces personalised recommendations. To investigate how the personalisation in hybrid-based recommender systems influences MIA, we propose a novel metric-based MIA. Specifically, we leverage the characteristic of personalisation to obtain reference recommendation for any target users. Then, a relative membership metric is proposed to exploit a target user's historical interactions, target recommendation, and reference recommendation to infer the membership of the target user's data. Finally, we theoretically and empirically demonstrate the efficacy of the proposed metric-based MIA on hybrid-based recommender systems."}
{"id": "2512.09485", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09485", "abs": "https://arxiv.org/abs/2512.09485", "authors": ["Xinye Cao", "Yihan Lin", "Guoshun Nan", "Qinchuan Zhou", "Yuhang Luo", "Yurui Gao", "Zeliang Zhang", "Haolang Lu", "Qimei Cui", "Yanzhao Hou", "Xiaofeng Tao", "Tony Q. S. Quek"], "title": "Advancing LLM-Based Security Automation with Customized Group Relative Policy Optimization for Zero-Touch Networks", "comment": "Accepted by IEEE JSAC. This work has been submitted to the IEEE for possible publication", "summary": "Zero-Touch Networks (ZTNs) represent a transformative paradigm toward fully automated and intelligent network management, providing the scalability and adaptability required for the complexity of sixth-generation (6G) networks. However, the distributed architecture, high openness, and deep heterogeneity of 6G networks expand the attack surface and pose unprecedented security challenges. To address this, security automation aims to enable intelligent security management across dynamic and complex environments, serving as a key capability for securing 6G ZTNs. Despite its promise, implementing security automation in 6G ZTNs presents two primary challenges: 1) automating the lifecycle from security strategy generation to validation and update under real-world, parallel, and adversarial conditions, and 2) adapting security strategies to evolving threats and dynamic environments. This motivates us to propose SecLoop and SA-GRPO. SecLoop constitutes the first fully automated framework that integrates large language models (LLMs) across the entire lifecycle of security strategy generation, orchestration, response, and feedback, enabling intelligent and adaptive defenses in dynamic network environments, thus tackling the first challenge. Furthermore, we propose SA-GRPO, a novel security-aware group relative policy optimization algorithm that iteratively refines security strategies by contrasting group feedback collected from parallel SecLoop executions, thereby addressing the second challenge. Extensive real-world experiments on five benchmarks, including 11 MITRE ATT&CK processes and over 20 types of attacks, demonstrate the superiority of the proposed SecLoop and SA-GRPO. We will release our platform to the community, facilitating the advancement of security automation towards next generation communications."}
{"id": "2512.09539", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09539", "abs": "https://arxiv.org/abs/2512.09539", "authors": ["Aink Acrie Soe Thein", "Nikolaos Pitropakis", "Pavlos Papadopoulos", "Sam Grierson", "Sana Ullah Jan"], "title": "Comparative Analysis of Hash-based Malware Clustering via K-Means", "comment": "To be published in the proceedings of the 8th International Conference on Reliable Information and Communication Technology (IRICT 2025). Springer Book Series: \"Lecture Notes on Data Engineering and Communications Technologies\"", "summary": "With the adoption of multiple digital devices in everyday life, the cyber-attack surface has increased. Adversaries are continuously exploring new avenues to exploit them and deploy malware. On the other hand, detection approaches typically employ hashing-based algorithms such as SSDeep, TLSH, and IMPHash to capture structural and behavioural similarities among binaries. This work focuses on the analysis and evaluation of these techniques for clustering malware samples using the K-means algorithm. More specifically, we experimented with established malware families and traits and found that TLSH and IMPHash produce more distinct, semantically meaningful clusters, whereas SSDeep is more efficient for broader classification tasks. The findings of this work can guide the development of more robust threat-detection mechanisms and adaptive security mechanisms."}
{"id": "2512.09549", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.09549", "abs": "https://arxiv.org/abs/2512.09549", "authors": ["Jonathan Evertz", "Niklas Risse", "Nicolai Neuer", "Andreas Müller", "Philipp Normann", "Gaetano Sapia", "Srishti Gupta", "David Pape", "Soumya Shaw", "Devansh Srivastav", "Christian Wressnegger", "Erwin Quiring", "Thorsten Eisenhofer", "Daniel Arp", "Lea Schönherr"], "title": "Chasing Shadows: Pitfalls in LLM Security Research", "comment": "About to appear at NDSS'26", "summary": "Large language models (LLMs) are increasingly prevalent in security research. Their unique characteristics, however, introduce challenges that undermine established paradigms of reproducibility, rigor, and evaluation. Prior work has identified common pitfalls in traditional machine learning research, but these studies predate the advent of LLMs. In this paper, we identify \\emph{nine} common pitfalls that have become (more) relevant with the emergence of LLMs and that can compromise the validity of research involving them. These pitfalls span the entire computation process, from data collection, pre-training, and fine-tuning to prompting and evaluation.\n  We assess the prevalence of these pitfalls across all 72 peer-reviewed papers published at leading Security and Software Engineering venues between 2023 and 2024. We find that every paper contains at least one pitfall, and each pitfall appears in multiple papers. Yet only 15.7\\% of the present pitfalls were explicitly discussed, suggesting that the majority remain unrecognized. To understand their practical impact, we conduct four empirical case studies showing how individual pitfalls can mislead evaluation, inflate performance, or impair reproducibility. Based on our findings, we offer actionable guidelines to support the community in future work."}
{"id": "2512.09769", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.09769", "abs": "https://arxiv.org/abs/2512.09769", "authors": ["Hanzhou Wu", "Yige Wang"], "title": "Defining Cost Function of Steganography with Large Language Models", "comment": "https://scholar.google.com/citations?hl=en&user=IdiF7M0AAAAJ", "summary": "In this paper, we make the first attempt towards defining cost function of steganography with large language models (LLMs), which is totally different from previous works that rely heavily on expert knowledge or require large-scale datasets for cost learning. To achieve this goal, a two-stage strategy combining LLM-guided program synthesis with evolutionary search is applied in the proposed method. In the first stage, a certain number of cost functions in the form of computer program are synthesized from LLM responses to structured prompts. These cost functions are then evaluated with pretrained steganalysis models so that candidate cost functions suited to steganography can be collected. In the second stage, by retraining a steganalysis model for each candidate cost function, the optimal cost function(s) can be determined according to the detection accuracy. This two-stage strategy is performed by an iterative fashion so that the best cost function can be collected at the last iteration. Experiments show that the proposed method enables LLMs to design new cost functions of steganography that significantly outperform existing works in terms of resisting steganalysis tools, which verifies the superiority of the proposed method. To the best knowledge of the authors, this is the first work applying LLMs to the design of advanced cost function of steganography, which presents a novel perspective for steganography design and may shed light on further research."}
{"id": "2512.09872", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09872", "abs": "https://arxiv.org/abs/2512.09872", "authors": ["Khurram Khalil", "Khaza Anuarul Hoque"], "title": "FlipLLM: Efficient Bit-Flip Attacks on Multimodal LLMs using Reinforcement Learning", "comment": "Accepted in IEEE HOST 2026", "summary": "Generative Artificial Intelligence models, such as Large Language Models (LLMs) and Large Vision Models (VLMs), exhibit state-of-the-art performance but remain vulnerable to hardware-based threats, specifically bit-flip attacks (BFAs). Existing BFA discovery methods lack generalizability and struggle to scale, often failing to analyze the vast parameter space and complex interdependencies of modern foundation models in a reasonable time. This paper proposes FlipLLM, a reinforcement learning (RL) architecture-agnostic framework that formulates BFA discovery as a sequential decision-making problem. FlipLLM combines sensitivity-guided layer pruning with Q-learning to efficiently identify minimal, high-impact bit sets that can induce catastrophic failure. We demonstrate the effectiveness and generalizability of FlipLLM by applying it to a diverse set of models, including prominent text-only LLMs (GPT-2 Large, LLaMA 3.1 8B, and DeepSeek-V2 7B), VLMs such as LLaVA 1.6, and datasets, such as MMLU, MMLU-Pro, VQAv2, and TextVQA. Our results show that FlipLLM can identify critical bits that are vulnerable to BFAs up to 2.5x faster than SOTA methods. We demonstrate that flipping the FlipLLM-identified bits plummets the accuracy of LLaMA 3.1 8B from 69.9% to ~0.2%, and for LLaVA's VQA score from 78% to almost 0%, by flipping as few as 5 and 7 bits, respectively. Further analysis reveals that applying standard hardware protection mechanisms, such as ECC SECDED, to the FlipLLM-identified bit locations completely mitigates the BFA impact, demonstrating the practical value of our framework in guiding hardware-level defenses. FlipLLM offers the first scalable and adaptive methodology for exploring the BFA vulnerability of both language and multimodal foundation models, paving the way for comprehensive hardware-security evaluation."}
{"id": "2512.09883", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.09883", "abs": "https://arxiv.org/abs/2512.09883", "authors": ["Daniel Gibert", "Felip Manyà"], "title": "ByteShield: Adversarially Robust End-to-End Malware Detection through Byte Masking", "comment": null, "summary": "Research has proven that end-to-end malware detectors are vulnerable to adversarial attacks. In response, the research community has proposed defenses based on randomized and (de)randomized smoothing. However, these techniques remain susceptible to attacks that insert large adversarial payloads. To address these limitations, we propose a novel defense mechanism designed to harden end-to-end malware detectors by leveraging masking at the byte level. This mechanism operates by generating multiple masked versions of the input file, independently classifying each version, and then applying a threshold-based voting mechanism to produce the final classification. Key to this defense is a deterministic masking strategy that systematically strides a mask across the entire input file. Unlike randomized smoothing defenses, which randomly mask or delete bytes, this structured approach ensures coverage of the file over successive versions. In the best-case scenario, this strategy fully occludes the adversarial payload, effectively neutralizing its influence on the model's decision. In the worst-case scenario, it partially occludes the adversarial payload, reducing its impact on the model's predictions. By occluding the adversarial payload in one or more masked versions, this defense ensures that some input versions remain representative of the file's original intent, allowing the voting mechanism to suppress the influence of the adversarial payload. Results achieved on the EMBER and BODMAS datasets demonstrate the suitability of our defense, outperforming randomized and (de)randomized smoothing defenses against adversarial examples generated with a wide range of functionality-preserving manipulations while maintaining high accuracy on clean examples."}
