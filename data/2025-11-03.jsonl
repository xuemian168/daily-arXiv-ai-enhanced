{"id": "2510.26833", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26833", "abs": "https://arxiv.org/abs/2510.26833", "authors": ["Simon Yu", "Peilin Yu", "Hongbo Zheng", "Huajie Shao", "Han Zhao", "Lui Sha"], "title": "VISAT: Benchmarking Adversarial and Distribution Shift Robustness in Traffic Sign Recognition with Visual Attributes", "comment": null, "summary": "We present VISAT, a novel open dataset and benchmarking suite for evaluating\nmodel robustness in the task of traffic sign recognition with the presence of\nvisual attributes. Built upon the Mapillary Traffic Sign Dataset (MTSD), our\ndataset introduces two benchmarks that respectively emphasize robustness\nagainst adversarial attacks and distribution shifts. For our adversarial attack\nbenchmark, we employ the state-of-the-art Projected Gradient Descent (PGD)\nmethod to generate adversarial inputs and evaluate their impact on popular\nmodels. Additionally, we investigate the effect of adversarial attacks on\nattribute-specific multi-task learning (MTL) networks, revealing spurious\ncorrelations among MTL tasks. The MTL networks leverage visual attributes\n(color, shape, symbol, and text) that we have created for each traffic sign in\nour dataset. For our distribution shift benchmark, we utilize ImageNet-C's\nrealistic data corruption and natural variation techniques to perform\nevaluations on the robustness of both base and MTL models. Moreover, we further\nexplore spurious correlations among MTL tasks through synthetic alterations of\ntraffic sign colors using color quantization techniques. Our experiments focus\non two major backbones, ResNet-152 and ViT-B/32, and compare the performance\nbetween base and MTL models. The VISAT dataset and benchmarking framework\ncontribute to the understanding of model robustness for traffic sign\nrecognition, shedding light on the challenges posed by adversarial attacks and\ndistribution shifts. We believe this work will facilitate advancements in\ndeveloping more robust models for real-world applications in autonomous driving\nand cyber-physical systems."}
{"id": "2510.26847", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.26847", "abs": "https://arxiv.org/abs/2510.26847", "authors": ["Shaked Zychlinski", "Yuval Kainan"], "title": "Broken-Token: Filtering Obfuscated Prompts by Counting Characters-Per-Token", "comment": "16 pages, 9 figures", "summary": "Large Language Models (LLMs) are susceptible to jailbreak attacks where\nmalicious prompts are disguised using ciphers and character-level encodings to\nbypass safety guardrails. While these guardrails often fail to interpret the\nencoded content, the underlying models can still process the harmful\ninstructions. We introduce CPT-Filtering, a novel, model-agnostic with\nnegligible-costs and near-perfect accuracy guardrail technique that aims to\nmitigate these attacks by leveraging the intrinsic behavior of Byte-Pair\nEncoding (BPE) tokenizers. Our method is based on the principle that\ntokenizers, trained on natural language, represent out-of-distribution text,\nsuch as ciphers, using a significantly higher number of shorter tokens. Our\ntechnique uses a simple yet powerful artifact of using language models: the\naverage number of Characters Per Token (CPT) in the text. This approach is\nmotivated by the high compute cost of modern methods - relying on added modules\nsuch as dedicated LLMs or perplexity models. We validate our approach across a\nlarge dataset of over 100,000 prompts, testing numerous encoding schemes with\nseveral popular tokenizers. Our experiments demonstrate that a simple CPT\nthreshold robustly identifies encoded text with high accuracy, even for very\nshort inputs. CPT-Filtering provides a practical defense layer that can be\nimmediately deployed for real-time text filtering and offline data curation."}
{"id": "2510.26941", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26941", "abs": "https://arxiv.org/abs/2510.26941", "authors": ["Seif Ikbarieh", "Maanak Gupta", "Elmahedi Mahalal"], "title": "LLM-based Multi-class Attack Analysis and Mitigation Framework in IoT/IIoT Networks", "comment": null, "summary": "The Internet of Things has expanded rapidly, transforming communication and\noperations across industries but also increasing the attack surface and\nsecurity breaches. Artificial Intelligence plays a key role in securing IoT,\nenabling attack detection, attack behavior analysis, and mitigation suggestion.\nDespite advancements, evaluations remain purely qualitative, and the lack of a\nstandardized, objective benchmark for quantitatively measuring AI-based attack\nanalysis and mitigation hinders consistent assessment of model effectiveness.\nIn this work, we propose a hybrid framework combining Machine Learning (ML) for\nmulti-class attack detection with Large Language Models (LLMs) for attack\nbehavior analysis and mitigation suggestion. After benchmarking several ML and\nDeep Learning (DL) classifiers on the Edge-IIoTset and CICIoT2023 datasets, we\napplied structured role-play prompt engineering with Retrieval-Augmented\nGeneration (RAG) to guide ChatGPT-o3 and DeepSeek-R1 in producing detailed,\ncontext-aware responses. We introduce novel evaluation metrics for quantitative\nassessment to guide us and an ensemble of judge LLMs, namely ChatGPT-4o,\nDeepSeek-V3, Mixtral 8x7B Instruct, Gemini 2.5 Flash, Meta Llama 4, TII Falcon\nH1 34B Instruct, xAI Grok 3, and Claude 4 Sonnet, to independently evaluate the\nresponses. Results show that Random Forest has the best detection model, and\nChatGPT-o3 outperformed DeepSeek-R1 in attack analysis and mitigation."}
{"id": "2510.27080", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.27080", "abs": "https://arxiv.org/abs/2510.27080", "authors": ["Arnabh Borah", "Md Tanvirul Alam", "Nidhi Rastogi"], "title": "Adapting Large Language Models to Emerging Cybersecurity using Retrieval Augmented Generation", "comment": null, "summary": "Security applications are increasingly relying on large language models\n(LLMs) for cyber threat detection; however, their opaque reasoning often limits\ntrust, particularly in decisions that require domain-specific cybersecurity\nknowledge. Because security threats evolve rapidly, LLMs must not only recall\nhistorical incidents but also adapt to emerging vulnerabilities and attack\npatterns. Retrieval-Augmented Generation (RAG) has demonstrated effectiveness\nin general LLM applications, but its potential for cybersecurity remains\nunderexplored. In this work, we introduce a RAG-based framework designed to\ncontextualize cybersecurity data and enhance LLM accuracy in knowledge\nretention and temporal reasoning. Using external datasets and the\nLlama-3-8B-Instruct model, we evaluate baseline RAG, an optimized hybrid\nretrieval approach, and conduct a comparative analysis across multiple\nperformance metrics. Our findings highlight the promise of hybrid retrieval in\nstrengthening the adaptability and reliability of LLMs for cybersecurity tasks."}
{"id": "2510.27127", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.27127", "abs": "https://arxiv.org/abs/2510.27127", "authors": ["Kunming Yang", "Ling Chen"], "title": "Lightweight CNN Model Hashing with Higher-Order Statistics and Chaotic Mapping for Piracy Detection and Tamper Localization", "comment": null, "summary": "With the widespread adoption of deep neural networks (DNNs), protecting\nintellectual property and detecting unauthorized tampering of models have\nbecome pressing challenges. Recently, Perceptual hashing has emerged as an\neffective approach for identifying pirated models. However, existing methods\neither rely on neural networks for feature extraction, demanding substantial\ntraining resources, or suffer from limited applicability and cannot be\nuniversally applied to all convolutional neural networks (CNNs). To address\nthese limitations, we propose a lightweight CNN model hashing technique that\nintegrates higher-order statistics (HOS) features with a chaotic mapping\nmechanism. Without requiring any auxiliary neural network training, our method\nenables efficient piracy detection and precise tampering localization.\nSpecifically, we extract skewness, kurtosis, and structural features from the\nparameters of each network layer to construct a model hash that is both robust\nand discriminative. Additionally, we introduce chaotic mapping to amplify minor\nchanges in model parameters by exploiting the sensitivity of chaotic systems to\ninitial conditions, thereby facilitating accurate localization of tampered\nregions. Experimental results validate the effectiveness and practical value of\nthe proposed method for model copyright protection and integrity verification."}
{"id": "2510.27140", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.27140", "abs": "https://arxiv.org/abs/2510.27140", "authors": ["Chenghao Du", "Quanfeng Huang", "Tingxuan Tang", "Zihao Wang", "Yue Xiao"], "title": "Measuring the Security of Mobile LLM Agents under Adversarial Prompts from Untrusted Third-Party Channels", "comment": null, "summary": "Large Language Models (LLMs) have transformed software development, enabling\nAI-powered applications known as LLM-based agents that promise to automate\ntasks across diverse apps and workflows. Yet, the security implications of\ndeploying such agents in adversarial mobile environments remain poorly\nunderstood. In this paper, we present the first systematic study of security\nrisks in mobile LLM agents. We design and evaluate a suite of adversarial case\nstudies, ranging from opportunistic manipulations such as pop-up advertisements\nto advanced, end-to-end workflows involving malware installation and cross-app\ndata exfiltration. Our evaluation covers eight state-of-the-art mobile agents\nacross three architectures, with over 2,000 adversarial and paired benign\ntrials. The results reveal systemic vulnerabilities: low-barrier vectors such\nas fraudulent ads succeed with over 80% reliability, while even workflows\nrequiring the circumvention of operating-system warnings, such as malware\ninstallation, are consistently completed by advanced multi-app agents. By\nmapping these attacks to the MITRE ATT&CK Mobile framework, we uncover novel\nprivilege-escalation and persistence pathways unique to LLM-driven automation.\nCollectively, our findings provide the first end-to-end evidence that mobile\nLLM agents are exploitable in realistic adversarial settings, where untrusted\nthird-party channels (e.g., ads, embedded webviews, cross-app notifications)\nare an inherent part of the mobile ecosystem."}
{"id": "2510.27190", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.27190", "abs": "https://arxiv.org/abs/2510.27190", "authors": ["Dominik Schwarz"], "title": "Unvalidated Trust: Cross-Stage Vulnerabilities in Large Language Model Architectures", "comment": "178 pages, mechanism-centered taxonomy of 41 LLM risk patterns,\n  extensive appendix with experiment prompts and consolidation tables. Full\n  traces available to reviewers and affected providers", "summary": "As Large Language Models (LLMs) are increasingly integrated into automated,\nmulti-stage pipelines, risk patterns that arise from unvalidated trust between\nprocessing stages become a practical concern. This paper presents a\nmechanism-centered taxonomy of 41 recurring risk patterns in commercial LLMs.\nThe analysis shows that inputs are often interpreted non-neutrally and can\ntrigger implementation-shaped responses or unintended state changes even\nwithout explicit commands. We argue that these behaviors constitute\narchitectural failure modes and that string-level filtering alone is\ninsufficient. To mitigate such cross-stage vulnerabilities, we recommend\nzero-trust architectural principles, including provenance enforcement, context\nsealing, and plan revalidation, and we introduce \"Countermind\" as a conceptual\nblueprint for implementing these defenses."}
{"id": "2510.27275", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.27275", "abs": "https://arxiv.org/abs/2510.27275", "authors": ["Kathrin Grosse", "Nico Ebert"], "title": "Prevalence of Security and Privacy Risk-Inducing Usage of AI-based Conversational Agents", "comment": "10 pages, 3 figures, 5 tables, under submission", "summary": "Recent improvement gains in large language models (LLMs) have lead to\neveryday usage of AI-based Conversational Agents (CAs). At the same time, LLMs\nare vulnerable to an array of threats, including jailbreaks and, for example,\ncausing remote code execution when fed specific inputs. As a result, users may\nunintentionally introduce risks, for example, by uploading malicious files or\ndisclosing sensitive information. However, the extent to which such user\nbehaviors occur and thus potentially facilitate exploits remains largely\nunclear. To shed light on this issue, we surveyed a representative sample of\n3,270 UK adults in 2024 using Prolific. A third of these use CA services such\nas ChatGPT or Gemini at least once a week. Of these ``regular users'', up to a\nthird exhibited behaviors that may enable attacks, and a fourth have tried\njailbreaking (often out of understandable reasons such as curiosity, fun or\ninformation seeking). Half state that they sanitize data and most participants\nreport not sharing sensitive data. However, few share very sensitive data such\nas passwords. The majority are unaware that their data can be used to train\nmodels and that they can opt-out. Our findings suggest that current academic\nthreat models manifest in the wild, and mitigations or guidelines for the\nsecure usage of CAs should be developed. In areas critical to security and\nprivacy, CAs must be equipped with effective AI guardrails to prevent, for\nexample, revealing sensitive information to curious employees. Vendors need to\nincrease efforts to prevent the entry of sensitive data, and to create\ntransparency with regard to data usage policies and settings."}
{"id": "2510.27298", "categories": ["cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.27298", "abs": "https://arxiv.org/abs/2510.27298", "authors": ["Rebeka Toth", "Richard A. Dubniczky", "Olga Limonova", "Norbert Tihanyi"], "title": "Sustaining Cyber Awareness: The Long-Term Impact of Continuous Phishing Training and Emotional Triggers", "comment": "9 pages, 4 figures, IEEE BigData 2025 Conference, repository:\n  https://github.com/CorporatePhishingStudy", "summary": "Phishing constitutes more than 90\\% of successful cyberattacks globally,\nremaining one of the most persistent threats to organizational security.\nDespite organizations tripling their cybersecurity budgets between 2015 and\n2025, the human factor continues to pose a critical vulnerability. This study\npresents a 12-month longitudinal investigation examining how continuous\ncybersecurity training and emotional cues affect employee susceptibility to\nphishing. The experiment involved 20 organizations and over 1,300 employees who\ncollectively received more than 13,000 simulated phishing emails engineered\nwith diverse emotional, contextual, and structural characteristics. Behavioral\nresponses were analyzed using non-parametric correlation and regression models\nto assess the influence of psychological manipulation, message personalization,\nand perceived email source. Results demonstrate that sustained phishing\nsimulations and targeted training programs lead to a significant reduction in\nemployee susceptibility, halving successful compromise rates within six months.\nAdditionally, employee turnover introduces measurable fluctuations in awareness\nlevels, underscoring the necessity of maintaining continuous training\ninitiatives. These findings provide one of the few long-term perspectives on\nphishing awareness efficacy, highlighting the strategic importance of ongoing\nbehavioral interventions in strengthening organizational cyber resilience. In\norder to support open science, we published our email templates, source code,\nand other materials at https://github.com/CorporatePhishingStudy"}
{"id": "2510.27346", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.27346", "abs": "https://arxiv.org/abs/2510.27346", "authors": ["Wenjie Liu", "Panos Papadimitratos"], "title": "Coordinated Position Falsification Attacks and Countermeasures for Location-Based Services", "comment": null, "summary": "With the rise of location-based service (LBS) applications that rely on\nterrestrial and satellite infrastructures (e.g., GNSS and crowd-sourced Wi-Fi,\nBluetooth, cellular, and IP databases) for positioning, ensuring their\nintegrity and security is paramount. However, we demonstrate that these\napplications are susceptible to low-cost attacks (less than $50), including\nWi-Fi spoofing combined with GNSS jamming, as well as more sophisticated\ncoordinated location spoofing. These attacks manipulate position data to\ncontrol or undermine LBS functionality, leading to user scams or service\nmanipulation. Therefore, we propose a countermeasure to detect and thwart such\nattacks by utilizing readily available, redundant positioning information from\noff-the-shelf platforms. Our method extends the receiver autonomous integrity\nmonitoring (RAIM) framework by incorporating opportunistic information,\nincluding data from onboard sensors and terrestrial infrastructure signals,\nand, naturally, GNSS. We theoretically show that the fusion of heterogeneous\nsignals improves resilience against sophisticated adversaries on multiple\nfronts. Experimental evaluations show the effectiveness of the proposed scheme\nin improving detection accuracy by 62% at most compared to baseline schemes and\nrestoring accurate positioning."}
{"id": "2510.27485", "categories": ["cs.CR", "cs.OS", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.27485", "abs": "https://arxiv.org/abs/2510.27485", "authors": ["Ben Fiedler", "Samuel Gruetter", "Timothy Roscoe"], "title": "Sockeye: a language for analyzing hardware documentation", "comment": null, "summary": "Systems programmers have to consolidate the ever growing hardware mess\npresent on modern System-on-Chips (SoCs). Correctly programming a multitude of\ncomponents, providing functionality but also security, is a difficult problem:\nsemantics of individual units are described in English prose, descriptions are\noften underspecified, and prone to inaccuracies. Rigorous statements about\nplatform security are often impossible.\n  We introduce a domain-specific language to describe hardware semantics,\nassumptions about software behavior, and desired security properties. We then\ncreate machine-readable specifications for a diverse set of eight SoCs from\ntheir reference manuals, and formally prove their (in-)security. In addition to\nsecurity proofs about memory confidentiality and integrity, we discover a\nhandful of documentation errors. Finally, our analysis also revealed a\nvulnerability on a real-world server chip. Our tooling offers system\nintegrators a way of formally describing security properties for entire SoCs,\nand means to prove them or find counterexamples to them."}
{"id": "2510.27554", "categories": ["cs.CR", "cs.AI", "cs.SI", "68T42, 68R10, 68M14, 68P20, 91D30", "H.3.3; H.2.8; I.2.11; K.4.4; G.2.2; C.2.4"], "pdf": "https://arxiv.org/pdf/2510.27554", "abs": "https://arxiv.org/abs/2510.27554", "authors": ["David Shi", "Kevin Joo"], "title": "Sybil-Resistant Service Discovery for Agent Economies", "comment": "5 pages", "summary": "x402 enables Hypertext Transfer Protocol (HTTP) services like application\nprogramming interfaces (APIs), data feeds, and inference providers to accept\ncryptocurrency payments for access. As agents increasingly consume these\nservices, discovery becomes critical: which swap interface should an agent\ntrust? Which data provider is the most reliable? We introduce TraceRank, a\nreputation-weighted ranking algorithm where payment transactions serve as\nendorsements. TraceRank seeds addresses with precomputed reputation metrics and\npropagates reputation through payment flows weighted by transaction value and\ntemporal recency. Applied to x402's payment graph, this surfaces services\npreferred by high-reputation users rather than those with high transaction\nvolume. Our system combines TraceRank with semantic search to respond to\nnatural language queries with high quality results. We argue that reputation\npropagation resists Sybil attacks by making spam services with many\nlow-reputation payers rank below legitimate services with few high-reputation\npayers. Ultimately, we aim to construct a search method for x402 enabled\nservices that avoids infrastructure bias and has better performance than purely\nvolume based or semantic methods."}
{"id": "2510.27629", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.27629", "abs": "https://arxiv.org/abs/2510.27629", "authors": ["Boyi Wei", "Zora Che", "Nathaniel Li", "Udari Madhushani Sehwag", "Jasper GÃ¶tting", "Samira Nedungadi", "Julian Michael", "Summer Yue", "Dan Hendrycks", "Peter Henderson", "Zifan Wang", "Seth Donoughe", "Mantas Mazeika"], "title": "Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation Models", "comment": "17 Pages, 5 figures", "summary": "Open-weight bio-foundation models present a dual-use dilemma. While holding\ngreat promise for accelerating scientific research and drug development, they\ncould also enable bad actors to develop more deadly bioweapons. To mitigate the\nrisk posed by these models, current approaches focus on filtering biohazardous\ndata during pre-training. However, the effectiveness of such an approach\nremains unclear, particularly against determined actors who might fine-tune\nthese models for malicious use. To address this gap, we propose \\eval, a\nframework to evaluate the robustness of procedures that are intended to reduce\nthe dual-use capabilities of bio-foundation models. \\eval assesses models'\nvirus understanding through three lenses, including sequence modeling,\nmutational effects prediction, and virulence prediction. Our results show that\ncurrent filtering practices may not be particularly effective: Excluded\nknowledge can be rapidly recovered in some cases via fine-tuning, and exhibits\nbroader generalizability in sequence modeling. Furthermore, dual-use signals\nmay already reside in the pretrained representations, and can be elicited via\nsimple linear probing. These findings highlight the challenges of data\nfiltering as a standalone procedure, underscoring the need for further research\ninto robust safety and security strategies for open-weight bio-foundation\nmodels."}
