<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 13]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [VISAT: Benchmarking Adversarial and Distribution Shift Robustness in Traffic Sign Recognition with Visual Attributes](https://arxiv.org/abs/2510.26833)
*Simon Yu,Peilin Yu,Hongbo Zheng,Huajie Shao,Han Zhao,Lui Sha*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present VISAT, a novel open dataset and benchmarking suite for evaluating
model robustness in the task of traffic sign recognition with the presence of
visual attributes. Built upon the Mapillary Traffic Sign Dataset (MTSD), our
dataset introduces two benchmarks that respectively emphasize robustness
against adversarial attacks and distribution shifts. For our adversarial attack
benchmark, we employ the state-of-the-art Projected Gradient Descent (PGD)
method to generate adversarial inputs and evaluate their impact on popular
models. Additionally, we investigate the effect of adversarial attacks on
attribute-specific multi-task learning (MTL) networks, revealing spurious
correlations among MTL tasks. The MTL networks leverage visual attributes
(color, shape, symbol, and text) that we have created for each traffic sign in
our dataset. For our distribution shift benchmark, we utilize ImageNet-C's
realistic data corruption and natural variation techniques to perform
evaluations on the robustness of both base and MTL models. Moreover, we further
explore spurious correlations among MTL tasks through synthetic alterations of
traffic sign colors using color quantization techniques. Our experiments focus
on two major backbones, ResNet-152 and ViT-B/32, and compare the performance
between base and MTL models. The VISAT dataset and benchmarking framework
contribute to the understanding of model robustness for traffic sign
recognition, shedding light on the challenges posed by adversarial attacks and
distribution shifts. We believe this work will facilitate advancements in
developing more robust models for real-world applications in autonomous driving
and cyber-physical systems.

</details>


### [2] [Broken-Token: Filtering Obfuscated Prompts by Counting Characters-Per-Token](https://arxiv.org/abs/2510.26847)
*Shaked Zychlinski,Yuval Kainan*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) are susceptible to jailbreak attacks where
malicious prompts are disguised using ciphers and character-level encodings to
bypass safety guardrails. While these guardrails often fail to interpret the
encoded content, the underlying models can still process the harmful
instructions. We introduce CPT-Filtering, a novel, model-agnostic with
negligible-costs and near-perfect accuracy guardrail technique that aims to
mitigate these attacks by leveraging the intrinsic behavior of Byte-Pair
Encoding (BPE) tokenizers. Our method is based on the principle that
tokenizers, trained on natural language, represent out-of-distribution text,
such as ciphers, using a significantly higher number of shorter tokens. Our
technique uses a simple yet powerful artifact of using language models: the
average number of Characters Per Token (CPT) in the text. This approach is
motivated by the high compute cost of modern methods - relying on added modules
such as dedicated LLMs or perplexity models. We validate our approach across a
large dataset of over 100,000 prompts, testing numerous encoding schemes with
several popular tokenizers. Our experiments demonstrate that a simple CPT
threshold robustly identifies encoded text with high accuracy, even for very
short inputs. CPT-Filtering provides a practical defense layer that can be
immediately deployed for real-time text filtering and offline data curation.

</details>


### [3] [LLM-based Multi-class Attack Analysis and Mitigation Framework in IoT/IIoT Networks](https://arxiv.org/abs/2510.26941)
*Seif Ikbarieh,Maanak Gupta,Elmahedi Mahalal*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The Internet of Things has expanded rapidly, transforming communication and
operations across industries but also increasing the attack surface and
security breaches. Artificial Intelligence plays a key role in securing IoT,
enabling attack detection, attack behavior analysis, and mitigation suggestion.
Despite advancements, evaluations remain purely qualitative, and the lack of a
standardized, objective benchmark for quantitatively measuring AI-based attack
analysis and mitigation hinders consistent assessment of model effectiveness.
In this work, we propose a hybrid framework combining Machine Learning (ML) for
multi-class attack detection with Large Language Models (LLMs) for attack
behavior analysis and mitigation suggestion. After benchmarking several ML and
Deep Learning (DL) classifiers on the Edge-IIoTset and CICIoT2023 datasets, we
applied structured role-play prompt engineering with Retrieval-Augmented
Generation (RAG) to guide ChatGPT-o3 and DeepSeek-R1 in producing detailed,
context-aware responses. We introduce novel evaluation metrics for quantitative
assessment to guide us and an ensemble of judge LLMs, namely ChatGPT-4o,
DeepSeek-V3, Mixtral 8x7B Instruct, Gemini 2.5 Flash, Meta Llama 4, TII Falcon
H1 34B Instruct, xAI Grok 3, and Claude 4 Sonnet, to independently evaluate the
responses. Results show that Random Forest has the best detection model, and
ChatGPT-o3 outperformed DeepSeek-R1 in attack analysis and mitigation.

</details>


### [4] [Adapting Large Language Models to Emerging Cybersecurity using Retrieval Augmented Generation](https://arxiv.org/abs/2510.27080)
*Arnabh Borah,Md Tanvirul Alam,Nidhi Rastogi*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Security applications are increasingly relying on large language models
(LLMs) for cyber threat detection; however, their opaque reasoning often limits
trust, particularly in decisions that require domain-specific cybersecurity
knowledge. Because security threats evolve rapidly, LLMs must not only recall
historical incidents but also adapt to emerging vulnerabilities and attack
patterns. Retrieval-Augmented Generation (RAG) has demonstrated effectiveness
in general LLM applications, but its potential for cybersecurity remains
underexplored. In this work, we introduce a RAG-based framework designed to
contextualize cybersecurity data and enhance LLM accuracy in knowledge
retention and temporal reasoning. Using external datasets and the
Llama-3-8B-Instruct model, we evaluate baseline RAG, an optimized hybrid
retrieval approach, and conduct a comparative analysis across multiple
performance metrics. Our findings highlight the promise of hybrid retrieval in
strengthening the adaptability and reliability of LLMs for cybersecurity tasks.

</details>


### [5] [Lightweight CNN Model Hashing with Higher-Order Statistics and Chaotic Mapping for Piracy Detection and Tamper Localization](https://arxiv.org/abs/2510.27127)
*Kunming Yang,Ling Chen*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: With the widespread adoption of deep neural networks (DNNs), protecting
intellectual property and detecting unauthorized tampering of models have
become pressing challenges. Recently, Perceptual hashing has emerged as an
effective approach for identifying pirated models. However, existing methods
either rely on neural networks for feature extraction, demanding substantial
training resources, or suffer from limited applicability and cannot be
universally applied to all convolutional neural networks (CNNs). To address
these limitations, we propose a lightweight CNN model hashing technique that
integrates higher-order statistics (HOS) features with a chaotic mapping
mechanism. Without requiring any auxiliary neural network training, our method
enables efficient piracy detection and precise tampering localization.
Specifically, we extract skewness, kurtosis, and structural features from the
parameters of each network layer to construct a model hash that is both robust
and discriminative. Additionally, we introduce chaotic mapping to amplify minor
changes in model parameters by exploiting the sensitivity of chaotic systems to
initial conditions, thereby facilitating accurate localization of tampered
regions. Experimental results validate the effectiveness and practical value of
the proposed method for model copyright protection and integrity verification.

</details>


### [6] [Measuring the Security of Mobile LLM Agents under Adversarial Prompts from Untrusted Third-Party Channels](https://arxiv.org/abs/2510.27140)
*Chenghao Du,Quanfeng Huang,Tingxuan Tang,Zihao Wang,Yue Xiao*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) have transformed software development, enabling
AI-powered applications known as LLM-based agents that promise to automate
tasks across diverse apps and workflows. Yet, the security implications of
deploying such agents in adversarial mobile environments remain poorly
understood. In this paper, we present the first systematic study of security
risks in mobile LLM agents. We design and evaluate a suite of adversarial case
studies, ranging from opportunistic manipulations such as pop-up advertisements
to advanced, end-to-end workflows involving malware installation and cross-app
data exfiltration. Our evaluation covers eight state-of-the-art mobile agents
across three architectures, with over 2,000 adversarial and paired benign
trials. The results reveal systemic vulnerabilities: low-barrier vectors such
as fraudulent ads succeed with over 80% reliability, while even workflows
requiring the circumvention of operating-system warnings, such as malware
installation, are consistently completed by advanced multi-app agents. By
mapping these attacks to the MITRE ATT&CK Mobile framework, we uncover novel
privilege-escalation and persistence pathways unique to LLM-driven automation.
Collectively, our findings provide the first end-to-end evidence that mobile
LLM agents are exploitable in realistic adversarial settings, where untrusted
third-party channels (e.g., ads, embedded webviews, cross-app notifications)
are an inherent part of the mobile ecosystem.

</details>


### [7] [Unvalidated Trust: Cross-Stage Vulnerabilities in Large Language Model Architectures](https://arxiv.org/abs/2510.27190)
*Dominik Schwarz*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As Large Language Models (LLMs) are increasingly integrated into automated,
multi-stage pipelines, risk patterns that arise from unvalidated trust between
processing stages become a practical concern. This paper presents a
mechanism-centered taxonomy of 41 recurring risk patterns in commercial LLMs.
The analysis shows that inputs are often interpreted non-neutrally and can
trigger implementation-shaped responses or unintended state changes even
without explicit commands. We argue that these behaviors constitute
architectural failure modes and that string-level filtering alone is
insufficient. To mitigate such cross-stage vulnerabilities, we recommend
zero-trust architectural principles, including provenance enforcement, context
sealing, and plan revalidation, and we introduce "Countermind" as a conceptual
blueprint for implementing these defenses.

</details>


### [8] [Prevalence of Security and Privacy Risk-Inducing Usage of AI-based Conversational Agents](https://arxiv.org/abs/2510.27275)
*Kathrin Grosse,Nico Ebert*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent improvement gains in large language models (LLMs) have lead to
everyday usage of AI-based Conversational Agents (CAs). At the same time, LLMs
are vulnerable to an array of threats, including jailbreaks and, for example,
causing remote code execution when fed specific inputs. As a result, users may
unintentionally introduce risks, for example, by uploading malicious files or
disclosing sensitive information. However, the extent to which such user
behaviors occur and thus potentially facilitate exploits remains largely
unclear. To shed light on this issue, we surveyed a representative sample of
3,270 UK adults in 2024 using Prolific. A third of these use CA services such
as ChatGPT or Gemini at least once a week. Of these ``regular users'', up to a
third exhibited behaviors that may enable attacks, and a fourth have tried
jailbreaking (often out of understandable reasons such as curiosity, fun or
information seeking). Half state that they sanitize data and most participants
report not sharing sensitive data. However, few share very sensitive data such
as passwords. The majority are unaware that their data can be used to train
models and that they can opt-out. Our findings suggest that current academic
threat models manifest in the wild, and mitigations or guidelines for the
secure usage of CAs should be developed. In areas critical to security and
privacy, CAs must be equipped with effective AI guardrails to prevent, for
example, revealing sensitive information to curious employees. Vendors need to
increase efforts to prevent the entry of sensitive data, and to create
transparency with regard to data usage policies and settings.

</details>


### [9] [Sustaining Cyber Awareness: The Long-Term Impact of Continuous Phishing Training and Emotional Triggers](https://arxiv.org/abs/2510.27298)
*Rebeka Toth,Richard A. Dubniczky,Olga Limonova,Norbert Tihanyi*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Phishing constitutes more than 90\% of successful cyberattacks globally,
remaining one of the most persistent threats to organizational security.
Despite organizations tripling their cybersecurity budgets between 2015 and
2025, the human factor continues to pose a critical vulnerability. This study
presents a 12-month longitudinal investigation examining how continuous
cybersecurity training and emotional cues affect employee susceptibility to
phishing. The experiment involved 20 organizations and over 1,300 employees who
collectively received more than 13,000 simulated phishing emails engineered
with diverse emotional, contextual, and structural characteristics. Behavioral
responses were analyzed using non-parametric correlation and regression models
to assess the influence of psychological manipulation, message personalization,
and perceived email source. Results demonstrate that sustained phishing
simulations and targeted training programs lead to a significant reduction in
employee susceptibility, halving successful compromise rates within six months.
Additionally, employee turnover introduces measurable fluctuations in awareness
levels, underscoring the necessity of maintaining continuous training
initiatives. These findings provide one of the few long-term perspectives on
phishing awareness efficacy, highlighting the strategic importance of ongoing
behavioral interventions in strengthening organizational cyber resilience. In
order to support open science, we published our email templates, source code,
and other materials at https://github.com/CorporatePhishingStudy

</details>


### [10] [Coordinated Position Falsification Attacks and Countermeasures for Location-Based Services](https://arxiv.org/abs/2510.27346)
*Wenjie Liu,Panos Papadimitratos*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: With the rise of location-based service (LBS) applications that rely on
terrestrial and satellite infrastructures (e.g., GNSS and crowd-sourced Wi-Fi,
Bluetooth, cellular, and IP databases) for positioning, ensuring their
integrity and security is paramount. However, we demonstrate that these
applications are susceptible to low-cost attacks (less than $50), including
Wi-Fi spoofing combined with GNSS jamming, as well as more sophisticated
coordinated location spoofing. These attacks manipulate position data to
control or undermine LBS functionality, leading to user scams or service
manipulation. Therefore, we propose a countermeasure to detect and thwart such
attacks by utilizing readily available, redundant positioning information from
off-the-shelf platforms. Our method extends the receiver autonomous integrity
monitoring (RAIM) framework by incorporating opportunistic information,
including data from onboard sensors and terrestrial infrastructure signals,
and, naturally, GNSS. We theoretically show that the fusion of heterogeneous
signals improves resilience against sophisticated adversaries on multiple
fronts. Experimental evaluations show the effectiveness of the proposed scheme
in improving detection accuracy by 62% at most compared to baseline schemes and
restoring accurate positioning.

</details>


### [11] [Sockeye: a language for analyzing hardware documentation](https://arxiv.org/abs/2510.27485)
*Ben Fiedler,Samuel Gruetter,Timothy Roscoe*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Systems programmers have to consolidate the ever growing hardware mess
present on modern System-on-Chips (SoCs). Correctly programming a multitude of
components, providing functionality but also security, is a difficult problem:
semantics of individual units are described in English prose, descriptions are
often underspecified, and prone to inaccuracies. Rigorous statements about
platform security are often impossible.
  We introduce a domain-specific language to describe hardware semantics,
assumptions about software behavior, and desired security properties. We then
create machine-readable specifications for a diverse set of eight SoCs from
their reference manuals, and formally prove their (in-)security. In addition to
security proofs about memory confidentiality and integrity, we discover a
handful of documentation errors. Finally, our analysis also revealed a
vulnerability on a real-world server chip. Our tooling offers system
integrators a way of formally describing security properties for entire SoCs,
and means to prove them or find counterexamples to them.

</details>


### [12] [Sybil-Resistant Service Discovery for Agent Economies](https://arxiv.org/abs/2510.27554)
*David Shi,Kevin Joo*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: x402 enables Hypertext Transfer Protocol (HTTP) services like application
programming interfaces (APIs), data feeds, and inference providers to accept
cryptocurrency payments for access. As agents increasingly consume these
services, discovery becomes critical: which swap interface should an agent
trust? Which data provider is the most reliable? We introduce TraceRank, a
reputation-weighted ranking algorithm where payment transactions serve as
endorsements. TraceRank seeds addresses with precomputed reputation metrics and
propagates reputation through payment flows weighted by transaction value and
temporal recency. Applied to x402's payment graph, this surfaces services
preferred by high-reputation users rather than those with high transaction
volume. Our system combines TraceRank with semantic search to respond to
natural language queries with high quality results. We argue that reputation
propagation resists Sybil attacks by making spam services with many
low-reputation payers rank below legitimate services with few high-reputation
payers. Ultimately, we aim to construct a search method for x402 enabled
services that avoids infrastructure bias and has better performance than purely
volume based or semantic methods.

</details>


### [13] [Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation Models](https://arxiv.org/abs/2510.27629)
*Boyi Wei,Zora Che,Nathaniel Li,Udari Madhushani Sehwag,Jasper GÃ¶tting,Samira Nedungadi,Julian Michael,Summer Yue,Dan Hendrycks,Peter Henderson,Zifan Wang,Seth Donoughe,Mantas Mazeika*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Open-weight bio-foundation models present a dual-use dilemma. While holding
great promise for accelerating scientific research and drug development, they
could also enable bad actors to develop more deadly bioweapons. To mitigate the
risk posed by these models, current approaches focus on filtering biohazardous
data during pre-training. However, the effectiveness of such an approach
remains unclear, particularly against determined actors who might fine-tune
these models for malicious use. To address this gap, we propose \eval, a
framework to evaluate the robustness of procedures that are intended to reduce
the dual-use capabilities of bio-foundation models. \eval assesses models'
virus understanding through three lenses, including sequence modeling,
mutational effects prediction, and virulence prediction. Our results show that
current filtering practices may not be particularly effective: Excluded
knowledge can be rapidly recovered in some cases via fine-tuning, and exhibits
broader generalizability in sequence modeling. Furthermore, dual-use signals
may already reside in the pretrained representations, and can be elicited via
simple linear probing. These findings highlight the challenges of data
filtering as a standalone procedure, underscoring the need for further research
into robust safety and security strategies for open-weight bio-foundation
models.

</details>
